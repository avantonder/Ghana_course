---
title: "2.4 Combining Commands"
---

**Teaching: 20 min** || **Exercises: 10 min**

## Overview

:::::{.callout}

:::{.callout-important icon=false}
### Questions:
- How can I combine existing commands to do new things?
:::

:::{.callout-important icon=false}
### Learning Objectives:
- Redirect a command's output to a file.
- Process a file instead of keyboard input using redirection.
- Construct command pipelines with two or more stages.
- Explain what usually happens if a program or pipeline isn't given any input to process.
:::

:::{.callout-tip}
### Keypoints:
- Other useful commands to manipulate text and which are often used together in pipes include: 
  - `cat` displays the contents of its inputs.
  - `head` displays the first 10 lines of its input.
  - `tail` displays the last 10 lines of its input.
  - `sort` used to order the input lines _alphabetically_ (default) or _numerically_ (`-n` option).
  - `wc` counts lines, words, and characters in its inputs.
  - `cut` to extract parts of a file that are separated by a delimiter. 
  - `uniq` to obtain only unique lines from its input. The `-c` option can also be used to count how often each of those unique lines occur. 
- `command > file` redirects a command's output to a file (overwriting any existing content).
- `command >> file` appends a command's output to a file.
- `<` operator redirects input to a command
- The `|` pipe allows to chain several commands together. 
- `first | second` is a pipeline: the output of the first command is used as the input to the second. 
- The best way to use the shell is to use pipes to combine simple single-purpose programs (filters).
:::
:::::

## A few more basic commands 

### Performing counts with `wc` 
We'll start with a directory called `molecules`
that contains six files describing some simple organic molecules.
The `.pdb` extension indicates that these files are in Protein Data Bank format,
a simple text format that specifies the type and position of each atom in the molecule.

:::{.callout}
```bash
ls molecules
```
```
cubane.pdb    ethane.pdb    methane.pdb
octane.pdb    pentane.pdb   propane.pdb
```
:::

Let's go into that directory with `cd` and run the command `wc *.pdb`.
`wc` is the "word count" command:
it counts the number of lines, words, and characters in files (from left to right, in that order).

The `*` in `*.pdb` matches zero or more characters,
so the shell turns `*.pdb` into a list of all `.pdb` files in the current directory:

:::{.callout}
```bash
cd molecules
wc *.pdb
```
```
  20  156  1158  cubane.pdb
  12  84   622   ethane.pdb
   9  57   422   methane.pdb
  30  246  1828  octane.pdb
  21  165  1226  pentane.pdb
  15  111  825   propane.pdb
 107  819  6081  total
```
:::


If we run `wc -l` instead of just `wc`,
the output shows only the number of lines per file:

:::{.callout}
```bash
wc -l *.pdb
```
```
  20  cubane.pdb
  12  ethane.pdb
   9  methane.pdb
  30  octane.pdb
  21  pentane.pdb
  15  propane.pdb
 107  total
```
:::

:::{.callout}
## Why Isn't It Doing Anything?

What happens if a command is supposed to process a file, but we
don't give it a filename? For example, what if we type:

```bash
wc -l
```

but don't type `*.pdb` (or anything else) after the command?
Since it doesn't have any filenames, `wc` assumes it is supposed to
process input given at the command prompt, so it just sits there and waits for us to give
it some data interactively. From the outside, though, all we see is it
sitting there: the command doesn't appear to do anything.

If you make this kind of mistake, you can escape out of this state by holding down
the control key (<kbd>Ctrl</kbd>) and typing the letter <kbd>C</kbd> once and letting go of the <kbd>Ctrl</kbd> key. <kbd>Ctrl</kbd>+<kbd>C</kbd>
:::


We can also use `-w` to get only the number of words,
or `-c` to get only the number of characters.

Which of these files contains the fewest lines?
It's an easy question to answer when there are only six files,
but what if there were 6000?
Our first step toward a solution is to run the command:

```bash
wc -l *.pdb > lengths.txt
```

The greater than symbol, `>`, tells the shell to **redirect** the command's output
to a file instead of printing it to the screen. (This is why there is no screen output:
everything that `wc` would have printed has gone into the file `lengths.txt` instead.)  The shell will create the file if it doesn't exist. If the file exists, it will be silently overwritten, which may lead to data loss and thus requires
some caution.
`ls lengths.txt` confirms that the file exists:

:::{.callout}
```bash
ls lengths.txt
```
```
lengths.txt
```
:::

### Printing file content to screen with `cat`

We can now send the content of `lengths.txt` to the screen using `cat lengths.txt`.
The `cat` command gets its name from "concatenate" i.e. join together,
and it prints the contents of files one after another.
There's only one file in this case,
so `cat` just shows us what it contains:

:::{.callout}
```bash
cat lengths.txt
```
```
  20  cubane.pdb
  12  ethane.pdb
   9  methane.pdb
  30  octane.pdb
  21  pentane.pdb
  15  propane.pdb
 107  total
```
:::

:::{.callout-note}
## Output Page by Page
We'll continue to use `cat` in this lesson, for convenience and consistency,
but it has the disadvantage that it always dumps the whole file onto your screen.
More useful in practice is the command `less`,
which you use with `less lengths.txt`.
This displays a screenful of the file, and then stops.
You can go forward one screenful by pressing the spacebar,
or back one by pressing `b`.  Press `q` to quit.
:::


### Sorting with the `sort` command

Now let's use the `sort` command to sort its contents.

:::::{.callout}
## What Does `sort -n` Do?
If we run `sort` on a file containing the following lines:

```
10
2
19
22
6
```

the output is:

:::{.callout}
```
10
19
2
22
6
```
:::

If we run `sort -n` on the same input, we get this instead:

:::{.callout}
```
2
6
10
19
22
```
:::

:::{.callout-note}
The `-n` option specifies a numerical rather than an alphanumerical sort.
:::
:::::


We will also use the `-n` option to specify that the sort is numerical instead of alphanumerical.
This does *not* change the file;
instead, it sends the sorted result to the screen:

:::{.callout}
```bash
sort -n lengths.txt
```
```
  9  methane.pdb
 12  ethane.pdb
 15  propane.pdb
 20  cubane.pdb
 21  pentane.pdb
 30  octane.pdb
107  total
```
:::

We can put the sorted list of lines in another temporary file called `sorted-lengths.txt`
by putting `> sorted-lengths.txt` after the command, just as we used `> lengths.txt` to put the output of `wc` into `lengths.txt`.
Once we've done that, we can run another command called `head` to get the first few lines in `sorted-lengths.txt`:

:::{.callout}
```bash
sort -n lengths.txt > sorted-lengths.txt
head -n 1 sorted-lengths.txt
```
```
  9  methane.pdb
```
:::

Using `-n 1` with `head` tells it that we only want the first line of the file;
`-n 20` would get the first 20, and so on.
Since `sorted-lengths.txt` contains the lengths of our files ordered from least to greatest, the output of `head` must be the file with the fewest lines.

### Redirecting to the same file `>>`

It's a very bad idea to try redirecting the output of a command that operates on a file to the same file. For example:

```bash
sort -n lengths.txt > lengths.txt
```

Doing something like this may give you incorrect results and/or delete the contents of `lengths.txt`.


#### What Does `>>` Mean?
We have seen the use of `>`, but there is a similar operator `>>` which works slightly differently.
We'll learn about the differences between these two operators by printing some strings.
We can use the `echo` command to print strings e.g.

:::{.callout}
```bash
echo The echo command prints text
```
```
The echo command prints text
```
:::

Now test the commands below to reveal the difference between the two operators:

*(Try executing each command twice in a row and then examining the output files)*

```bash
echo hello > testfile01.txt
```

and:

```bash
echo hello >> testfile02.txt
```

:::{.callout-note}
In the first example with `>`, the string "hello" is written to `testfile01.txt`,
but the file gets overwritten each time we run the command.

We see from the second example that the `>>` operator also writes "hello" to a file (in this case`testfile02.txt`), but appends the string to the file if it already exists (i.e. when we run it for the second time).
:::


:::::{.callout-important icon=false}
## ***Exercise:*** Appending Data
We have already met the `head` command, which prints lines from the start of a file.
`tail` is similar, but prints lines from the end of a file instead.

Consider the file `data-shell/data/animals.txt`. After these commands, select the answer that corresponds to the file `animals-subset.txt`:

```bash
head -n 3 animals.txt > animals-subset.txt
tail -n 2 animals.txt >> animals-subset.txt
```

1. The first three lines of `animals.txt`
2. The last two lines of `animals.txt`
3. The first three lines and the last two lines of `animals.txt`
4. The second and third lines of `animals.txt`

:::{.callout collapse="true"}
## ***Solution:***
Option 3 is correct.
For option 1 to be correct we would only run the `head` command.
For option 2 to be correct we would only run the `tail` command.
For option 4 to be correct we would have to pipe the output of `head` into `tail -n 2` by doing `head -n 3 animals.txt | tail -n 2 > animals-subset.txt`
:::
:::::


If you think this is confusing, that's fine: even once you understand what `wc`, `sort`, and `head` do, all those intermediate files make it hard to follow what's going on.

:::{.callout} 
Now that we know a few basic commands,
we can finally look at the shell's most powerful feature:
the ease with which it lets us combine existing programs in new ways.
:::

## The `|` Pipe
The way we can combine commands together is using a **pipe**, which uses the special operator `|`.

From our previous exercise, we can make it easier to understand by running `sort` and `head` together:

:::{.callout}
```bash
sort -n lengths.txt | head -n 1
```
```
  9  methane.pdb
```
:::

The vertical bar, `|`, between the two commands is called a **pipe**. It tells the shell that we want to use the output of the command on the left as the input to the command on the right.

Nothing prevents us from chaining pipes consecutively.
That is, we can for example send the output of `wc` directly to `sort`, and then the resulting output to `head`.
Thus we first use a pipe to send the output of `wc` to `sort`:

:::{.callout}
```bash
wc -l *.pdb | sort -n
```
```
   9 methane.pdb
  12 ethane.pdb
  15 propane.pdb
  20 cubane.pdb
  21 pentane.pdb
  30 octane.pdb
 107 total
```
:::

And now we send the output of this pipe, through another pipe, to `head`, so that the full pipeline becomes:

:::{.callout}
```bash
wc -l *.pdb | sort -n | head -n 1
```
```
   9  methane.pdb
```
:::

This is exactly like a mathematician nesting functions like *log(3x)* and saying "the log of three times *x*".
In our case, the calculation is "head of sort of line count of `*.pdb`".


The redirection and pipes used in the last few commands are illustrated below:

![Redirects and Pipes](../fig/redirects-and-pipes.png)


:::::{.callout-important icon=false}
## ***Exercise:*** Piping Commands Together
In our current directory, we want to find the 3 files which have the least number of
lines. Which command listed below would work?

1. `wc -l * > sort -n > head -n 3`
2. `wc -l * | sort -n | head -n 1-3`
3. `wc -l * | head -n 3 | sort -n`
4. `wc -l * | sort -n | head -n 3`

:::{.callout collapse="true"}
## ***Solution:***
Option 4 is the solution.
The pipe character `|` is used to connect the output from one command to
the input of another.
`>` is used to redirect standard output to a file.
Try it in the `data-shell/molecules` directory!
:::
:::::


:::::{.callout-important icon=false}
## ***Exercise:*** An example pipeline: Checking Files
There are 17 files from an assay in the `~/Desktop/data-shell/north-pacific-gyre/2012-07-03` directory 
Suppose you want to do some quick sanity checks on the content of the files. You know that files > are supposed to have 300 lines.
Starting by moving to that directory,

1. How would you check if there are any files with fewer than 300 lines in the directory?
2. How would you check if there are any files with more than 300 lines in the directory?

:::{.callout collapse="true"}
## ***Solution:***
1. `wc -l *.txt | sort -n | head -n 5`. You can report the number of lines of all the text files in the directory, sort them, and then get the top (if any files have fewer than 300 lines they will appear here).
2. `wc -l *.txt | sort -n -r | head -n 5`. Same as above but now we want to sort the files in reverse order.
:::
:::::


This idea of linking programs together is why Unix has been so successful. Instead of creating enormous programs that try to do many different things, Unix programmers focus on creating lots of simple tools that each do one job well, and that work well with each other. This programming model is called "pipes and filters". 

We've already seen pipes; a **filter** is a program like `wc` or `sort` that transforms a stream of input into a stream of output. Almost all of the standard Unix tools can work this way: unless told to do otherwise, they read from standard input, do something with what they've read, and write to standard output.

The key is that any program that reads lines of text from standard input and writes lines of text to standard output can be combined with every other program that behaves this way as well. You can *and should* write your programs this way so that you and other people can put those programs into pipes to multiply their power.


:::::{.callout-important icon=false}
## ***Exercise:*** Pipe Reading Comprehension
A file called `animals.txt` (in the `data-shell/data` folder) contains the following data:

```
2012-11-05,deer
2012-11-05,rabbit
2012-11-05,raccoon
2012-11-06,rabbit
2012-11-06,deer
2012-11-06,fox
2012-11-07,rabbit
2012-11-07,bear
```

What text passes through each of the pipes and the final redirect in the pipeline below?

```bash
cat animals.txt | head -n 5 | tail -n 3 | sort -r > final.txt
```

**Hint:** build the pipeline up one command at a time to test your understanding

:::{.callout collapse="true"}
## ***Solution:***
The `head` command extracts the first 5 lines from `animals.txt`.
Then, the last 3 lines are extracted from the previous 5 by using the `tail` command.
With the `sort -r` command those 3 lines are sorted in reverse order and finally,
the output is redirected to a file `final.txt`.
The content of this file can be checked by executing `cat final.txt`.
The file should contain the following lines:

```
2012-11-06,rabbit
2012-11-06,deer
2012-11-05,raccoon
```
:::
:::::


:::::{.callout-important icon=false}
## ***Exercise:*** Pipe Construction
For the file `animals.txt` from the previous exercise, consider the following command:

:::{.callout}
```bash
cut -d , -f 2 animals.txt
```

The `cut` command is used to remove or "cut out" certain sections of each line in the file. The optional `-d` flag is used to define the delimiter. A **delimiter** is a character that is used to separate each line of text into columns. The default delimiter is <kbd>Tab</kbd>, meaning that the `cut` command will automatically assume that values in different columns will be separated by a tab. The `-f` flag is used to specify the field (column) to cut out.
The command above uses the `-d` option to split each line by comma, and the `-f` option to print the second field in each line, to give the following output:

```
deer
rabbit
raccoon
rabbit
deer
fox
rabbit
bear
```
:::

The `uniq` command filters out adjacent matching lines in a file.
How could you extend this pipeline (using `uniq` and another command) to find out what animals the file contains (without any duplicates in their names)?

:::{.callout collapse="true"}
## ***Solution:***
```bash
cut -d , -f 2 animals.txt | sort | uniq
```
:::
:::::


:::{.callout-note}
## Awk: a more powerful tool for text processing
We have seen the `cut` command that allows the selection of columns in tabular data.
If you need more powerful manipulation of tabular data you can use the command `awk`, which permits more powerful operations (selection, calculations etc.) on columns. For more complex operations, however, we recommend going to your favourite programming language!
:::


:::::{.callout-important icon=false}
## ***Exercise:*** Which Pipe?
The file `animals.txt` contains 8 lines of data formatted as follows:

:::{.callout}
```
2012-11-05,deer
2012-11-05,rabbit
2012-11-05,raccoon
2012-11-06,rabbit
...
```
:::

The `uniq` command has a `-c` option which gives a count of the number of times a line occurs in its input.  Assuming your current directory is `data-shell/data/`, what command would you use to produce a table that shows the total count of each type of animal in the file?

1.  `sort animals.txt | uniq -c`
2.  `sort -t, -k2,2 animals.txt | uniq -c`
3.  `cut -d, -f 2 animals.txt | uniq -c`
4.  `cut -d, -f 2 animals.txt | sort | uniq -c`
5.  `cut -d, -f 2 animals.txt | sort | uniq -c | wc -l`

:::{.callout collapse="true"}
## ***Solution:***
Option 4. is the correct answer.
If you have difficulty understanding why, try running the commands, or sub-sections of the pipelines (make sure you are in the `data-shell/data` directory).
:::
:::::


:::::{.callout-important icon=false}
## ***Exercise:*** Filtering by patterns
`grep` is another command that searches for patterns in text. Patterns could be simple text or a combination of text and the wildcard characters we have seen before like ? and *. Like > other commands we have seen `grep` can be used on multiple files. 
For example if we wanted to find all occurences of name in all the text files we could write:

```bash
grep "name" *.txt
```

Using the `animals.txt` file suppose we wanted to copy all the rabbit dates to a separate file `rabbit-dates.txt`. 
Which combination of commands would achieve this?

:::{.callout collapse="true"}
## ***Solution:***
grep "rabbit" animals.txt | cut -d, -f 1 > rabbit-dates.txt
:::
:::::


## More exercises on `|` Pipe

<!--FIXME: Data for these exercises are in the unix-shell folder. But update these with personal bacteria data-->

:::::{.callout-important icon=false}
## ***Exercise:*** Pipe Comprehension
If you had the following two text files:

```bash
cat animals_1.txt
```

```
deer
rabbit
raccoon
rabbit
```

```bash
cat animals_2.txt
```

```
deer
fox
rabbit
bear
```

What would be the result of the following command?

```bash
cat animals*.txt | head -n 6 | tail -n 1
```

:::{.callout collapse="true"}
## ***Solution:***
The result would be "fox". 
Here is a diagram illustrating what happens in each of the three steps:

```
                    deer
                    rabbit                  deer
cat animals*.txt    raccoon    head -n 6    rabbit     tail -n 1
----------------->  rabbit    ----------->  raccoon   ----------->  fox
                    deer                    rabbit
                    fox                     deer
                    rabbit                  fox
                    bear
```

- `cat animals*.txt` would combine the content of both files, and then
- `head -n 6` would print the first six lines of the combined file, and then
- `tail -n 1` would return the last line of this output.
:::
:::::


:::::{.callout-important icon=false}
## ***Exercise:*** `zcat` and `grep`
In the `coronavirus` folder, you will find a file named `proteins.fa.gz`. 
This is a file that contains the amino acid sequences of the proteins in the SARS-CoV-2 coronavirus in a text-based format known as FASTA. 
However, this file is _compressed_ using an algorithm known as _GZip_, which is indicated by the file extension `.gz`.  
To look inside compressed files, you can use an alternative to `cat` called `zcat` (the 'z' at the beggining indicates it will work on _zipped_ files). 

1. Use `zcat` together with `less` to look inside this file. 
  <details><summary>Hint</summary>
  Remember you can press <kbd>Q</kbd> to exit the `less` program.
  </details>
2. The content of this file may look a little strange, if you're not familiar with the FASTA file format. 
  But basically, each protein sequence name starts with the `>` symbol. 
  Combine `zcat` with `grep` to extract the sequence names only. 
  How many proteins does SARS-CoV-2 have?

:::{.callout collapse="true"}
## ***Solution:***

**Task 1**

The following command allows us to "browse" through the content of this file: 

```bash
zcat proteins.fa.gz | less
```

We can use <kbd>↑</kbd> and <kbd>↓</kbd> to move line-by-line or the <kbd>Page Up</kbd> and <kbd>Page Down</kbd> keys to move page-by-page. 
You can exit `less` by pressing <kbd>Q</kbd> (for "quit"). 
This will bring you back to the console. 

----

**Task 2**

We can look at the sequences' names by running: 

```bash
zcat proteins.fa.gz | grep ">"
```

```
>ORF1ab protein=ORF1ab polyprotein
>ORF1ab protein=ORF1a polyprotein
>S protein=surface glycoprotein
>ORF3a protein=ORF3a protein
>E protein=envelope protein
>M protein=membrane glycoprotein
>ORF6 protein=ORF6 protein
>ORF7a protein=ORF7a protein
>ORF7b protein=ORF7b
>ORF8 protein=ORF8 protein
>N protein=nucleocapsid phosphoprotein
>ORF10 protein=ORF10 protein
```

We could further count how many sequences, by piping this output to `wc`:

```bash
zcat proteins.fa.gz | grep ">" | wc -l
```

```
12
```
:::
:::::


### More on Cut, Sort, Unique & Count

Let's now explore a few more useful commands to manipulate text that can be combined to quickly answer useful questions about our data. 

Let's start with the command `cut`, which is used to extract sections from each line of its input. 
For example, let's say we wanted to retrieve only the second _field_ (or column) of our CSV file, which contains the clade classification of each of our omicron samples:

:::{.callout}
```bash
cat *_variants.csv | cut -d "," -f 2
```

```
clade
20I (Alpha; V1)
20A
20I (Alpha; V1)
20A

... (more output omitted) ...
```
:::

The two options used with this command are: 

- `-d` defines the _delimiter_ used to separate different parts of the line. 
  Because this is a CSV file, we use the comma as our delimiter. 
  The _tab_ is used as the default delimiter. 
- `-f` defines the _field_ or part of the line we want to extract. 
  In our case, we want the second _field_ (or column) of our CSV file. 
  It's worth knowing that you can specify more than one _field_, so for example if you had a CSV file with more columns and wanted columns 3 and 7 you could set `-d 3,7`.

The next command we will explore is called `sort`, which sorts the lines of its input _alphabetically_ (default) or _numerically_ (if using the `-n` option). 
Let's combine it with our previous command to see the result:

:::{.callout}
```bash
cat *_variants.csv | cut -d "," -f 2 | sort
```

```
19B
19B
20A
20A
20A
20A
20A
20A
20A
20A

... (more output omitted) ...
```
:::

You can see that the output is now sorted alphabetically. 

The `cut` command is often used in conjunction with another command: `uniq`. 
This command returns the unique lines in its input. 
Importantly, _it only works as intended if the input is sorted_. 
That's why it's often used together with `sort`.  
Let's see it in action, by continuing building our command: 

:::{.callout}
```bash
cat *_variants.csv | cut -d "," -f 2 | sort | uniq
```

```
19B
20A
20B
20C
20E (EU1)
20I (Alpha; V1)
21A (Delta)
21I (Delta)
21J (Delta)
21K (Omicron)
21L (Omicron)
21M (Omicron)
NA
clade
```
:::

We can see that now the output is de-duplicated, so only unique values are returned. 
And so, with a few simple commands, we've answered a very useful question from our data: what are the unique variants in our collection of samples?

![Illustration of the `sort` + `uniq` commands by [Julia Evans](https://wizardzines.com/comics/sort-uniq/)](https://wizardzines.com/comics/sort-uniq/sort-uniq.png)


:::::{.callout-important icon=false}
## ***Exercise:*** Sort & Count I

Let's continue working on our command: 

```bash
cat *_variants.csv | cut -d "," -f 2 | sort | uniq
```

As you saw, this output also returns a line called "clade". 
This was part of the header (or column name) of our CSV file, which is not really useful to have in our output.  
Let's try and solve that problem, and also ask the question of how frequent each of these variants are in our data. 

1. Looking at the help page for `grep` (`grep --help` or `man grep`), see if you can find an option to invert a match, i.e. to return the lines that _do not match_ a pattern. 
  Can you think of how to include this in our pipeline to remove that line from our output?
  <details><summary>Hint</summary>
  The option to invert a match with `grep` is `-v`. Using one of our previous examples, `grep -v "Alpha"` would return the lines that _do not match_ the word "Alpha".
  </details>
2. The `uniq` command has an option called `-c`. 
  Try adding that option to the command and infer what it does (or look at `uniq --help`).
3. Finally, produce a sorted table of counts for each of our variants in _descending order_ (the most common variant at the top). 
  <details><summary>Hint</summary>
  The `sort` command has an option to order the output in _reverse order_: `-r`.
  </details>

:::{.callout collapse="true"}
## ***Solution:***

**Task 1**

Looking at the help of this function with `grep --help`, we can find the following option: 

```
 -v, --invert-match        select non-matching lines
```

So, we can continue working on our pipeline by adding another step at the end: 

:::{.callout}
```bash
cat *_variants.csv | cut -d "," -f 2 | sort | uniq | grep -v "clade"
```

```
19B
20A
20B
20C
20E (EU1)
20I (Alpha; V1)
21A (Delta)
21I (Delta)
21J (Delta)
21K (Omicron)
21L (Omicron)
21M (Omicron)
NA
```
:::

This now removes the line that matched the word "clade". 

----

**Task 2**

Looking at the help for `uniq --help`, we can see that:

```
 -c, --count           prefix lines by the number of occurrences
```

So, if we add this option, we will get the count of how many times each unique line appears in our data: 

:::{.callout}
```bash
cat *_variants.csv | cut -d "," -f 2 | sort | uniq -c | grep -v "clade"
```

```
 2 19B
30 20A
 8 20B
 1 20C
 1 20E (EU1)
38 20I (Alpha; V1)
 8 21A (Delta)
 1 21I (Delta)
66 21J (Delta)
87 21K (Omicron)
 4 21L (Omicron)
 2 21M (Omicron)
 3 NA
```
:::

----

**Task 3**

Now that we've counted each of our variants, we can again sort this result, this time by the counts value, by adding another `sort` step at the end: 

:::{.callout}
```bash
cat *_variants.csv | cut -d "," -f 2 | sort | uniq -c | grep -v "clade" | sort -r
```

```
87 21K (Omicron)
66 21J (Delta)
38 20I (Alpha; V1)
30 20A
 8 21A (Delta)
 8 20B
 4 21L (Omicron)
 3 NA
 2 21M (Omicron)
 2 19B
 1 21I (Delta)
 1 20E (EU1)
 1 20C
```
:::

We used the option `-r`, which from the help page `sort --help`, says: 

```
  -r, --reverse               reverse the result of comparisons
```
:::
:::::


:::::{.callout-important icon=false}
## ***Exercise:*** Sort & Count II

**This is an (optional) advanced exercise.**

In the `sequencing/` directory, you will find a file named `gene_annotation.gtf.gz`. 
This is a file containing the names and locations of genes in the Human genome in a standard text-based format called GTF.  
This is a tab-delimited file, where the 3rd column contains information about the kind of annotated feature (e.g. a gene, an exon, start codon, etc.). 

Using a combination of the commands we've seen so far:

1. Count how many occurrences of each feature (3rd column) there is in this file. 
2. How many transcripts does the gene "ENSG00000113643" have?

<details><summary>Hint</summary>

- Start by investigating the content of the file with `zcat gene_annotation.gtf.gz | less -S`. 
  You will notice the first few lines of the file contain comments starting with `#` symbol. 
  You should remove these lines before continuing. 
- Check the help for `grep` to remind yourself what the option is to return lines _not_ matching a pattern.
- Remember that the `cut` program uses tab as its default delimiter. 

</details>

:::{.callout collapse="true"}
## ***Solution:***

**Task 1**

We could use:

:::{.callout}
```bash
zcat gene_annotation.gtf.gz | grep -v "#" | cut -f 3 | sort | uniq -c
```

```
 871196 CDS
    119 Selenocysteine
1624585 exon
 171504 five_prime_utr
  61860 gene
  96934 start_codon
  90724 stop_codon
 203201 three_prime_utr
 251121 transcript
```
:::

- We use `zcat` because the file is compressed (we can tell from its extension ending with `.gz`). 
- We use `grep` to remove the first few lines of the file that start with `#` character. 
- We use `cut` to extract the third "field" (column) of the file. Because it's a tab-delimited file, we don't need to specify a delimiter with `cut`, as that is the default.
- We use `sort` to order the features in alphabetical order.
- Finally, `uniq` is used to return the unique values as well as count their occurrence (with the `-c` option). 

----

**Task 2**

The answer is 10. We could use the following command:

```bash
zcat gene_annotation.gtf.gz | grep "ENSG00000113643" | cut -f 3 | grep "transcript" | wc -l
```

:::
:::::


::: {.callout-note collapse=true}
#### `sort`: Alphabetically or Numerically?

Note that, by default the `sort` command will order input lines _alphabetically_. 
So, for example, if it received this as input: 

```
10
2
1
20
```

The result of sorting would be:

```
1
10
2
20
```

Because that's the alphabetical order of those characters. 
We can use the option `sort -n` to make sure it sorts these as numbers, in which case the output would be as expected: 

```
1
2
10
20
```

But, you may be asking yourself: why did it work with the output of `uniq` without specifying `-n`?  
This is because the output of `uniq` left-aligns all the numbers by prefixing the smaller numbers with a space, such as this:

```
10
 2
 1
20
```

And because the _space_ character comes first in the computer's "alphabet", we don't actually need to use the `-n` option.  

Here's the main message: **always use the `-n` option if you want things that look like numbers to be sorted numerically** (if the input doesn't look like a number, then `sort` will just order them alphabetically instead).

:::


## Credit
Information on this page has been adapted and modified from the following source(s):

- *Gabriel A. Devenyi (Ed.), Gerard Capes (Ed.), Colin Morris (Ed.), Will Pitchers (Ed.),Greg Wilson, Gerard Capes, Gabriel A. Devenyi, Christina Koch, Raniere Silva, Ashwin Srinath, … Vikram Chhatre. (2019, July). swcarpentry/shell-novice: Software Carpentry: the UNIX shell, June 2019 (Version v2019.06.1).*

- https://github.com/cambiotraining/UnixIntro

- https://github.com/cambiotraining/unix-shell
