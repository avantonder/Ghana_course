[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Bacterial Genomics",
    "section": "",
    "text": "Introduction to Bacterial Genomics\n\n\n\n\nThis course will teach you how to begin analysing whole genome sequencing data from bacterial isolates. We will introduce many of the software tools commonly used when analysing bacterial genomes and provide an outline of workflows such as quality control, short read mapping and phylogenetic tree inference. Along the way, you will gain foundational bioinformatic skills, including the use of the Unix command line and learn to write simple scripts to ensure your analysis is reproducible.\n\n\n\n\n\n\nLearning Objectives:\n\n\n\nBy the end of this course, learners should be able to:\n\nUnderstand what the main technologies used for high throughput sequencing are.\nUse the Unix command line to navigate a filesystem, manipulate files, launch programs and write scripts for reproducible analysis.\nRecognise the structure of common sequence file formats in bioinformatics and run basic quality control tools on them.\nUse mapping tools to map short read sequence data to a reference genome, call variants and visualize the results of the mapping process.\nUnderstand how to create a de novo assembly, assess the quality of the assembly and annotate the genome using a pre-compiled database of annotations.\nManage and install bioinformatics software using Conda and set up Nextflow workflows.\nUse Nextflow pipelines from the nf-core community and map sequence data from several isolates to a reference genome with the bactmap pipeline.\nUnderstand the different approaches to constructing a phylogenetic tree as well build and visualize a phylogeny.\nRun different genotyping tools to predict characteristics such as serotypes and antimicrobial resistance.\nUnderstand what high performance computing is and how this can be used to analyse large numbers of genomes efficiently and quickly.\n\n\n\n\n\nThis course is aimed at life scientists interested in the bioinformatic analysis of bacterial genomes.\n\n\n\nWe assume no prior bioinformatics experience or experience with the tools introduced in this course. An elementary knowledge of molecular and bacterial biology is assumed (concepts such as: DNA, RNA, SNPs).\n\n\n\n\nAbout the authors:\n\nAndries van Tonder  \nAffiliation: Department of Veterinary Medicine, University of Cambridge\nRoles: writing - original draft; conceptualisation; coding\nPrince Asare  \nAffiliation: Noguchi Memorial Institute for Medical Research, University of Ghana\nRoles: writing - original draft; conceptualisation; coding\n\n\n\n\nYou can cite these materials as:\nAsare P and van Tonder A. (2023) “Introduction to bacterial genomics”, https://avantonder.github.io/Ghana_course/\nOr in BibTeX format:\n@Misc{,\n  author = {Asare, Prince and van Tonder, Andries},\n  title = {avantonder.github.io/Ghana_course/: Introduction to bacterial genomics},\n  month = {January},\n  year = {2023},\n  url = {https://avantonder.github.io/Ghana_course/},\n}\n\n\n\nThese materials have been developed as a collaboration between the Department of Veterinary Medicine at the University of Cambridge and the Noguchi Memorial Institute for Medical Research at the University of Ghana.\nWe also thank the wider community for publicly sharing training resources, including:\n\nThe excellent training courses put together by University of Cambridge Bioinformatics Training Facility\nThe Carpentries project"
  },
  {
    "objectID": "setup.html",
    "href": "setup.html",
    "title": "Setup",
    "section": "",
    "text": "Workshop Attendees\n\n\n\nIf you are attending one of our workshops, we will provide a training environment with all of the required software and data.\nIf you want to setup your own computer to run the analysis demonstrated on this course, you can follow the instructions below.\nNote that we use tabsets to provide instructions for all three major operating systems. However, as much as possible we advice you use a Linux system, as our training environment is built on that."
  },
  {
    "objectID": "setup.html#installing-conda",
    "href": "setup.html#installing-conda",
    "title": "Setup",
    "section": "Installing conda",
    "text": "Installing conda\nWe will perform a fresh installation of the conda package using the miniconda installation option.\n\n\n\n\n\n\nNote\n\n\n\nIf you already have Miniconda or Anaconda installed, and you just want to upgrade, you should not proceed to making a fresh installation. Just use conda update to update your existing version of conda.\nconda update conda\nAfter updatiing conda, you can proceed to the instructions from number 8 to install mamba into the base environment from the conda-forge channel.\n\n\n\nWindowsMac OSLinux\n\n\nFollow this link to install miniconda and this link to install mamba on your windows system.\n\n\n\nOpen a terminal and follow the following instructions:\n\nNavigate to your home directory:\n\ncd ~\n\nDownload the Miniconda3 installer for mac by running:\n\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-x86_64.sh\n\n\n\n\n\n\nM processor users\n\n\n\nFor M1 processor users, you will need to run the below command:\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-arm64.sh\n\n\n\nRun the installation script just downloaded:\n\nbash Miniconda3-latest-MacOSX-x86_64.sh\n\nFollow the installation instructions accepting default options (answering ‘yes’ to any questions)\n\n\nIf you are unsure about any setting, accept the defaults. You can change them later.\n\n\nTo make the changes take effect, close and then re-open your terminal window.\nTest your installation.\n\n\nIn your terminal window, run the command conda list:\n\nconda list\n\nA list of installed packages appears if it has been installed correctly.\n\n\nRemove the installation script as it is no longer needed if successfully installed:\n\nrm Miniconda3-latest-MacOSX-x86_64.sh\n\nRun the following command to add channels:\n\nconda config --add channels defaults; conda config --add channels bioconda; conda config --add channels conda-forge; conda config --set channel_priority strict\nThis adds two channels (sources of software) useful for bioinformatics and data science applications.\n\nInstall Mamba into the base environment from the conda-forge channel with the below command:\n\nconda install mamba -n base -c conda-forge\n\nRun this to initiate mamba:\n\nmamba init\n\n\nOpen a terminal and follow the following instructions:\n\nNavigate to your home directory:\n\ncd ~\n\nDownload the Miniconda3 installer for Linux by running:\n\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n\nRun the installation script just downloaded:\n\nbash Miniconda3-latest-Linux-x86_64.sh\n\nFollow the installation instructions accepting default options (answering ‘yes’ to any questions)\n\n\nIf you are unsure about any setting, accept the defaults. You can change them later.\n\n\nTo make the changes take effect, close and then re-open your terminal window.\nTest your installation.\n\n\nIn your terminal window, run the command conda list:\n\nconda list\n\nA list of installed packages appears if it has been installed correctly.\n\n\nRemove the installation script as it is no longer needed if successfully installed:\n\nrm Miniconda3-latest-Linux-x86_64.sh\n\nRun the following command to add channels:\n\nconda config --add channels defaults; conda config --add channels bioconda; conda config --add channels conda-forge; conda config --set channel_priority strict\nThis adds two channels (sources of software) useful for bioinformatics and data science applications.\n\nInstall Mamba into the base environment from the conda-forge channel with the below command:\n\nconda install mamba -n base -c conda-forge\n\nRun this to initiate mamba:\n\nmamba init"
  },
  {
    "objectID": "setup.html#creating-conda-environments-for-the-workshop",
    "href": "setup.html#creating-conda-environments-for-the-workshop",
    "title": "Setup",
    "section": "Creating conda environments for the workshop",
    "text": "Creating conda environments for the workshop\n\nWindowsMac OSLinux\n\n\nContent will soon be uploaded.\n\n\n\n\n\n\n\n\n\ncreating the qc environment and installing required packages\n\n\n\nOpen a terminal, make sure you are in the conda base environment and run this command to install all required packages and their dependencies:\nmamba create -n qc fastq-scan=1.0.0 fastqc=0.11.9 fastp=0.23.2 kraken2=2.1.2 bracken=2.7 multiqc=1.13a\nThis creates an environment called qc with the specified package versions and their dependencies.\nNB. The tools fastq-scan and bracken runs python scripts which require python libraries pandas, json, glob.\nUse the below command to install the packages in the qc environment:\nconda install pandas=1.4.3 -n qc -c conda-forge \nWe will activate and use this environment in chapter 4 — Sequencing Quality Control.\n\n\n\n\n\n\n\n\ncreating the mapping environment and installing required packages\n\n\n\nrun this command to install all required packages and their dependencies:\nmamba create -n mapping bwa=0.7.17 samtools=1.15 bcftools=1.14 pysam=0.16.0.1 biopython=1.78\nThis creates an environment called mapping with the specified package versions and their dependencies.\nWe will activate and use this environment in chapter 5 — Short Read Mapping.\n\n\n\n\n\n\n\n\nInstalling required packages for Assembly and Annotation\n\n\n\nNB. For the Assembly and Annotation module, we will create three different environments because there are conflicts in the conda recipes and it’ll be tricky to get all the tools working in a single environment.\nWe will thus, create each environment seperately with the following names:\nmamba create -n shovill -c bioconda shovill=1.1.0 \n\nmamba create -n quast -c bioconda quast=5.2.0 \n\nmamba create -n bakta -c bioconda bakta=1.6.1\nWe will activate and use these environments in Chapter 6 — Assembly and Annotation.\n\n\n\n\n\n\n\n\ncreating the phylogenetics environment and installing required packages\n\n\n\nrun this command to install all required packages and their dependencies:\nmamba create -n phylogenetics -c bioconda iqtree=2.2.0.3 snp-sites=2.5.1\nThis creates an environment called phylogenetics with the specified package versions and their dependencies.\nWe will activate and use this environment in Chapter 10 — Introduction to Phylogenetics.\n\n\n\n\n\n\n\n\ncreating the genotyping environments and installing required packages\n\n\n\nNB. For the genotyping and AMR prediction, we will create five different environments because some tools require specific versions of python and other related packages hence we cannot install all the packages in a single environment.\nWe will thus, create each environment seperately with the following names:\n\nmlst\nseroba\nspoligotyping\ntbprofiler\nariba\n\nrun the following commands to create the specified environment and install all required packages and their dependencies for:\n\nmlst:\n\nmamba create -n mlst mlst=2.22.1\n\nseroba:\n\nmamba create -n seroba seroba=1.0.2\n\nspoligotyping:\n\nmamba create -n spoligotyping spotyping=2.1\n\ntbprofiler:\n\nmamba create -n tbprofiler tb-profiler=4.1.1\n\nariba:\n\nmamba create -n ariba ariba=2.14.6\nThese create the specified environment names mlst, seroba, spoligotyping, tbprofiler and ariba with the specified package versions and their dependencies.\nWe will activate and use these environments in chapter 11 — Bacterial Genotyping and Drug Resistance Prediction.\n\n\n\n\n\n\n\n\nSpecify version of toool to install\n\n\n\nAs you may see, all the tools installed have specified version numbers added to the tool names in the format tool=version_numer. This allows us to install the exact version of tools used for the training.\nFor your personal use, if you wish to use the latest version of these tools, just omit specifying the version z-version_number` and the latest version of the tool will hopefully be installed.\n\n\n\n\n\n\n\n\n\n\ncreating the qc environment and installing required packages\n\n\n\nOpen a terminal, make sure you are in the conda base environment and run this command to install all required packages and their dependencies:\nmamba create -n qc fastq-scan=1.0.0 fastqc=0.11.9 fastp=0.23.2 kraken2=2.1.2 bracken=2.7 multiqc=1.13a\nThis creates an environment called qc with the specified package versions and their dependencies.\nNB. The tools fastq-scan and bracken runs python scripts which require python libraries pandas, json, glob.\nUse the below command to install the packages in the qc environment:\nconda install pandas -n qc -c conda-forge \nWe will activate and use this environment in chapter 4 — Sequencing Quality Control.\n\n\n\n\n\n\n\n\ncreating the mapping environment and installing required packages\n\n\n\nrun this command to install all required packages and their dependencies:\nmamba create -n mapping bwa=0.7.17 samtools=1.15 bcftools=1.14 pysam=0.16.0.1 biopython=1.78\nThis creates an environment called mapping with the specified package versions and their dependencies.\nNB. Creating the pseudogenomes step runs python scripts which require some python libraries.\nUse the below command to install the packages in the mapping environment:\nconda install pandas -n qc -c conda-forge \nWe will activate and use this environment in chapter 5 — Short Read Mapping.\n\n\n\n\n\n\n\n\ncreating the genotyping environment and installing required packages\n\n\n\nNB. For the genotyping and AMR prediction, we will create five different environments because some tools require specific versions of python and other related packages hence we cannot install all the packages in a single environment.\nWe will thus, create each environment seperately with the following names:\n\nmlst\nseroba\nspoligotyping\ntbprofiler\nariba\n\nrun the following commands to create the specified environment and install all required packages and their dependencies for:\n\nmlst:\n\nmamba create -n mlst mlst=2.22.1\n\nseroba:\n\nmamba create -n seroba seroba=1.0.2\n\nspoligotyping:\n\nmamba create -n spoligotyping spotyping=2.1\n\ntbprofiler:\n\nmamba create -n tbprofiler tb-profiler=4.1.1\n\nariba:\n\nmamba create -n ariba ariba=2.14.6\nThese create the specified environment names mlst, seroba, spoligotyping, tbprofiler and ariba with the specified package versions and their dependencies.\nWe will activate and use these environments in chapter 11 — Bacterial Genotyping and Drug Resistance Prediction.\n\n\n\n\n\n\n\n\nSpecify version of toool to install\n\n\n\nAs you may see, all the tools installed have specified version numbers added to the tool names in the format tool=version_numer. This allows us to install the exact version of tools used for the training.\nFor your personal use, if you wish to use the latest version of these tools, just omit specifying the version z-version_number` and the latest version of the tool will hopefully be installed."
  },
  {
    "objectID": "setup.html#downloading-databases",
    "href": "setup.html#downloading-databases",
    "title": "Setup",
    "section": "Downloading databases",
    "text": "Downloading databases\n\n\n\n\n\n\n\nminikraken2 database\n\n\n\nDownload the kracken database “minikraken2_v1_8GB” into the database directory:\nwget ftp://ftp.ccb.jhu.edu/pub/data/kraken2_dbs/old/minikraken2_v1_8GB_201904.tgz\nUncompress the database\ntar xvfz minikraken2_v1_8GB_201904.tgz \nIf the unzipped database is not same as the one use in the workshop, rename the it to match the workshop codes used using:\nmv <unzipped_database_name> minikraken2_v1_8GB\nYou can now remove the zipped downloaded database as it is no longer required\nrm minikraken2_v1_8GB_201904.tgz\n\n\n\n\n\n\n\n\nbakta database\n\n\n\nDownload the Bakta database “db.tar.gz” into the database directory and unzip.\n\n\n\n\n\n\none step\n\n\n\nIf you have the denove_assembly environment activated, you can perform this step.\nbakta_db download --output <output-path> \nIf you use this option, you don’t need to perform the AMRFinderPlus step as the AMR-DB will be included automatically.\n\n\nwget https://bakta-db.s3.computational.bio.uni-giessen.de/db.tar.gz\nor\nwget https://zenodo.org/record/7025248/files/db.tar.gz\nUncompress the database\ntar -xzf db.tar.gz\nRename the database to match the workshop codes used\nmv db bakta_db\nDelete zipped file after unzipping\nrm db.tar.gz\nDownload the AMRFinderPlus database\namrfinder_update --force_update --database bakta_db/amrfinderplus-db/\nUpdating an existing bakta database\nbakta_db update --db <existing-db-path> [--tmp-dir <tmp-directory>]\n\n\n\n\n\n\n\n\nseroba database\n\n\n\nFor git users, navigate to your database directory and clone the git repository:\ngit clone https://github.com/sanger-pathogens/seroba.git\nCopy the database from the seroba/ to your database directory — this should be your current directory:\ncp -r seroba/database .\nDelete the git repository to clean up your system:\nrm -r seroba\nStill in your database directory, rename the database to match the workshop codes used:\nmv database seroba_db"
  },
  {
    "objectID": "setup.html#nextflow",
    "href": "setup.html#nextflow",
    "title": "Setup",
    "section": "Nextflow",
    "text": "Nextflow\n\nWindowsMac OSLinux"
  },
  {
    "objectID": "setup.html#singularity",
    "href": "setup.html#singularity",
    "title": "Setup",
    "section": "Singularity",
    "text": "Singularity\n\nWindowsMac OSLinux\n\n\nYou can use Singularity from the Windows Subsystem for Linux (see @wsl).\nOnce you setup WSL, you can follow the instructions for Linux.\n\n\nSingularity is not available for Mac OS.\n\n\nThese instructions are for Ubuntu or Debian-based distributions1.\nsudo apt update && sudo apt upgrade && sudo apt install runc\nCODENAME=$(lsb_release -c | sed 's/Codename:\\t//')\nwget -O singularity.deb https://github.com/sylabs/singularity/releases/download/v3.10.2/singularity-ce_3.10.2-${CODENAME}_amd64.deb\nsudo dpkg -i singularity.deb\nrm singularity.deb"
  },
  {
    "objectID": "setup.html#visual-studio-code",
    "href": "setup.html#visual-studio-code",
    "title": "Setup",
    "section": "Visual Studio Code",
    "text": "Visual Studio Code\n\nWindowsMac OSLinux (Ubuntu)\n\n\n\nGo to the Visual Studio Code download page and download the installer for your operating system. Double-click the downloaded file to install the software, accepting all the default options.\nAfter completing the installation, go to your Windows Menu, search for “Visual Studio Code” and launch the application.\nGo to “File > Preferences > Settings”, then select “Text Editor > Files” on the drop-down menu on the left. Scroll down to the section named “EOL” and choose “\\n” (this will ensure that the files you edit on Windows are compatible with the Linux operating system).\n\n\n\n\nGo to the Visual Studio Code download page and download the installer for Mac.\nGo to the Downloads folder and double-click the file you just downloaded to extract the application. Drag-and-drop the “Visual Studio Code” file to your “Applications” folder.\nYou can now open the installed application to check that it was installed successfully (the first time you launch the application you will get a warning that this is an application downloaded from the internet - you can go ahead and click “Open”).\n\n\n\n\nGo to the Visual Studio Code download page and download the installer for your Linux distribution. Install the package using your system’s installer."
  },
  {
    "objectID": "setup.html#r-and-rstudio",
    "href": "setup.html#r-and-rstudio",
    "title": "Setup",
    "section": "R and RStudio",
    "text": "R and RStudio\n\nWindowsMac OSLinux\n\n\nDownload and install all these using default options:\n\nR\nRTools\nRStudio\n\n\n\nDownload and install all these using default options:\n\nR\nRStudio\n\n\n\n\nGo to the R installation folder and look at the instructions for your distribution.\nDownload the RStudio installer for your distribution and install it using your package manager."
  },
  {
    "objectID": "setup.html#workshop-data",
    "href": "setup.html#workshop-data",
    "title": "Setup",
    "section": "Workshop Data",
    "text": "Workshop Data"
  },
  {
    "objectID": "materials.html",
    "href": "materials.html",
    "title": "Materials",
    "section": "",
    "text": "location of materials and sub-material\n\n\n\n\n\n\n\n\n← The materials are located on the left navigation pane\n\nYou can navigate to the various sub-material within each material in the right navigation pane→"
  },
  {
    "objectID": "materials/01-intro_wgs/1.1-intro_wgs.html",
    "href": "materials/01-intro_wgs/1.1-intro_wgs.html",
    "title": "1.1 Introduction to Whole Genome Sequencing (WGS)",
    "section": "",
    "text": "Teaching: 20 min || Exercises: 15 min"
  },
  {
    "objectID": "materials/01-intro_wgs/1.1-intro_wgs.html#overview",
    "href": "materials/01-intro_wgs/1.1-intro_wgs.html#overview",
    "title": "1.1 Introduction to Whole Genome Sequencing (WGS)",
    "section": "Overview",
    "text": "Overview\n\n\n\n\n\n\n\n\n\n\n\n\nQuestions:\n\n\n\n\nWhat are the main technologies available for high-throughput sequencing?\nWhat are some of the common file formats used in bioinformatics?\nHow do I assess the quality of Illumina sequencing data?\n\n\n\n\n\n\n\n\n\nLearning Objectives:\n\n\n\n\nDescribe differences between sequencing data produced by Illumina and Nanopore platforms.\nRecognise the structure of common file formats in bioinformatics, in particular FASTA and FASTQ files.\nTo introduce participants to the use of FastQC and MulitQC to produce quality reports for Illumina sequences.\n\n\n\n\n\n\n\n\n\nKey Points:\n\n\n\n\nIllumina sequencing produces short reads (50bp - 200bp), typically from both ends of a DNA fragment. It is a comparatively cheap sequencing platform which produces very high-quality sequences.\nNanopore sequencing produces very long reads (typically hundreds of kilobases long). It is comparatively more expensive and has higher error rates. However, it is more flexible with some of its platforms being fully portable.\nSequencing reads are stored in a file format called FASTQ. This file contains both the nucleotide sequence and quality of each base.\nThe quality of Illumina sequence reads can be assessed using the software FastQC.\nOne common task in bioinformatics is to align or map reads to a reference genome. This involves:\n\nCreating a genome index - this only needs to be done once.\nMapping the reads to the reference genome (e.g. using bowtie2) - the output is in SAM format.\nSorting the reads in the mapped file (using samtools sort) - the output is in BAM format.\nIndexing the BAM alignment file (using samtools index).\n\nThe software MultiQC can be used to generate a single report that compiles statistics across several samples.\nBioinformatics uses many standard file formats. One of the most common ones is the FASTA format, which is used to store nucleotide or amino acid sequences (no quality information is contained in these files). This is a standard format that assembled genomes are stored as."
  },
  {
    "objectID": "materials/01-intro_wgs/1.1-intro_wgs.html#next-generation-sequencing",
    "href": "materials/01-intro_wgs/1.1-intro_wgs.html#next-generation-sequencing",
    "title": "1.1 Introduction to Whole Genome Sequencing (WGS)",
    "section": "1.1.1 Next Generation Sequencing",
    "text": "1.1.1 Next Generation Sequencing\nThe sequencing of genomes has become more routine due to the rapid drop in DNA sequencing costs seen since the development of Next Generation Sequencing (NGS) technologies in 2007. One main feature of these technologies is that they are high-throughput, allowing one to more fully characterise the genetic material in a sample of interest.\nThere are three main technologies in use nowadays, often referred to as 2nd and 3rd generation sequencing:\n\nIllumina’s sequencing by synthesis (2nd generation)\nOxford Nanopore, shortened ONT (3rd generation)\nPacific Biosciences, shortened PacBio (3rd generation)\n\nThe video below from the iBiology team gives a great overview of these technologies.\n\n\n\n\n\nIllumina Sequencing\nIllumina’s technology has become a widely popular method, with many applications to study transcriptomes (RNA-seq), epigenomes (ATAC-seq, BS-seq), DNA-protein interactions (ChIP-seq), chromatin conformation (Hi-C/3C-Seq), population and quantitative genetics (variant detection, GWAS), de-novo genome assembly, amongst many others.\nAn overview of the sequencing procedure is shown in the animation video below. Generally, samples are processed to generate so-called sequencing libraries, where the genetic material (DNA or RNA) is processed to generate fragments of DNA with attached oligo adapters necessary for the sequencing procedure (if the starting material is RNA, it can be converted to DNA by a step of reverse transcription). Each of these DNA molecule is then sequenced from both ends, generating pairs of sequences from each molecule, i.e. paired-end sequencing (single-end sequencing, where the molecule is only sequenced from one end is also possible, although much less common nowadays).\nThis technology is a type of short-read sequencing, because we only obtain short sequences from the original DNA molecules. Typical protocols will generate 2x50bp to 2x250bp sequences (the 2x denotes that we sequence from each end of the molecule).\n\n\n\n\nThe main advantage of Illumina sequencing is that it produces very high-quality sequence reads (current protocols generate reads with an error rate of less than <1%) at a low cost. However, the fact that we only get relatively short sequences means that there are limitations when it comes to resolving particular problems such as long sequence repeats (e.g. around centromeres or transposon-rich areas of the genome), distinguishing gene isoforms (in RNA-seq), or resolving haplotypes (combinations of variants in each copy of an individual’s diploid genome).\n\n\nNanopore Sequencing\nNanopore sequencing is a type of long-read sequencing technology. The main advantage of this technology is that it can sequence very long DNA molecules (up to megabase-sized), thus overcoming the main shortcoming of short-read sequencing mentioned above. Another big advantage of this technology is its portability, with some of its devices designed to work via USB plugged to a standard laptop. This makes it an ideal technology to use in situations where it is not possible to equip a dedicated sequencing facility/laboratory (for example, when doing field work).\n\n\n\nOverview of Nanopore sequencing showing the highly-portable MinION device. The device contains thousands of nanopores embedded in a membrane where current is applied. As individual DNA molecules pass through these nanopores they cause changes in this current, which is detected by sensors and read by a dedicated computer program. Each DNA base causes different changes in the current, allowing the software to convert this signal into base calls.\n\n\nOne of the bigger challenges in effectively using this technology is to produce sequencing libraries that contain high molecular weight, intact, DNA. Another disadvantage is that, compared to Illumina sequencing, the error rates at higher, at around 5%.\n\n\n\n\n\n\nNote\n\n\n\nIllumina or Nanopore for Bacteria sequencing?\nBoth of these platforms have been widely popular for sequencing bacterial pathogens. They can both generate data with high-enough quality for the assembly and analysis of bacterial genomes. PacBio is also used for sequencing Bacterial genomes but we may not have time to talk about it here. Mostly, which one you use will depend on what sequencing facilities you have access to.\nWhile Illumina provides the cheapest option per sample of the two, it has a higher setup cost, requiring access to the expensive sequencing machines. On the other hand, Nanopore is a very flexible platform, especially its portable MinION devices. They require less up-front cost allowing getting started with sequencing very quickly in a standard molecular biology lab."
  },
  {
    "objectID": "materials/01-intro_wgs/1.1-intro_wgs.html#sequencing-analysis",
    "href": "materials/01-intro_wgs/1.1-intro_wgs.html#sequencing-analysis",
    "title": "1.1 Introduction to Whole Genome Sequencing (WGS)",
    "section": "1.1.2 Sequencing Analysis",
    "text": "1.1.2 Sequencing Analysis\nIn this section we will demonstrate two common tasks in sequencing data analysis: sequence quality control and mapping to a reference genome. There are many other tasks involved in analysing sequencing data, but looking at these two examples will demonstrate the principles of running bioinformatic programs. We will later see how bioinformaticians can automate more complex analyses in the Sequencing Quality Control and Short Read Mapping sections.\nOne of the main features in bioinformatic analysis is the use of standard file formats. It allows software developers to create tools that work well with each other. For example, the raw data from Illumina and Nanopore platforms is very different: Illumina generates images; Nanopore generates electrical current signal. However, both platforms come with software that converts those raw data to a standard text-based format called FASTQ. We will briefly describe a couple of these file formats here, but will talk about them in more details in a later session – Day 2 – Common File Formats\n\nFASTQ Files\nFASTQ files are used to store nucleotide sequences along with a quality score for each nucleotide of the sequence. These files are the typical format obtained from NGS sequencing platforms such as Illumina and Nanopore (after base calling).\nThe file format is as follows:\n@SEQ_ID                   <-- SEQUENCE NAME\nAGCGTGTACTGTGCATGTCGATG   <-- SEQUENCE\n+                         <-- SEPARATOR\n%%).1***-+*''))**55CCFF   <-- QUALITY SCORES\nIn FASTQ files each sequence is always represented across four lines. The quality scores are encoded in a compact form, using a single character. They represent a score that can vary between 0 and 40 (see Illumina’s Quality Score Encoding). The reason single characters are used to encode the quality scores is that it saves space when storing these large files. Software that work on FASTQ files automatically convert these characters into their score, so we don’t have to worry about doing this conversion ourselves.\nThe quality value in common use is called a Phred score and it represents the probability that the respective base is an error. For example, a base with quality 20 has a probability \\(10^{-2} = 0.01 = 1\\%\\) of being an error. A base with quality 30 has \\(10^{-3} = 0.001 = 0.1\\%\\) chance of being an error. Typically, a Phred score threshold of >20 or >30 is used when applying quality filters to sequencing reads.\nBecause FASTQ files tend to be quite large, they are often compressed to save space. The most common compression format is called gzip and uses the extension .gz. To look at a gzip file, we can use the command zcat, which decompresses the file and prints the output as text.\nFor example, we can use the following command to count the number of lines in a compressed FASTQ file:\n\n\n\n\n\n\nCounting lines of a fastq file\n\n\n\nFirst navigate to the working directory\ncd ~/Desktop/workshop_files_Bact_Genomics_2023/01_intro_WGS/\nNow, copy the below code (using the copy link to the right of the code), paste in your terminal and hit Enter to execute it. Note that it may take a few seconds to run as this is a large file.\nzcat G26832_R1.fastq.gz | wc -l\n12099628\n\n\nIf we want to know how many sequences there are in the file, we can divide the result by 4 (since each sequence is always represented across four lines).\n\n\nFASTQ Quality Control\nOne of the most basic tasks in Illumina sequence analysis is to run a quality control step on the FASTQ files we obtained from the sequencing machine.\nThe program used to assess FASTQ quality is called FastQC. It produces several statistics and graphs for each file in a nice html report that can be used to identify any quality issues with our sequences.\n\n\n\nFastQC quality report\n\n\nWe will explore more on FASTQC later.\n\n\n\n\n\n\nExercise 1.1.2.1: Other files in the ìntro_WGS directory\n\n\n\nIn the course materials directory ~/Desktop/workshop_files_Bact_Genomics_2023/01_intro_WGS we have other files in addition to the FASTQ.\n\nCan you tell how many files we have in there using the terminal?\nGo ahead and list the file names.\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n\nFirst navigate to the working directory\ncd ~/Desktop/workshop_files_Bact_Genomics_2023/01_intro_WGS/\nThen use the list ls command to list the files there\nls\nG26832_R1.fastq.gz  MTB_H37Rv.fasta\n\n\n\n\n\n\nQuality Control Nanopore Reads\nAlthough FastQC can run its analysis on any FASTQ files, it has mostly been designed for Illumina data. You can still run FastQC on base-called Nanopore data, but some of the output modules may not be as informative. FastQC can also run on FAST5 files, using the option --nano.\nYou can also use MinIONQC, which takes as input the sequence_summary.txt file, which is a standard output file from the Guppy software used to convert Nanopore electrical signal to sequence calls.\n\n\n\nRead Mapping\nA common task in processing sequencing reads is to align them to a reference genome, which is typically referred to as read mapping or read alignment. We will continue exemplifying how this works for Illumina data, however the principle is similar for Nanopore data (although the software used is often different, due to the higher error rates and longer reads typical of these platforms).\nAgain, this is just an introduction, we will delve deeper into it in subsequent sessions.\nGenerally, these are the steps involved in read mapping (figure below):\n\nGenome Indexing | Because reference genomes can be quite long, most mapping algorithms require that the genome is pre-processed, which is called genome indexing. You can think of a genome index in a similar way to an index at the end of a textbook, which tells you in which pages of the book you can find certain keywords. Similarly, a genome index is used by mapping algorithms to quickly search through its sequence and find a good match with the reads it is trying to align against it. Each mapping software requires its own index, but we only have to generate the genome index once.\nRead mapping | This is the actual step of aligning the reads to a reference genome. There are different popular read mapping programs such as bowtie2 or bwa. The input to these programs includes the genome index (from the previous step) and the FASTQ file(s) with reads. The output is an alignment in a file format called SAM (text-based format - takes a lot of space) or BAM (compressed binary format - much smaller file size).\nBAM Sorting | The mapping programs output the sequencing reads in a random order (the order in which they were processed). But, for downstream analysis, it is good to sort the reads by their position in the genome, which makes it faster to process the file.\nBAM Indexing | This is similar to the genome indexing we mentioned above, but this time creating an index for the alignment file. This index is often required for downstream analysis and for visualising the alignment with programs such as IGV.\n\n\n\n\nDiagram illustrating the steps involved in mapping sequencing reads to a reference genome. Mapping programs allow some differences between the reads and the reference genome (red mutation shown as an example). Before doing the mapping, reads are usually filtered for high-quality and to remove any sequencing adapters. The reference genome is also indexed before running the mapping step. The mapped file (BAM format) can be used in many downstream analyses. See text for more details.\n\n\n\n\nQuality Reports\nWe’ve seen the example of using the program FastQC to assess the quality of our FASTQ sequencing files.\nWhat if we have so many FASTQ files and want to view their quality metrics at once.\nWhen processing multiple samples at once, it can become hard to check all of these quality metrics individually for each sample. This is the problem that the software MultiQC tries to solve. This software automatically scans a directory and looks for files it recognises as containing quality statistics. It then compiles all those statistics in a single report, so that we can more easily look across dozens or even hundreds of samples at once.\nLike FastQC, MultiQC also generates an html report. From this report we can get an overview of the quality across all our samples.\n\n\n\nSnapshot of some of the report sections from MultiQC. In this example we can see the “General Statistics” table and the “Sequence Quality Histograms” plot. One of the samples has lower quality towards the end of the read compared to other samples (red line in the bottom panel).\n\n\nFor example, from the section “General Statistics” we can see that the number of reads varies a lot between samples. Sample ERR5926784 has around 0.1 million reads, which is substantially lower than other samples that have over 1 million reads. This may affect the quality of the consensus assembly that we will do afterwards.\nFrom the section “Sequence Quality Histograms”, we can see that one sample in particular - ERR5926784 - has lower quality in the second pair of the read. We can open the original FastQC report and confirm that several sequences even drop below a quality score of 20 (1% change of error). A drop in sequencing quality towards the end of a read can often happen, especially for longer reads. Usually, analysis workflows include a step to remove reads with low quality so these should not affect downstream analysis too badly. However, it’s always good to make a note of potentially problematic samples, and see if they produce lower quality results downstream."
  },
  {
    "objectID": "materials/01-intro_wgs/1.1-intro_wgs.html#bioinformatics-file-formats",
    "href": "materials/01-intro_wgs/1.1-intro_wgs.html#bioinformatics-file-formats",
    "title": "1.1 Introduction to Whole Genome Sequencing (WGS)",
    "section": "1.1.3. Bioinformatics File Formats",
    "text": "1.1.3. Bioinformatics File Formats\nLike we said above, bioinformatics uses many standard file formats to store different types of data. We have just seen one of these file formats: FASTQ for sequencing reads.\nAnother very common file format is the FASTA file, which is the format that our reference genome is stored as. The consensus sequences that we will generate are also stored as FASTA files. We detail this format below, but we will explore other File formats in subsequent sessions.\n\nFASTA Files\nAnother very common file that we should consider is the FASTA format. FASTA files are used to store nucleotide or amino acid sequences.\nThe general structure of a FASTA file is illustrated below:\n>sample01                 <-- NAME OF THE SEQUENCE\nAGCGTGTACTGTGCATGTCGATG   <-- SEQUENCE ITSELF\nEach sequence is represented by a name, which always starts with the character >, followed by the actual sequence.\nA FASTA file can contain several sequences, for example:\n>sample01\nAGCGTGTACTGTGCATGTCGATG\n>sample02\nAGCGTGTACTGTGCATGTCGATG\nEach sequence can sometimes span multiple lines, and separate sequences can always be identified by the > character. For example, this contains the same sequences as above:\n>sample01      <-- FIRST SEQUENCE STARTS HERE\nAGCGTGTACTGT\nGCATGTCGATG\n>sample02      <-- SECOND SEQUENCE STARTS HERE\nAGCGTGTACTGT\nGCATGTCGATG\nTo count how many sequences there are in a FASTA file, we can use the following command:\ngrep \">\" sequences.fa | wc -l\n\n\n\n\n\n\nExercise 1.1.3.1: Printing the first 2 lines of a fasta file\n\n\n\nWe now know how many files are in our current working directory.\nCan you write a simple command to display the first two lines of the fasta file?\n\n\n\n\n\n\nSolution:\n\n\n\n\n\nRun this command to view the first two lines of the fasta file\nhead -n 2 MTB_H37Rv.fasta\n>NC_000962.3 Mycobacterium tuberculosis H37Rv, complete genome\nTTGACCGATGACCCCGGTTCAGGCTTCACCACAGTGTGGAACGCGGTCGTCTCCGAACTTAACGGCGACC\n\n\n\n\n\nWe will see FASTA files several times throughout this course, so it’s important to be familiar with them.\nIf you are new to the terminal and the command language, don’t worry, these exercises were designed to just wet your appetite.\nIn our next session, we will give you an introduction to the Unix Shell for you to appreciate it better."
  },
  {
    "objectID": "materials/01-intro_wgs/1.1-intro_wgs.html#credit",
    "href": "materials/01-intro_wgs/1.1-intro_wgs.html#credit",
    "title": "1.1 Introduction to Whole Genome Sequencing (WGS)",
    "section": "1.1.4 Credit",
    "text": "1.1.4 Credit\nInformation on this page has been adapted and modified from the following source:\n\nhttps://github.com/cambiotraining/sars-cov-2-genomics/blob/main/03-intro_ngs.md"
  },
  {
    "objectID": "materials/02-unix_intro/2.3-text_manipulation.html",
    "href": "materials/02-unix_intro/2.3-text_manipulation.html",
    "title": "2.3 Text Manipulation",
    "section": "",
    "text": "Teaching: 20 min || Exercises: 10 min"
  },
  {
    "objectID": "materials/02-unix_intro/2.3-text_manipulation.html#overview",
    "href": "materials/02-unix_intro/2.3-text_manipulation.html#overview",
    "title": "2.3 Text Manipulation",
    "section": "Overview",
    "text": "Overview\n\n\n\n\n\n\n\n\n\n\n\n\nQuestions:\n\n\n\n\nHow can I inspect the content of and manipulate text files?\nHow can I find and replace text patterns within files?\nWhat is an escape character?\nWhat are wildcards and how do I use them?\n\n\n\n\n\n\n\n\n\nLearning Objectives:\n\n\n\n\nInspect the content of text files (head, tail, cat, zcat, less).\nUse the * wildcard to work with multiple files at once.\nRedirect the output of a command to a file (>, >>).\nFind a pattern in a text file (grep) and do basic pattern replacement (sed).\nDemonstrate the usage of the sed and the “substitute” command s in processing text.\nDemonstrate on the use of the escape character \\\nDemonstrate on the use of wildcards ” ., * and ?” in manipulating text using sed.\n\n\n\n\n\n\n\n\n\nKey Points:\n\n\n\n\nThe head and tail commands can be used to look at the top or bottom of a file, respectively.\nThe less command can be used to interactively investigate the content of a file. Use ↑ and ↓ to browse the file and Q to quit and return to the console.\nThe cat command can be used to combine multiple files together. The zcat command can be used instead if the files are compressed.\nThe > operator redirects the output of a command into a file. If the file already exists, it’s content will be overwritten.\nThe >> operator also redirects the output of a command into a file, but appends it to any content that already exists.\nThe grep command can be used to find the lines in a text file that match a text pattern.\nThe sed tool can be used for advanced text manipulation. The “substitute” command can be used to text replacement: sed 's/pattern/replacement/options'.\nThe escape character \\ invokes an alternative interpretation of the following/next character usually *, ., [, ], ?, $, ^, /, \\.\nThe escape character \\ can also be used to provide visual representations of non-printing characters and characters that usually have special meanings. Eg. \\n: a newline., \\r: a carriage return., \\t: a horizontal tab.\nWildcards ” ., * and ?” are very useful characters used for manipulating text and they basically function as placeholders."
  },
  {
    "objectID": "materials/02-unix_intro/2.3-text_manipulation.html#looking-inside-files",
    "href": "materials/02-unix_intro/2.3-text_manipulation.html#looking-inside-files",
    "title": "2.3 Text Manipulation",
    "section": "2.3.1 Looking Inside Files",
    "text": "2.3.1 Looking Inside Files\nOften we want to investigate the content of a file, without having to open it in a text editor. This is especially useful if the file is very large (as is often the case in bioinformatic applications).\nFor example, let’s take a look at the file TBNmA041_annotation_truncated_1.gff3. We will start by printing the whole content of the file with the cat command, which stands for “concatenate” (we will see why it’s called this way in a little while):\nNavigate to the ~/Desktop/workshop_files_Bact_Genomics_2023/02_unix_intro directory and run the following command\ncat TBNmA041_annotation_truncated_1.gff3\nThis outputs a lot of text, because the file is quite long! Take a moment and scroll through from beginning to end. In fact, this file (as you may have realized from its name), is a truncated file, and you can imagine having to scroll through the entire length of the full file. You will later find out what file format this file is and better explore its contents.\nInstead of looking at the entire content of a file, it is often more useful to look only at the top few lines of the file. We can do this with the head command:\n\n\n\n\n\n\n\n\n\n\nhead TBNmA041_annotation_truncated_1.gff3\n##gff-version 3\n##feature-ontology https://github.com/The-Sequence-Ontology/SO-Ontologies/blob/v3.1/so.obo\n# Annotated with Bakta\n# Software: v1.5.0\n# Database: v4.0\n# DOI: 10.1099/mgen.0.000685\n# URL: github.com/oschwengers/bakta\n##sequence-region contig_1 1 321788\ncontig_1    Bakta   region  1   321788  .   +   .   ID=contig_1;Name=contig_1\ncontig_1    Prodigal    CDS 15  272 .   +   0   ID=BDKKDL_00005;Name=hypothetical protein;...\n\n\nBy default, head prints the first 10 lines of the file. We can change this using the -n option, followed by a number, for example:\n\n\n\n\n\n\nPrint out first 3 lines\n\n\n\nhead -n 3 TBNmA041_annotation_truncated_1.gff3\n##gff-version 3\n##feature-ontology https://github.com/The-Sequence-Ontology/SO-Ontologies/blob/v3.1/so.obo\n# Annotated with Bakta\n\n\nSimilarly, we can look at the bottom few lines of a file with the tail command:\n\n\n\n\n\n\nPrint out last 3 lines\n\n\n\ntail -n 3 TBNmA041_annotation_truncated_1.gff3\n>contig_148\nGAGTTACGTCCAGGGGTGTGGTGTACGGGCAGGTAAGGCCGGTGGGCGTGTCGTAGCCCA\nGTAGTGGGCGGTCATCGCGTGATCCTTCGAAACGACCAGCAAAAGTCAATCG\n\n\nFinally, if we want to open the file and browse through it, we can use the less command:\nless TBNmA041_annotation_truncated_1.gff3\nless will open the file and you can use ↑ and ↓ to move line-by-line or the Page Up and Page Down keys to move page-by-page. You can also use the Space Bar to navigate in bits. You can exit less by pressing Q (for “quit”). This will bring you back to the console.\nFinally, it can sometimes be useful to count how many lines, words and characters a file has. We can use the wc command for this:\n\n\n\n\n\n\nwc TBNmA041_annotation_truncated*\n     617    1519   37336 TBNmA041_annotation_truncated_1.gff3\n     722    1740   53321 TBNmA041_annotation_truncated_2.gff3\n    1339    3259   90657 total\n\n\n\nIn this case, we used the * wildcard to count lines, words and characters (in that order, left-to-right) of both truncated gff3 files. Often, we only want to count one of these things, and wc has options for all of them:\n\n-l counts lines only.\n-w counts words only.\n-c counts characters only.\n\nFor example, if we just want to know how many lines we have in each files:\n\n\n\n\n\n\nCount only lines in a file\n\n\n\nwc -l TBNmA041_annotation_truncated*\n     617 TBNmA041_annotation_truncated_1.gff3\n     722 TBNmA041_annotation_truncated_2.gff3\n    1339 total\n\n\nWe will go into more details on the wc command in our next lesson.\n\n\n\n\n\n\nExercise 2.3.1.1: Looking inside files with less\n\n\n\n\nUse the less command to look inside the file MTB_H37Rv_truncated.fasta.\nHow many lines does this file contain?\nUse the less command again but with the option -S. Can you understand what this option does?\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n\nWe can investigate the content of the reference file using less MTB_H37Rv_truncated.fasta. From this view, it looks like this file contains several lines of content: the truncated genome is more than 150kb long, so it’s not surprising we see so much text! We can use Q to quit and go back to the console.\nTo check the number of lines in the file, we can use the wc -l MTB_H37Rv_truncated.fasta command. The answer is only 2.\nIf we use less -S MTB_H37Rv_truncated.fasta the display is different this time. We see only two lines in the output. If we use the → and ← arrows we can see that the text now goes “out of the screen”. So, what happens is that by default less will “wrap” long lines, so if a line of text is too long, it will continue it on the next line of the screen. When we use the option -S it instead displays each line individually, and we can use the arrow keys to see the content that does not fit on the screen.\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe annotation_truncated files we just looked into are in a format called GFF. This is a standard bioinformatic file format that stores gene coordinates and other features and has the file extension .gff. It is used to describe genes and other features of DNA, RNA and protein sequences. In this case, it corresponds to the coordinates of each annotated gene (start and end position) in the Mycobacterium tuberculosis reference genome (MTB_H37Rv). It also uses a header region with a ## string to include metadata.\nIn the exercise we looked at another standard file format called FASTA. This one is used to store nucleotide or amino acid sequences. In this case, the truncated nucleotide sequence of the Mycobacterium tuberculosis reference genome (MTB_H37Rv).\nWe will learn more about these files under File formats."
  },
  {
    "objectID": "materials/02-unix_intro/2.3-text_manipulation.html#combining-several-files",
    "href": "materials/02-unix_intro/2.3-text_manipulation.html#combining-several-files",
    "title": "2.3 Text Manipulation",
    "section": "2.3.2 Combining several files",
    "text": "2.3.2 Combining several files\nWe said that the cat command we used above stands for “concatenate”. This is because this command can be used to concatenate (combine) several files together. For example, if we wanted to combine both sets of annotation_truncated .gff files into a single file:\ncat TBNmA041_annotation_truncated_1.gff3 TBNmA041_annotation_truncated_2.gff3\nRunning the above command actually combines (concatenates) both files and print out the output. But what we will really want to do is to redirect the output to a different file."
  },
  {
    "objectID": "materials/02-unix_intro/2.3-text_manipulation.html#redirecting-output",
    "href": "materials/02-unix_intro/2.3-text_manipulation.html#redirecting-output",
    "title": "2.3 Text Manipulation",
    "section": "2.3.3 Redirecting Output",
    "text": "2.3.3 Redirecting Output\nThe cat command we just used printed the output to the screen. But what if we wanted to save it into a file? We can achieve this by sending (or redirecting) the output of the command to a file using the > operator.\ncat TBNmA041_annotation_truncated_1.gff3 TBNmA041_annotation_truncated_2.gff3 > TBNmA041_annotation_combined.gff3\nNow, the output is not printed to the console, but instead sent to a new file. We can check that the file was created with ls.\nIf we use > and the output file already exists, its content will be replaced. If what we want to do is append the result of the command to the existing file, we should use >> instead. Let’s see this in practice in the next exercise.\n\n\n\n\n\n\nExercise 2.3.3.1: Adding data to an existing file\n\n\n\n\nList the files in the sequencing_run1/ directory. Save the output in a file called “sequencing_files.txt”.\nAfter performing task 1 above, what happens if you run the command ls sequencing_run2/ > sequencing_files.txt?\nThe operator >> can be used to append the output of a command to an existing file. Try re-running both of the previous commands, but instead using the >> operator. What happens now?\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n\nTask 1\nTo list the files in the directory we use ls, followed by > to save the output in a file:\nls sequencing_run1/ > sequencing_files.txt\nWe can check the content of the file:\n\n\n\n\n\n\ncat sequencing_files.txt\nsample1_run1.fastq\nsample2_run1.fastq\nsample3_run1.fastq\nsample4_run1.fastq\nsample5_run1.fastq\n\n\n\n\n\nTask 2\nIf we run ls sequencing_run2/ > sequencing_files.txt, we will replace the content of the file:\n\n\n\n\n\n\ncat sequencing_files.txt\nsample1_run2.fastq\nsample2_run2.fastq\nsample3_run2.fastq\nsample4_run2.fastq\nsample5_run2.fastq\n\n\n\n\nTask 3\nIf we start again from the beginning, but instead use the >> operator the second time we run the command, we will append the output to the file instead of replacing it:\n\n\n\n\n\n\nls sequencing_run1/ > sequencing_files.txt\nls sequencing_run2/ >> sequencing_files.txt\ncat sequencing_files.txt\nsample1_run1.fastq\nsample2_run1.fastq\nsample3_run1.fastq\nsample4_run1.fastq\nsample5_run1.fastq\nsample1_run2.fastq\nsample2_run2.fastq\nsample3_run2.fastq\nsample4_run2.fastq\nsample5_run2.fastq"
  },
  {
    "objectID": "materials/02-unix_intro/2.3-text_manipulation.html#finding-patterns",
    "href": "materials/02-unix_intro/2.3-text_manipulation.html#finding-patterns",
    "title": "2.3 Text Manipulation",
    "section": "2.3.4 Finding Patterns",
    "text": "2.3.4 Finding Patterns\nThis is just to serve as an introduction to the grep command. Check out detailed description in the bonus lesson and also on how to use find\nSometimes it can be very useful to find lines of a file that match a particular text pattern. We can use the tool grep (“global regular expression print”) to achieve this. For example, let’s find the word >contig in one of our annotation_truncated files:\n\n\n\n\n\n\ngrep \">contig\" TBNmA041_annotation_truncated_1.gff3\n>contig_1\n>contig_2\n>contig_3\n>contig_4\n>contig_5\n>contig_100\n>contig_101\n>contig_102\n>contig_103\n...\n\n\n\nWe can see the result is all the lines that matched this word pattern.\n\n\n\n\n\n\nExercise 2.3.4.1: Finding patterns\n\n\n\nConsider our two annotation_truncated files – TBNmA041_annotation_truncated_1.gff3 and TBNmA041_annotation_truncated_2.gff3.\n\nCreate a new file called TBNmA041_contigs.txt that contains only the lines of text with the word “>contig” from both .gff files.\n\n\nHint\n\nYou can use grep to find a pattern in a file. You can use > to redirect the output of a command to a new file, and you cann use >> to add onto an existing file.\n\nCreate a second file called TBNmA041_CDS.txt that contains only the lines of text with the acronym “CDS”. CDS stands for CoDing Sequence.\nNow count the number of contigs and CDS in the combined files. Are they different? What did you expect? assume each CDS represents a specific gene\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n\nTask 1\nWe can use grep to find the pattern in our first .gff files and use > to save the output in a new file:\ngrep \">contig\" TBNmA041_annotation_truncated_1.gff3 > TBNmA041_contigs.txt\nNext, we use >> to add the output of our second run to the file we created above TBNmA041_contigs.txt.\ngrep \">contig\" TBNmA041_annotation_truncated_2.gff3 >> TBNmA041_contigs.txt\nWe could investigate the output of our command using less TBNmA041_contigs.txt.\n\nTask 2\nWe will follow the same code as used in Task 1 above, by first looking for the acronym CDS in the first .gff3 file and output its result to TBNmA041_CDS.txt.\ngrep \"CDS\" TBNmA041_annotation_truncated_1.gff3 > TBNmA041_CDS.txt\nNext, we use >> to add the output of our second run to the file we created above TBNmA041_contigs.txt.\ngrep \"CDS\" TBNmA041_annotation_truncated_2.gff3 >> TBNmA041_CDS.txt\nWe could investigate the output of our command using less TBNmA041_CDS.txt.\n\nTask 3\nWe can use wc to count the lines of the newly created files:\n\n\n\n\n\n\nwc -l TBNmA041_CDS.txt TBNmA041_contigs.txt\n      84 TBNmA041_CDS.txt\n      85 TBNmA041_contigs.txt\n     169 total\n\n\n\nThis is a hypothetical data, however, note that, a given contig can have several CDS and some contigs may have no CDS. Consequently, we don’t expect the numbers to be the same in a real situation."
  },
  {
    "objectID": "materials/02-unix_intro/2.3-text_manipulation.html#text-replacement-sed---stream-editor",
    "href": "materials/02-unix_intro/2.3-text_manipulation.html#text-replacement-sed---stream-editor",
    "title": "2.3 Text Manipulation",
    "section": "2.3.5 Text Replacement: sed - stream editor",
    "text": "2.3.5 Text Replacement: sed - stream editor\nOne of the most prominent text-processing utilities on GNU/Linux is the sed command, which is short for “stream editor”. A stream editor is used to perform basic text transformations on an input stream (a file or input from a pipeline).\n\n\n\n\n\n\nNote\n\n\n\nIn this tutorial, we’ll use the GNU version of sed (available on Ubuntu and other Linux operating systems). The macOS has the BSD version of sed which has different options and arguments. You can install the GNU version of sed with Homebrew using brew install gnu-sed.\n\n\n\nBasic Usage\nThere are many instances when we want to substitute a text in a line or filter out specific lines. In such cases, we can take advantage of sed. sed operates on a stream of text which it gets either from a text file or from standard input (STDIN). It means you can use the output of another command as the input of sed – in short, you can combine sed with other commands.\nBy default, sed outputs everything to standard output (STDOUT). It means, unless redirected, sed will print its output onto the terminal/screen instead of saving it in a file.\n\n\n\n\n\n\nNote\n\n\n\nsed edits line-by-line and in a non-interactive way.\n\n\nThe basic usage of sed:\nsed [OPTIONS] SCRIPT INPUT-FILES\nsed contains several sub-commands, but the main one we will use is the substitute or s command. To see a list of all sed’s commands, visit https://www.gnu.org/software/sed/manual/html_node/sed-commands-list.html.\nThe basic syntax for the s command is:\nsed 's/pattern/replacement/options'\nWhere s is called the sed command, pattern is the word we want to substitute also known as a regular expression (more on this later) and replacement is the new word we want to use instead.\nSo the basic concept of the ‘s’ command is: it attempts to match the pattern in a line, if the match is successful, then that portion of the pattern is replaced with replacement. To learn more about the ‘s’ command visit https://www.gnu.org/software/sed/manual/html_node/The-_0022s_0022-Command.html.\nThere are also other “options” added at the end of the command, which change the default behaviour of the text substitution. Some of the common options are:\n\ng: by default sed will only substitute the first match of the pattern. If we use the g option (“global”), then sed will substitute all matching text.\ni: by default sed matches the pattern in a case-sensitive manner. For example ‘A’ (Uppercase A) and ‘a’ (Lowercase A) are treated as different. If we use the i option (“case-insensitive”) then sed will treat ‘A’ and ‘a’ as the same.\n\nFor example, let’s create a file with some text inside it:\necho \"Hello world. How are you world?\" > hello.txt\n\n\n\n\n\n\nNote\n\n\n\nThe echo command is used to print some text on the console. In this case we are sending that text to a file to use in our example. Another pretty way of easily creating a text file.\n\n\nIf we do:\n\n\n\n\n\n\nsed 's/world/participant/' hello.txt\nThis is the result\nHello participant. How are you world?\n\n\n\nWe can see that the first “world” word was replaced with “participant”. This is the default behaviour of sed: only the first pattern it finds in a line of text is replaced with the new word. We can modify this by using the g option after the last /:\nNB. first rewrite the hello.txt file to see the full effect as we have already changed the first world.\n\n\n\n\n\n\necho \"Hello world. How are you world?\" > hello.txt\nsed 's/world/participant/g' hello.txt\nHello participant. How are you participant?\n\n\n\n\n\n\n\n\n\nClick to expand for more interesting examples on the use of the s command\n\n\n\n\n\nCreate a new file ‘input.txt’ and write the following text in it:\n\n\n\n\n\n\nHello, this is a test line. This is a very short line.\nThis is test line two. In this line, we have two occurrences of the test.\nThis line has many occurrences of the Test with different cases. test tEst TesT.\n\n\n\nNow try to replace ‘test’ with ‘hello’. We can do something like this:\n\n\n\n\n\n\nsed 's/test/hello/' input.txt\nHello, this is a hello line. This is a very short line.\nThis is hello line two. In this line, we have two occurrences of the test.\nThis line has many occurrences of the Test with different case. hello tEst TesT.\n\n\n\nYou may have noticed that lines two and three still have ‘test’. This is because we ask the sed to only replace the first text which matches. To replace all the matches, we have to use g option. Let’s try with g option.\n\n\n\n\n\n\nsed 's/test/hello/g' input.txt\nHello, this is a hello line. This is a very short line.\nThis is hello line two. In this line, we have two occurrences of the hello.\nThis line has many occurrences of the Test with different case. hello tEst TesT.\n\n\n\nAh, something is still wrong with the third line. It is not replacing ‘Test’, ‘tEst’ and ‘TesT’. We have to do something to tell the sed that we want to replace all of them. We can do this by using i option. Let’s add one more option:\n\n\n\n\n\n\nsed 's/test/hello/gi' input.txt\nHello, this is a hello line. This is a very short line.\nThis is hello line two. In this line, we have two occurrences of the hello.\nThis line has many occurrences of the hello with different case. hello hello hello.\n\n\n\nWonderful!\nLet’s say now we only want to replace all the occurrences of the ‘test’ at only line 3 or from line 2 to 3. Try to remember the basic syntax for the sed command. Remember! You can add the address of the line or a range of lines that you want to edit. Here is an example:\n\n\n\n\n\n\nsed '3s/test/hello/gi' input.txt\nHello, this is a test line. This is a very short line.\nThis is test line two. In this line, we have two occurrences of the test.\nThis line has many occurrences of the hello with different case. hello hello hello.\n\n\n\nSee only the third line is executed by the sed. The first two lines are as it is. At the beginning of the ‘s’ command, we are adding the line number which we want to edit. We can also add a range of lines. Here is one more example:\n\n\n\n\n\n\nsed '2,3s/test/hello/gi' input.txt\nAs you may have got the idea, it will edit lines 2 and 3. The output will be:\nHello, this is a test line. This is a very short line.\nThis is hello line two. In this line, we have two occurrences of the hello.\nThis line has many occurrences of the hello with different case. hello hello hello.\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nRegular Expressions\nFinding patterns in text can be a very powerful skill to master. In our examples we have been finding a literal word and replacing it with another word. However, we can do more complex text substitutions by using special keywords that define a more general pattern. These are known as regular expressions.\nFor example, in regular expression syntax, the character . stands for “any character”. So, for example, the pattern H. would match a “H” followed by any character, and the expression:\n\n\n\n\n\n\nsed 's/H./X/g' hello.txt\nResults in:\nXllo world. Xw are you world?\n\n\n\nNotice how both “He” (at the start of the word “Hello”) and “Ho” (at the start of the word “How”) are replaced with the letter “X”. Because both of them match the pattern “H followed by any character” (H.).\nTo learn more see this Regular Expression Cheatsheet.\n\n\n\n\nThe \\ Escape Character\nYou may have asked yourself, if the “forward slash” / character is used to separate parts of the sed substitute command, then how would we replace the “/” character itself in a piece of text? For example, let’s add a new line of text to our file:\necho \"Welcome to this workshop/course.\" >> hello.txt\nLet’s say we wanted to replace “workshop/course” with “tutorial” in this text. If we did:\n\n\n\n\n\n\nsed 's/workshop/course/tutorial/' hello.txt\nWe would get an error:\nsed: -e expression #1, char 5: unknown option to `s'\n\n\n\nThis is because we ended up with too many / in the command, and sed uses that to separate its different parts of the command. In this situation we need to tell sed to ignore that / as being a special character but instead treat it as the literal “/” character. To do this, we need to use “backslash” \\ before /, which is called the “escape” character. That will tell sed to treat the / as a normal character rather than a separator of its commands.\nSo:\nsed 's/workshop\\/course/tutorial/' hello.txt\n                ↑\n           This / is \"escaped\" with \\ beforehand\nThis looks a little strange, but the main thing to remember is that \\/ will be interpreted as the character “/” rather than the separator of sed’s substitute command.\n\n\n\n\n\n\nEscape character\n\n\n\nAn escape character is a character that invokes an alternative interpretation of the following character. Sometimes it is also used to insert unallowed characters in a string. An escape character is a backslash \\ followed by a character (or characters). Some of the keywords/characters which you want to escape are as follows:\n\n*: Asterisk.\n.: Dot.\n[: Left square bracket.\n]: Right square bracket.\n?: Question mark.\n$: Dollar sign.\n^: Caret\n/: Forward slash\n\\: Backward slash\n\n\n\n\n\n\n\n\n\nSpecial use of the escape character\n\n\n\nEscape characters are also used to provide visual representations of non-printing characters and characters that usually have special meanings. The list of commonly used escape characters in the sed is as follows:\n\n\\n: a newline.\n\\r: a carriage return.\n\\t: a horizontal tab.\n\nFor more details visit https://www.gnu.org/software/sed/manual/sed.html#Escapes.\n\n\n\n\n\n\n\n\nExercise 2.3.5.1: Use of the escape character \\\n\n\n\nThe file in bacteria_rpob/bacteria_truncated_rpob.fasta contains the nucleotide sequences of bacteria RNA polymerase beta subunit genes (rpob) for 4 bacteria samples.\n\n\n\n\n\n\nLet’s have a lool at the file\n\n\n\ncat bacteria_rpob/bacteria_truncated_rpob.fasta\n>Mycobacterium_tuberculosis_H37Rv_trunc_rpob_gene\nTTGGCAGATTCCCGCCAGAGCAAAACAGCCGCTAGTCCTAGTCCGAGTCGCCCGCAAAGTTCCTCGAATAACTC\n>Salmonella_enterica_truncated/incomplete_rpob_gene\nGAGGAACCCTATGGTTTACTCCTATACCGAGAAAAAACGTATTCGTAAGGATTTTGGTA...............\n>Staphylococcus_aureus_subsp._aureus_Trunc_rpob_gene\nTTGGCAGGTCAAGTTGTCCAATATGGAAGACATCGTAAACGTAGAAACTACGCGAGAATTTCAGAAGTATTAAC\n>Streptococcus_pneumoniae_TRUNC_rpob_gene\nTTGGCAGGACATGACGTTCAATACGGGAAACATCGTACCCGTCGTAGTTTTTCAAGAATCAAAGAAGTTCAAAT\n\n\nWe will cover FASTA files in a subsequent section, for now all we need to know is that each sequence has a name in a line that starts with the character >.\nUse sed to achieve the following:\n\nSubstitute the word trunc with truncated.\n\n\nHint\n\nSimilar to how you can use the g option to do “global” substitution, you can also use the i option to do case-insensitive text substitution.\n\nSubstitute the word /incomplete with -missing.\nThe character . is also a keyword used in regular expressions to mean “any character”. See what happens if you run the command sed 's/./X/g' bacteria_rpob/bacteria_truncated_rpob.fasta. How would you fix this command to literally only substitute the character . with X?\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n\nTask 1\n\n\n\n\n\n\nTo replace the word trunc with truncated, we can do:\n\n\n\nsed 's/trunc/truncated/i' bacteria_rpob/bacteria_truncated_rpob.fasta\n>Mycobacterium_tuberculosis_H37Rv_truncated_rpob_gene\nTTGGCAGATTCCCGCCAGAGCAAAACAGCCGCTAGTCCTAGTCCGAGTCGCCCGCAAAGTTCCTCGAATAACTC\n>Salmonella_enterica_truncatedated/incomplete_rpob_gene\nGAGGAACCCTATGGTTTACTCCTATACCGAGAAAAAACGTATTCGTAAGGATTTTGGTA...............\n>Staphylococcus_aureus_subsp._aureus_truncated_rpob_gene\nTTGGCAGGTCAAGTTGTCCAATATGGAAGACATCGTAAACGTAGAAACTACGCGAGAATTTCAGAAGTATTAAC\n>Streptococcus_pneumoniae_truncated_rpob_gene\nTTGGCAGGACATGACGTTCAATACGGGAAACATCGTACCCGTCGTAGTTTTTCAAGAATCAAAGAAGTTCAAAT\n\n\nWe have to use the option i to tell the sed that we want to match the word trunc in case-insensitive manner.\n\nTask 2\nFor the second task, if we do:\nsed 's//incomplete/-missing/'  bacteria_rpob/bacteria_truncated_rpob.fasta\nWe will get an error. We need to use \\ to escape the / keyword in our pattern, so:\n\n\n\n\n\n\nUse of the \\ escape character - I\n\n\n\nsed 's/\\/incomplete/-missing/'  bacteria_rpob/bacteria_truncated_rpob.fasta\n>Mycobacterium_tuberculosis_H37Rv_trunc_rpob_gene\nTTGGCAGATTCCCGCCAGAGCAAAACAGCCGCTAGTCCTAGTCCGAGTCGCCCGCAAAGTTCCTCGAATAACTC\n>Salmonella_enterica_truncated-missing_rpob_gene\nGAGGAACCCTATGGTTTACTCCTATACCGAGAAAAAACGTATTCGTAAGGATTTTGGTA...............\n>Staphylococcus_aureus_subsp._aureus_Trunc_rpob_gene\nTTGGCAGGTCAAGTTGTCCAATATGGAAGACATCGTAAACGTAGAAACTACGCGAGAATTTCAGAAGTATTAAC\n>Streptococcus_pneumoniae_TRUNC_rpob_gene\nTTGGCAGGACATGACGTTCAATACGGGAAACATCGTACCCGTCGTAGTTTTTCAAGAATCAAAGAAGTTCAAAT\n\n\n\nTask 3\nFinally, to replace the character ., if we do:\n\n\n\n\n\n\nUse of the \\ escape character - II\n\n\n\nsed 's/./X/g' bacteria_rpob/bacteria_truncated_rpob.fasta\nXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\nXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\nXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\nXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\nXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\nXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\nXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\nXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n\n\nEverything becomes “X”! That’s because . is a keyword used in regular expressions to mean “any character”. Because we are using the g option (for “global substitution”), we replaced every single character with “X”. To literally replace the character “.”, we need to again use the \\ escape character, so:\n\n\n\n\n\n\nsed 's/\\./X/g' bacteria_rpob/bacteria_truncated_rpob.fasta\n>Mycobacterium_tuberculosis_H37Rv_trunc_rpob_gene\nTTGGCAGATTCCCGCCAGAGCAAAACAGCCGCTAGTCCTAGTCCGAGTCGCCCGCAAAGTTCCTCGAATAACTC\n>Salmonella_enterica_truncated/incomplete_rpob_gene\nGAGGAACCCTATGGTTTACTCCTATACCGAGAAAAAACGTATTCGTAAGGATTTTGGTAXXXXXXXXXXXXXXX\n>Staphylococcus_aureus_subspX_aureus_Trunc_rpob_gene\nTTGGCAGGTCAAGTTGTCCAATATGGAAGACATCGTAAACGTAGAAACTACGCGAGAATTTCAGAAGTATTAAC\n>Streptococcus_pneumoniae_TRUNC_rpob_gene\nTTGGCAGGACATGACGTTCAATACGGGAAACATCGTACCCGTCGTAGTTTTTCAAGAATCAAAGAAGTTCAAAT\n\n\n\n\n\n\n\n\n\n\nWildcards in sed\nWe have already covered a bit of wildcards on “Navigating Files and Directories” page; here, we will discuss the use of ., * and ? in the sed. These wildcards are part of the regular expression.\n\nYou can use . as a placeholder for any character except newline (\\n) or empty text. For example, if you use . in your regular expression like x.z then it will match to strings like xaz, xbz, x1z, xzz, etc., but, it will not match to xz.\n\nAlso recall:\n\nYou can use * to match 0 or more occurrences of the previous character. For example, xy*z will match to strings like xz (0 occurrences of y), xyz (1 occurrence of y), xyyz and so on.\n? is a bit similar to *. The difference is it will only match for 0 or 1 occurrence of the previous character. For example, xy?z will match to strings like xz (0 occurrences of y), xyz (1 occurrence of y) but not to xyyz.\n\nNow, let’s do some coding. Create a file input2.txt and copy the following sequence:\nATGCCTGATTGGGCTACGTCGTAAGCGATGGCTAGGTATCGTAAAGGGGTTTGGGAACCCCAATCACTAGCT\n\n\nHint\n\nYou can do this simply by using echo:\necho \n\"ATGCCTGATTGGGCTACGTCGTAAGCGATGGCTAGGTATCGTAAAGGGGTTTGGGAACCCCAATCACTAGCT\" \n> input2.txt \n\nLet’s say we want to replace anything between A and G with U. We can do this using the sed:\n\n\n\n\n\n\nsed 's/A.G/AUG/g' input2.txt\nAUGCCTGATTGGGCTAUGTCGTAUGCGAUGGCTAUGTATCGTAAUGGGGTTTGGGAACCCCAATCACTAGCT\n\n\n\n\n\n\n\n\n\nExercise 2.3.5.2: Replace Me\n\n\n\nNavigate to the bacteria_rpob/ direcrory and have a look at the rpob_gene.fasta file. You have four tasks to do on this file. They are:\n\nReplace the word bacteria with sample. Make sure to replace all words matching to bacteria in a case insensitive manner.\nReplace all the . with X.\nRemove /incomplete from one of the sample names.\nSave the output to rpob_gene_processed.fasta.\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n\nLet’s first navigate to the bacteria_rpob/ directory.\ncd bacteria_rpob/\nWe will now walk through the solution step by step. At first, to replace the word bacteria with sample, we can do something like this:\n\n\n\n\n\n\nsed 's/bacteria/sample/i' rpob_gene.fasta\n>sample_1_rpob_gene\nCCGAGTCGCCCGCAAAGTTCCTCGAATAACTCTTGGCAGATTCCCGCCAGAGCAAAACAGCCGCTAGTCCTAGT\n>sample_2_rpob_gene\nAATATGGAAGACATCGTAAACGTAGAAACTACGCTTGGCAGGTCAAGTTGTCCGAGAATTTCAGAAGTATTAAC\n>sample_3_rpob_gene\nTTGGCAGGACATGACGTTCAATACGGGAAACATCGT..............TTCAAGAATCAAAGAAGTTCAAAT\n>sample_4/incomplete_rpob_gene\nGAGGAACCCTATGGTTTACTCCTATACCGAGAAAAAACGTATTCGTAAGGATTTTGGTA...............\n\n\n\nWe have to use the flag i to tell the sed that we want to match the word bacteria in case insensitive manner.\nSecond we have to replace all the . with X. Remember the . is a keyword (or has special meaning in the sed). So, to have the literal meaning of ., we have to escape the . with \\. We can replace all the . as follows:\n\n\n\n\n\n\nsed 's/\\./X/g' rpob_gene.fasta \n>bacteria_1_rpob_gene\nCCGAGTCGCCCGCAAAGTTCCTCGAATAACTCTTGGCAGATTCCCGCCAGAGCAAAACAGCCGCTAGTCCTAGT\n>Bacteria_2_rpob_gene\nAATATGGAAGACATCGTAAACGTAGAAACTACGCTTGGCAGGTCAAGTTGTCCGAGAATTTCAGAAGTATTAAC\n>BACTERIA_3_rpob_gene\nTTGGCAGGACATGACGTTCAATACGGGAAACATCGTXXXXXXXXXXXXXXTTCAAGAATCAAAGAAGTTCAAAT\n>Bacteria_4/incomplete_rpob_gene\nGAGGAACCCTATGGTTTACTCCTATACCGAGAAAAAACGTATTCGTAAGGATTTTGGTAXXXXXXXXXXXXXXX\n\n\n\nIn lines 3 and 4, you can see that all the . are replaced by X. Note that to replace all the . we also have to use g flag.\nThird, we have to remove \\incomplete. Again / is a keyword in the sed. We have to do something similar to the previous step.\n\n\n\n\n\n\nsed 's/\\/incomplete//' rpob_gene.fasta\n>bacteria_1_rpob_gene\nCCGAGTCGCCCGCAAAGTTCCTCGAATAACTCTTGGCAGATTCCCGCCAGAGCAAAACAGCCGCTAGTCCTAGT\n>Bacteria_2_rpob_gene\nAATATGGAAGACATCGTAAACGTAGAAACTACGCTTGGCAGGTCAAGTTGTCCGAGAATTTCAGAAGTATTAAC\n>BACTERIA_3_rpob_gene\nTTGGCAGGACATGACGTTCAATACGGGAAACATCGT..............TTCAAGAATCAAAGAAGTTCAAAT\n>Bacteria_4_rpob_gene\nGAGGAACCCTATGGTTTACTCCTATACCGAGAAAAAACGTATTCGTAAGGATTTTGGTA...............\n\n\n\nNow, we have to combine all three steps (this we can do by using pipe|) and then redirect the output to a file rather than the default output. We will talk more about pipe | in the next session.\n\n\n\n\n\n\nsed 's/bacteria/sample/i' rpob_gene.fasta | sed 's/\\./X/g' | \nsed 's/\\/incomplete//' > rpob_gene_processed.fasta\nYou will see no output in the terminal. But you will see a new file rpob_gene_processed.fasta, which will look similar to the following:\n>sample_1_rpob_gene\nCCGAGTCGCCCGCAAAGTTCCTCGAATAACTCTTGGCAGATTCCCGCCAGAGCAAAACAGCCGCTAGTCCTAGT\n>sample_2_rpob_gene\nAATATGGAAGACATCGTAAACGTAGAAACTACGCTTGGCAGGTCAAGTTGTCCGAGAATTTCAGAAGTATTAAC\n>sample_3_rpob_gene\nTTGGCAGGACATGACGTTCAATACGGGAAACATCGTXXXXXXXXXXXXXXTTCAAGAATCAAAGAAGTTCAAAT\n>sample_4_rpob_gene\nGAGGAACCCTATGGTTTACTCCTATACCGAGAAAAAACGTATTCGTAAGGATTTTGGTAXXXXXXXXXXXXXXX\n\n\n\n\n\n\n\n\nTo learn more about the use of sed visit https://www.gnu.org/software/sed/manual/sed.html"
  },
  {
    "objectID": "materials/02-unix_intro/2.3-text_manipulation.html#credit",
    "href": "materials/02-unix_intro/2.3-text_manipulation.html#credit",
    "title": "2.3 Text Manipulation",
    "section": "2.3.6 Credit",
    "text": "2.3.6 Credit\nInformation on this page has been adapted and modified from the following source:\n\nhttps://github.com/cambiotraining/sars-cov-2-genomics"
  },
  {
    "objectID": "materials/02-unix_intro/2.7-bonus_find_and_grep.html",
    "href": "materials/02-unix_intro/2.7-bonus_find_and_grep.html",
    "title": "2.7 Bonus : Finding Things",
    "section": "",
    "text": "Teaching: 20 min || Exercises: 10 min"
  },
  {
    "objectID": "materials/02-unix_intro/2.7-bonus_find_and_grep.html#overview",
    "href": "materials/02-unix_intro/2.7-bonus_find_and_grep.html#overview",
    "title": "2.7 Bonus : Finding Things",
    "section": "Overview",
    "text": "Overview\n\n\n\n\n\n\n\n\n\n\n\n\nQuestions:\n\n\n\n\nHow can I find files?\nHow can I find things in files?\n\n\n\n\n\n\n\n\n\nLearning Objectives:\n\n\n\n\nUse grep to select lines from text files that match simple patterns.\nUse find to find files and directories whose names match simple patterns.\nUse the output of one command as the command-line argument(s) to another command.\nExplain what is meant by ‘text’ and ‘binary’ files, and why many common tools don’t handle the latter well.\n\n\n\n\n\n\n\n\n\nKey Points:\n\n\n\n\nfind finds files with specific properties that match patterns.\ngrep selects lines in files that match patterns.\n--help is an option supported by many bash commands, and programs that can be run from within Bash, to display more information on how to use these commands or programs.\nman [command] displays the manual page for a given command.\n$([command]) inserts a command’s output in place."
  },
  {
    "objectID": "materials/02-unix_intro/2.7-bonus_find_and_grep.html#grep",
    "href": "materials/02-unix_intro/2.7-bonus_find_and_grep.html#grep",
    "title": "2.7 Bonus : Finding Things",
    "section": "2.7.1 grep",
    "text": "2.7.1 grep\nIn the same way that many of us now use ‘Google’ as a verb meaning ‘to find’, Unix programmers often use the word ‘grep’. ‘grep’ is a contraction of ‘global/regular expression/print’, a common sequence of operations in early Unix text editors. It is also the name of a very useful command-line program.\ngrep finds and prints lines in files that match a pattern. For our examples, we will use a file that contains three haiku taken from a 1998 competition in Salon magazine. For this set of examples, we are going to be working in the 02_unix_intro/writing subdirectory:\n\n\n\n\n\n\ncd\ncd Desktop/workshop_files_Bact_Genomics_2023/02_unix_intro/writing\ncat haiku.txt\nThe Tao that is seen\nIs not the true Tao, until\nYou bring fresh toner.\n\nWith searching comes loss\nand the presence of absence:\n\"My Thesis\" not found.\n\nYesterday it worked\nToday it is not working\nSoftware is like that.\n\n\n\n\n\n\n\n\n\nForever, or Five Years\n\n\n\nWe haven’t linked to the original haiku because they don’t appear to be on Salon’s site any longer. As Jeff Rothenberg said, ‘Digital information lasts forever — or five years, whichever comes first.’ Luckily, popular content often has backups.\n\n\nLet’s find lines that contain the word ‘not’:\n\n\n\n\n\n\ngrep not haiku.txt\nIs not the true Tao, until\n\"My Thesis\" not found\nToday it is not working\n\n\n\nHere, not is the pattern we’re searching for. The grep command searches through the file, looking for matches to the pattern specified. To use it type grep, then the pattern we’re searching for and finally the name of the file (or files) we’re searching in.\nThe output is the three lines in the file that contain the letters ‘not’.\nBy default, grep searches for a pattern in a case-sensitive way. In addition, the search pattern we have selected does not have to form a complete word, as we will see in the next example.\nLet’s search for the pattern: ‘The’.\n\n\n\n\n\n\ngrep The haiku.txt\nThe Tao that is seen\n\"My Thesis\" not found.\n\n\n\nThis time, two lines that include the letters ‘The’ are outputted, one of which contained our search pattern within a larger word, ‘Thesis’.\nTo restrict matches to lines containing the word ‘The’ on its own, we can give grep with the -w option. This will limit matches to word boundaries.\nLater in this lesson, we will also see how we can change the search behavior of grep with respect to its case sensitivity.\n\n\n\n\n\n\ngrep -w The haiku.txt\nThe Tao that is seen\n\n\n\nNote that a ‘word boundary’ includes the start and end of a line, so not just letters surrounded by spaces. Sometimes we don’t want to search for a single word, but a phrase. This is also easy to do with grep by putting the phrase in quotes.\n\n\n\n\n\n\ngrep -w \"is not\" haiku.txt\nToday it is not working\n\n\n\nWe’ve now seen that you don’t have to have quotes around single words, but it is useful to use quotes when searching for multiple words. It also helps to make it easier to distinguish between the search term or phrase and the file being searched. We will use quotes in the remaining examples.\nAnother useful option is -n, which numbers the lines that match:\n\n\n\n\n\n\ngrep -n \"it\" haiku.txt\n5:With searching comes loss\n9:Yesterday it worked\n10:Today it is not working\n\n\n\nHere, we can see that lines 5, 9, and 10 contain the letters ‘it’.\nWe can combine options (i.e. flags) as we do with other Unix commands. For example, let’s find the lines that contain the word ‘the’. We can combine the option -w to find the lines that contain the word ‘the’ and -n to number the lines that match:\n\n\n\n\n\n\ngrep -n -w \"the\" haiku.txt\n2:Is not the true Tao, until\n6:and the presence of absence:\n\n\n\nNow we want to use the option -i to make our search case-insensitive:\n\n\n\n\n\n\ngrep -n -w -i \"the\" haiku.txt\n1:The Tao that is seen\n2:Is not the true Tao, until\n6:and the presence of absence:\n\n\n\nNow, we want to use the option -v to invert our search, i.e., we want to output the lines that do not contain the word ‘the’.\n\n\n\n\n\n\ngrep -n -w -v \"the\" haiku.txt\n1:The Tao that is seen\n3:You bring fresh toner.\n4:\n5:With searching comes loss\n7:\"My Thesis\" not found.\n8:\n9:Yesterday it worked\n10:Today it is not working\n11:Software is like that.\n\n\n\nIf we use the -r (recursive) option, grep can search for a pattern recursively through a set of files in subdirectories.\nLet’s search recursively for Yesterday in the workshop_files_Bact_Genomics_2023/02_unix_intro/writing directory:\n\n\n\n\n\n\ngrep -r Yesterday .\n./LittleWomen.txt:\"Yesterday, when Aunt was asleep and I was trying to be\n as still as a\n./LittleWomen.txt:Yesterday at dinner, when an Austrian officer stared at\n us and then\n./LittleWomen.txt:Yesterday was a quiet day spent in teaching, sewing, and\n writing in my\n./haiku.txt:Yesterday it worked\n\n\n\ngrep has lots of other options. To find out what they are, we can type:\n\n\n\n\n\n\ngrep --help\nUsage: grep [OPTION]... PATTERN [FILE]...\nSearch for PATTERN in each FILE or standard input.\nPATTERN is, by default, a basic regular expression (BRE).\nExample: grep -i 'hello world' menu.h main.c\n\nRegexp selection and interpretation:\n  -E, --extended-regexp     PATTERN is an extended regular expression (ERE)\n  -F, --fixed-strings       PATTERN is a set of newline-separated fixed strings\n  -G, --basic-regexp        PATTERN is a basic regular expression (BRE)\n  -P, --perl-regexp         PATTERN is a Perl regular expression\n  -e, --regexp=PATTERN      use PATTERN for matching\n  -f, --file=FILE           obtain PATTERN from FILE\n  -i, --ignore-case         ignore case distinctions\n  -w, --word-regexp         force PATTERN to match only whole words\n  -x, --line-regexp         force PATTERN to match only whole lines\n  -z, --null-data           a data line ends in 0 byte, not newline\n\nMiscellaneous:\n...        ...        ...\n\n\n\n\n\n\n\n\n\nExercise 2.7.1.1: Using grep\n\n\n\nWhich command would result in the following output:\n\n\n\n\n\n\nand the presence of absence:\n\n\n\n\ngrep \"of\" haiku.txt\ngrep -E \"of\" haiku.txt\ngrep -w \"of\" haiku.txt\ngrep -i \"of\" haiku.txt\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n\nThe correct answer is 3, because the -w option looks only for whole-word matches. The other options will also match ‘of’ when part of another word.\n\n\n\n\n\n\nWildcards\ngrep‘s real power doesn’t come from its options, though; it comes from the fact that patterns can include wildcards. (The technical name for these is regular expressions, which is what the ’re’ in ‘grep’ stands for.) Regular expressions are both complex and powerful; if you want to do complex searches, please visit his link.\nAs taster, we can find lines that have an ‘o’ in the second position like this:\n\n\n\n\n\n\ngrep -E \"^.o\" haiku.txt\nYou bring fresh toner.\nToday it is not working\nSoftware is like that.\n\n\n\nWe use the -E option and put the pattern in quotes to prevent the shell from trying to interpret it. (If the pattern contained a *, for example, the shell would try to expand it before running grep.) The ^ in the pattern anchors the match to the start of the line. The . matches a single character (just like ? in the shell), while the o matches an actual ‘o’.\n\n\n\n\n\n\nExercise 2.7.1.2: Tracking a Species\n\n\n\nLeah has several hundred data files saved in one directory, each of which is formatted like this:\n24/01/2023,Escherichia_coli,5\n24/01/2023,Mycobacteria_tuberculosis,22\n24/01/2023,Salmonella_enterica,7\n24/01/2023,Staphylococcus_aureus,19\n25/01/2023,Streptococcus_pneumoniae,2\n25/01/2023,Pseudomonas_aeruginosa,1\n25/01/2023,Haemophilus_influenzae ,18\n26/01/2023,Carsonella_ruddii,1\n26/01/2023,Escherichia_coli,6\nShe wants to write a shell script that takes a species as the first command-line argument and a directory as the second argument. The script should return one file called <species>.txt containing a list of dates and the number of that species seen on each date. For example using the data shown above, Escherichia_coli.txt would contain:\n24/01/2023,5\n26/01/2023,6\nBelow, each line contains an individual command, or pipe. Arrange their sequence in one command in order to achieve Leah’s goal:\ncut -d : -f 2\ngrep -w $1 -r $2\n$1.txt\ncut -d , -f 1,3\nHint: use man grep to look for how to grep text recursively in a directory and man cut to select more than one field in a line. An example of such a file is provided in workshop_files_Bact_Genomics_2023/02_unix_intro/bacteria_counts.csv\n\n\n\n\n\n\nSolution:\n\n\n\n\n\ngrep -w $1 -r $2 | cut -d : -f 2 | cut -d , -f 1,3 > $1.txt\nActually, you can swap the order of the two cut commands and it still works. At the command line, try changing the order of the cut commands, and have a look at the output from each step to see why this is the case.\nYou would call the script above like this:\nbash count-species.sh Mycobacteria_tuberculosis .\n\n\n\n\n\n\n\n\n\n\n\nExercise 2.7.1.3: Little Women\n\n\n\nYou and your friend, having just finished reading Little Women by Louisa May Alcott, are in an argument. Of the four sisters in the book, Jo, Meg, Beth, and Amy, your friend thinks that Jo was the most mentioned. You, however, are certain it was Amy. Luckily, you have a file LittleWomen.txt containing the full text of the novel (workshop_files_Bact_Genomics_2023/02_unix_intro/writing/LittleWomen.txt). Using a for loop, how would you tabulate the number of times each of the four sisters is mentioned? If you haven’t encountered for loops yet, you may want to go review the next bonus lesson on loops.\n\n\nHint:\n\nOne solution might employ the commands grep and wc and a |, while another might utilize grep options. There is often more than one way to solve a programming task, so a particular solution is usually chosen based on a combination of yielding the correct result, elegance, readability, and speed.\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n\nfor sis in Jo Meg Beth Amy\ndo\n    echo $sis:\n    grep -ow $sis LittleWomen.txt | wc -l\ndone\nAlternative, slightly inferior solution:\nfor sis in Jo Meg Beth Amy\ndo\n    echo $sis:\n    grep -ocw $sis LittleWomen.txt\ndone\nThis solution is inferior because grep -c only reports the number of lines matched. The total number of matches reported by this method will be lower if there is more than one match per line.\nPerceptive observers may have noticed that character names sometimes appear in all-uppercase in chapter titles (e.g. ‘MEG GOES TO VANITY FAIR’). If you wanted to count these as well, you could add the -i option for case-insensitivity (though in this case, it doesn’t affect the answer to which sister is mentioned most frequently)."
  },
  {
    "objectID": "materials/02-unix_intro/2.7-bonus_find_and_grep.html#find",
    "href": "materials/02-unix_intro/2.7-bonus_find_and_grep.html#find",
    "title": "2.7 Bonus : Finding Things",
    "section": "2.7.2 find",
    "text": "2.7.2 find\nWhile grep finds lines in files, the find command finds files themselves. Again, it has a lot of options; to show how the simplest ones work, we’ll use the workshop_files_Bact_Genomics_2023/02_unix_intro/ directory tree shown below.\n\n\n\n\n\n\n\n.\n├── bacteria_counts.csv\n├── bacteria_rpob/\n│   ├── bacteria_truncated_rpob.fasta\n|   ├── Mycobacterium_tuberculosis_H37Rv_rpob.fasta\n|   ├── rpob_gene.fasta\n|   ├── Salmonella_enterica.fasta\n|   ├── Staphylococcus_aureus.fasta\n|   ├── Streptococcus_pneumoniae.fasta\n├── bacteria.txt\n├── creatures/\n│   ├── basilisk\n|   ├── minotaur.dat\n|   ├── unicorn.dat\n├── exp/\n│   ├── 2015-10-23-calibration.txt\n|   ├── 2015-10-23-dataset_overview.txt\n|   ├── 2015-10-23-dataset1.txt\n     ...\n├── G26832.gff3.gz\n├── G26832.tsv\n├── molecules/\n│   ├── cubane.pdb\n│   ├── ethane.pdb\n│   ├── methane.pdb\n│   ├── octane.pdb\n│   ├── pentane.pdb\n│   └── propane.pdb\n...\nmore here\n...\n└── writing/\n    ├── haiku.txt\n    └── LittleWomen.txt\n\n\n\nThe 02_unix_intro directory contains thirteen files, [bacteria_counts.csv, bacteria.txt, G26832.gff3.gz, G26832.tsv etc.] and ten directories: bacteria_rpob, creatures, exp, molecules etc., containing various files.\nFor our first command, let’s run find . (remember to run this command from the workshop_files_Bact_Genomics_2023/02_unix_intro folder).\n\n\n\n\n\n\nfind .\n.\n.\n./sequencing_run1\n./sequencing_run1/sample2_run1.fastq\n./sequencing_run1/sample5_run1.fastq\n./sequencing_run1/sample3_run1.fastq\n./sequencing_run1/sample1_run1.fastq\n./sequencing_run1/sample4_run1.fastq\n./molecules\n./molecules/propane.pdb\n./molecules/.DS_Store\n./molecules/octane.pdb\n./molecules/cubane.pdb\n./molecules/ethane.pdb\n./molecules/pentane.pdb\n./molecules/methane.pdb\n./TBNmA041_annotation_truncated_1.tsv\n...\n\n\n\nAs always, the . on its own means the current working directory, which is where we want our search to start. find’s output is the names of every file and directory under the current working directory. This can seem useless at first but find has many options to filter the output and in this lesson we will discover some of them.\nThe first option in our list is -type d that means ‘things that are directories’. Sure enough, find’s output is the names of the nine directories (including .):\n\n\n\n\n\n\nfind . -type d\n.\n./sequencing_run1\n./molecules\n./sequencing\n./sequencing/run2\n./sequencing/run1\n./bacteria_rpob\n./sequencing_run2\n./north-pacific-gyre\n./north-pacific-gyre/2012-07-03\n./creatures\n./exp\n./mycobacteria_rpob\n./writing\n\n\n\nNotice that the objects find finds are not listed in any particular order. If we change -type d to -type f, we get a listing of all the files instead:\n\n\n\n\n\n\nfind . -type f\n./sequencing_run1/sample2_run1.fastq\n./sequencing_run1/sample5_run1.fastq\n./sequencing_run1/sample3_run1.fastq\n./sequencing_run1/sample1_run1.fastq\n./sequencing_run1/sample4_run1.fastq\n./molecules/propane.pdb\n./molecules/.DS_Store\n./molecules/octane.pdb\n./molecules/cubane.pdb\n./molecules/ethane.pdb\n./molecules/pentane.pdb\n./molecules/methane.pdb\n./TBNmA041_annotation_truncated_1.tsv\n...\n\n\n\nNow let’s try matching by name:\n\n\n\n\n\n\nfind . -name *.txt\n\nfind: morse.txt: unknown primary or operator\n\n\n\nWe expected it to find all the text files, but it only prints out ./morse.txt. The problem is that the shell expands wildcard characters like * before commands run. Since *.txt in the current directory expands to ./morse.txt, the command we actually ran was:\nfind . -name morse.txt\nfind did what we asked; we just asked for the wrong thing.\nTo get what we want, let’s do what we did with grep: put *.txt in quotes to prevent the shell from expanding the * wildcard. This way, find actually gets the pattern *.txt, not the expanded filename morse.txt:\n\n\n\n\n\n\nfind . -name \"*.txt\"\n./bacteria.txt\n./morse.txt\n./north-pacific-gyre/2012-07-03/NENE01729B.txt\n./north-pacific-gyre/2012-07-03/NENE02040B.txt\n./north-pacific-gyre/2012-07-03/NENE01729A.txt\n./north-pacific-gyre/2012-07-03/NENE02040A.txt\n./north-pacific-gyre/2012-07-03/NENE01971Z.txt\n...\n\n\n\n\n\n\n\n\n\nListing vs. Finding\n\n\n\nls and find can be made to do similar things given the right options, but under normal circumstances, ls lists everything it can, while find searches for things with certain properties and shows them.\n\n\nAs we said earlier, the command line’s power lies in combining tools. We’ve seen how to do that with pipes; let’s look at another technique. As we just saw, find . -name \"*.txt\" gives us a list of all text files in or below the current directory. How can we combine that with wc -l to count the lines in all those files?\nThe simplest way is to put the find command inside $():\n\n\n\n\n\n\nwc -l $(find . -name \"*.txt\")\n       8 ./bacteria.txt\n      50 ./morse.txt\n     300 ./north-pacific-gyre/2012-07-03/NENE01729B.txt\n     300 ./north-pacific-gyre/2012-07-03/NENE02040B.txt\n     300 ./north-pacific-gyre/2012-07-03/NENE01729A.txt\n     300 ./north-pacific-gyre/2012-07-03/NENE02040A.txt\n     300 ./north-pacific-gyre/2012-07-03/NENE01971Z.txt\n...\n\n\n\nWhen the shell executes this command, the first thing it does is run whatever is inside the $(). It then replaces the $() expression with that command’s output. Since the output of find are all the filenames ./bacteria.txt, ./morse.txt, ./north-pacific-gyre/2012-07-03/NENE01729B.txt, etc. the shell constructs the command:\nwc -l ./bacteria.txt ./morse.txt ./north-pacific-gyre/2012-07-03/NENE01729B.txt etc.\nwhich is what we wanted. This expansion is exactly what the shell does when it expands wildcards like * and ?, but lets us use any command we want as our own ‘wildcard’.\nIt’s very common to use find and grep together. The first finds files that match a pattern; the second looks for lines inside those files that match another pattern. Here, for example, we can find txt files that contain the word “searching” by looking for the string ‘searching’ in all the .txt files in the current directory:\n\n\n\n\n\n\ngrep \"searching\" $(find . -name \"*.txt\")\n./writing/LittleWomen.txt:sitting on the top step, affected to be searching\n for her book, but was\n./writing/haiku.txt:With searching comes loss\n\n\n\n\n\n\n\n\n\nExercise 2.7.2.1: Matching and Subtracting\n\n\n\nThe -v option to grep inverts pattern matching, so that only lines which do not match the pattern are printed. Given that, which of the following commands will find all .pdb files in molecules/ except octane.pdb? Once you have thought about your answer, you can test the commands in the workshop_files_Bact_Genomics_2023/02_unix_intro/molecules directory.\n\nfind molecules -name \"*.pdb\" | grep -v octane\nfind molecules -name *.pdb | grep -v octane\ngrep -v \"octane\" $(find molecules -name \"*.pdb\")\nNone of the above.\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n\nOption 1 is correct. Putting the match expression in quotes prevents the shell expanding it, so it gets passed to the find command.\nOption 2 also works in this instance because the shell tries to expand *.pdb but there are no *.pdb files in the current directory, so the wildcard expression gets passed to find.\nOption 3 is incorrect because it searches the contents of the files for lines which do not match ‘octane’, rather than searching the file names.\n\n\n\n\n\n\n\n\n\n\n\nBinary Files\n\n\n\nWe have focused exclusively on finding patterns in text files. What if your data is stored as images, in databases, or in some other format?\nA handful of tools extend grep to handle a few non text formats. But a more generalizable approach is to convert the data to text, or extract the text-like elements from the data. On the one hand, it makes simple things easy to do. On the other hand, complex things are usually impossible. For example, it’s easy enough to write a program that will extract X and Y dimensions from image files for grep to play with, but how would you write something to find values in a spreadsheet whose cells contained formulas?\nA last option is to recognize that the shell and text processing have their limits, and to use another programming language. When the time comes to do this, don’t be too hard on the shell: many modern programming languages have borrowed a lot of ideas from it, and imitation is also the sincerest form of praise.\n\n\nThe Unix shell is older than most of the people who use it. It has survived so long because it is one of the most productive programming environments ever created — maybe even the most productive. Its syntax may be cryptic, but people who have mastered it can experiment with different commands interactively, then use what they have learned to automate their work. Graphical user interfaces may be easier to use at first, but once learned, the productivity in the shell is unbeatable. And as Alfred North Whitehead wrote in 1911, ‘Civilization advances by extending the number of important operations which we can perform without thinking about them.’\n\n\n\n\n\n\nExercise 2.7.2.2: find Pipeline Reading Comprehension\n\n\n\nWrite a short explanatory comment for the following shell script:\nwc -l $(find . -name \"*.pdb\") | sort -n\n\n\n\n\n\n\nSolution:\n\n\n\n\n\n\nFind all files with a .pdb extension recursively from the current directory\nCount the number of lines each of these files contains\nSort the output from step 2, numerically"
  },
  {
    "objectID": "materials/02-unix_intro/2.7-bonus_find_and_grep.html#credit",
    "href": "materials/02-unix_intro/2.7-bonus_find_and_grep.html#credit",
    "title": "2.7 Bonus : Finding Things",
    "section": "2.7.3 Credit",
    "text": "2.7.3 Credit\nInformation on this page has been adapted and modified from the following source:\n\nhttps://github.com/swcarpentry/shell-novice"
  },
  {
    "objectID": "materials/02-unix_intro/2.9-bonus_shell_scripts.html",
    "href": "materials/02-unix_intro/2.9-bonus_shell_scripts.html",
    "title": "2.9 Bonus: Shell Scripts",
    "section": "",
    "text": "Teaching: 30 min || Exercises: 10 min"
  },
  {
    "objectID": "materials/02-unix_intro/2.9-bonus_shell_scripts.html#overview",
    "href": "materials/02-unix_intro/2.9-bonus_shell_scripts.html#overview",
    "title": "2.9 Bonus: Shell Scripts",
    "section": "Overview",
    "text": "Overview\n\n\n\n\n\n\n\n\n\n\n\n\nQuestions:\n\n\n\n\nHow can I save and re-use commands?\n\n\n\n\n\n\n\n\n\nLearning Objectives:\n\n\n\n\nWrite a shell script that runs a command or series of commands for a fixed set of files.\nRun a shell script from the command line.\nWrite a shell script that operates on a set of files defined by the user on the command line.\nCreate pipelines that include shell scripts you, and others, have written.\n\n\n\n\n\n\n\n\n\nKey Points:\n\n\n\n\nSave commands in files (usually called shell scripts) for re-use.\nbash {filename} runs/executes the commands saved in a file.\n$1, $2, etc., refer to the first command-line argument, the second command-line argument, etc.\n$@ refers to all of a shell script’s command-line arguments.\nPlace variables in quotes if the values might have spaces in them.\nLetting users decide what files to process is more flexible and more consistent with built-in Unix commands."
  },
  {
    "objectID": "materials/02-unix_intro/2.9-bonus_shell_scripts.html#background",
    "href": "materials/02-unix_intro/2.9-bonus_shell_scripts.html#background",
    "title": "2.9 Bonus: Shell Scripts",
    "section": "2.9.1 Background",
    "text": "2.9.1 Background\nSo far, we have been running commands directly on the console in an interactive way. We are finally ready to see what makes the shell such a powerful programming environment. We are going to take the commands we repeat frequently and save them in files so that we can re-run all those operations again later by typing a single command. For historical reasons, a bunch of commands saved in a file is usually called a shell script, but make no mistake: these are actually small programs."
  },
  {
    "objectID": "materials/02-unix_intro/2.9-bonus_shell_scripts.html#my-first-script",
    "href": "materials/02-unix_intro/2.9-bonus_shell_scripts.html#my-first-script",
    "title": "2.9 Bonus: Shell Scripts",
    "section": "2.9.2 My first script",
    "text": "2.9.2 My first script\nLet’s start by going back to molecules/ and creating a new file, middle.sh which will become our shell script:\ncd molecules\nnano middle.sh\nThe command nano middle.sh opens the file middle.sh within the text editor ‘nano’ (which runs within the shell). If the file does not exist, it will be created. We can use the text editor to directly edit the file – we’ll simply insert the following line:\nhead -n 15 octane.pdb | tail -n 5\n\nThe command we just saved is a variation on the pipe we constructed earlier: it selects lines 11-15 of the file octane.pdb. Remember, we are not running it as a command just yet: we are putting the commands in a file.\nThen we save the file (Ctrl-O in nano), and exit the text editor (Ctrl-X in nano). Check that the directory molecules now contains a file called middle.sh.\nOnce we have saved the file, we can ask the shell to execute the commands it contains. Our shell is called bash, so we run the following command:\n\n\n\n\n\n\nbash middle.sh\nATOM      9  H           1      -4.502   0.681   0.785  1.00  0.00\nATOM     10  H           1      -5.254  -0.243  -0.537  1.00  0.00\nATOM     11  H           1      -4.357   1.252  -0.895  1.00  0.00\nATOM     12  H           1      -3.009  -0.741  -1.467  1.00  0.00\nATOM     13  H           1      -3.172  -1.337   0.206  1.00  0.00\n\n\n\nSure enough, our script’s output is exactly what we would get if we ran that pipeline directly.\n\n\n\n\n\n\nText vs. Whatever\n\n\n\nWe usually call programs like Microsoft Word or LibreOffice Writer “text editors”, but we need to be a bit more careful when it comes to programming. By default, Microsoft Word uses .docx files to store not only text, but also formatting information about fonts, headings, and so on. This extra information isn’t stored as characters, and doesn’t mean anything to tools like head: they expect input files to contain nothing but the letters, digits, and punctuation on a standard computer keyboard. When editing programs, therefore, you must either use a plain text editor, or be careful to save files as plain text."
  },
  {
    "objectID": "materials/02-unix_intro/2.9-bonus_shell_scripts.html#running-bash-scripts-with-arguments",
    "href": "materials/02-unix_intro/2.9-bonus_shell_scripts.html#running-bash-scripts-with-arguments",
    "title": "2.9 Bonus: Shell Scripts",
    "section": "2.9.3 Running bash scripts with arguments",
    "text": "2.9.3 Running bash scripts with arguments\nWhat if we want to select lines from an arbitrary file? We could edit middle.sh each time to change the filename, but that would probably take longer than typing the command out again in the shell and executing it with a new file name. Instead, let’s edit middle.sh and make it more versatile:\nnano middle.sh\nNow, within “nano”, replace the text octane.pdb with the special variable called $1:\n\n\n\n\n\n\nhead -n 15 \"$1\" | tail -n 5\n\n\n\nInside a shell script, $1 means ‘the first filename (or other argument) on the command line’. We can now run our script like this:\n\n\n\n\n\n\nbash middle.sh octane.pdb\nATOM      9  H           1      -4.502   0.681   0.785  1.00  0.00\nATOM     10  H           1      -5.254  -0.243  -0.537  1.00  0.00\nATOM     11  H           1      -4.357   1.252  -0.895  1.00  0.00\nATOM     12  H           1      -3.009  -0.741  -1.467  1.00  0.00\nATOM     13  H           1      -3.172  -1.337   0.206  1.00  0.00\n\n\n\nor on a different file like this:\n\n\n\n\n\n\nbash middle.sh pentane.pdb\nATOM      9  H           1       1.324   0.350  -1.332  1.00  0.00\nATOM     10  H           1       1.271   1.378   0.122  1.00  0.00\nATOM     11  H           1      -0.074  -0.384   1.288  1.00  0.00\nATOM     12  H           1      -0.048  -1.362  -0.205  1.00  0.00\nATOM     13  H           1      -1.183   0.500  -1.412  1.00  0.00\n\n\n\n\n\n\n\n\n\nDouble-Quotes Around Arguments\n\n\n\nFor the same reason that we put the loop variable inside double-quotes, in case the filename happens to contain any spaces, we surround $1 with double-quotes.\n\n\nWe still need to edit middle.sh each time we want to adjust the range of lines, though. Let’s fix that by using the special variables $2 and $3 for the number of lines to be passed to head and tail respectively:\nnano middle.sh\n\n\n\n\n\n\nhead -n \"$2\" \"$1\" | tail -n \"$3\"\n\n\n\nWe can now run:\n\n\n\n\n\n\nbash middle.sh pentane.pdb 15 5\nATOM      9  H           1       1.324   0.350  -1.332  1.00  0.00\nATOM     10  H           1       1.271   1.378   0.122  1.00  0.00\nATOM     11  H           1      -0.074  -0.384   1.288  1.00  0.00\nATOM     12  H           1      -0.048  -1.362  -0.205  1.00  0.00\nATOM     13  H           1      -1.183   0.500  -1.412  1.00  0.00\n\n\n\nBy changing the arguments to our command we can change our script’s behaviour:\n\n\n\n\n\n\nbash middle.sh pentane.pdb 20 5\nATOM     14  H           1      -1.259   1.420   0.112  1.00  0.00\nATOM     15  H           1      -2.608  -0.407   1.130  1.00  0.00\nATOM     16  H           1      -2.540  -1.303  -0.404  1.00  0.00\nATOM     17  H           1      -3.393   0.254  -0.321  1.00  0.00\nTER      18              1"
  },
  {
    "objectID": "materials/02-unix_intro/2.9-bonus_shell_scripts.html#adding-comments-to-bash-scripts",
    "href": "materials/02-unix_intro/2.9-bonus_shell_scripts.html#adding-comments-to-bash-scripts",
    "title": "2.9 Bonus: Shell Scripts",
    "section": "2.9.4 Adding comments to bash scripts",
    "text": "2.9.4 Adding comments to bash scripts\nOur middle.sh script works perfectly well, but it may take the next person who reads it a moment to figure out what it does. We can improve our script by adding some comments at the top:\nnano middle.sh\n\n\n\n\n\n\n# Select lines from the middle of a file.\n# Usage: bash middle.sh filename end_line num_lines\nhead -n \"$2\" \"$1\" | tail -n \"$3\"\n\n\n\nA comment starts with a # character and runs to the end of the line. The computer ignores comments, but they’re invaluable for helping people (including your future self) understand and use scripts. The only caveat is that each time you modify the script, you should check that the comment is still accurate: an explanation that sends the reader in the wrong direction is worse than none at all.\nWhat if we want to process many files in a single pipeline? For example, if we want to sort our .pdb files by length, we would type:\nwc -l *.pdb | sort -n\nbecause wc -l lists the number of lines in the files (recall that wc stands for ‘word count’, adding the -l option means ‘count lines’ instead) and sort -n sorts things numerically. We could put this in a file, but then it would only ever sort a list of .pdb files in the current directory. If we want to be able to get a sorted list of other kinds of files, we need a way to get all those names into the script. We can’t use $1, $2, and so on because we don’t know how many files there are. Instead, we use the special variable $@, which means, ‘All of the command-line arguments to the shell script’. We also should put $@ inside double-quotes to handle the case of arguments containing spaces (\"$@\" is equivalent to \"$1\" \"$2\" …) Here’s an example:\n\n\n\n\n\n\nnano sorted.sh\n# Sort files by their length.\n# Usage: bash sorted.sh one_or_more_filenames\nwc -l \"$@\" | sort -n\n\n\n\n\n\n\n\n\n\nbash sorted.sh *.pdb ../creatures/*.dat\n9 methane.pdb\n12 ethane.pdb\n15 propane.pdb\n20 cubane.pdb\n21 pentane.pdb\n30 octane.pdb\n163 ../creatures/basilisk.dat\n163 ../creatures/minotaur.dat\n163 ../creatures/unicorn.dat\n596 total\n\n\n\n\n\n\n\n\n\nExercise 2.9.4.1: List Unique Species\n\n\n\nLeah has several hundred data files, each of which is formatted like this:\n\n\n\n\n\n\n24/01/2023,Escherichia_coli,5\n24/01/2023,Mycobacteria_tuberculosis,22\n24/01/2023,Salmonella_enterica,7\n24/01/2023,Staphylococcus_aureus,19\n25/01/2023,Streptococcus_pneumoniae,2\n25/01/2023,Pseudomonas_aeruginosa,1\n25/01/2023,Haemophilus_influenzae ,18\n26/01/2023,Carsonella_ruddii,1\n26/01/2023,Escherichia_coli,6\n\n\n\nAn example of this type of file is given in 02_unix_intro/bacteria_counts.csv.\nWe can use the command cut -d , -f 2 bacteria_counts.csv | sort | uniq to produce the unique species in bacteria_counts.csv. In order to avoid having to type out this series of commands every time, a scientist may choose to write a shell script instead.\nWrite a shell script called species.sh that takes any number of filenames as command-line arguments, and uses a variation of the above command to print a list of the unique species appearing in each of those files separately.\n\n\n\n\n\n\nSolution:\n\n\n\n\n\n# Script to find unique species in csv files where species is the second data field\n# This script accepts any number of file names as command line arguments\n\n# Loop over all files\nfor file in $@\ndo\n    echo \"Unique species in $file:\"\n    # Extract species names\n    cut -d , -f 2 $file | sort | uniq\ndone\n\n\n\n\n\nSuppose we have just run a series of commands that did something useful — for example, that created a graph we would like to use in a paper. We would like to be able to re-create the graph later if we need to, so we want to save the commands in a file. Instead of typing them in again (and potentially getting them wrong) we can do this:\nhistory | tail -n 5 > redo-figure-3.sh\nThe file redo-figure-3.sh now contains:\n\n\n\n\n\n\n297 bash goostats NENE01729B.txt stats-NENE01729B.txt\n298 bash goodiff stats-NENE01729B.txt /data/validated/01729.txt > 01729-differences.txt\n299 cut -d ',' -f 2-3 01729-differences.txt > 01729-time-series.txt\n300 ygraph --format scatter --color bw --borders none 01729-time-series.txt figure-3.png\n301 history | tail -n 5 > redo-figure-3.sh\n\n\n\nAfter a moment’s work in an editor to remove the serial numbers on the commands, and to remove the final line where we called the history command, we have a completely accurate record of how we created that figure.\n\n\n\n\n\n\nExercise 2.9.4.2: Why Record Commands in the History Before Running Them?\n\n\n\nIf you run the command:\nhistory | tail -n 5 > recent.sh\nthe last command in the file is the history command itself, i.e., the shell has added history to the command log before actually running it. In fact, the shell always adds commands to the log before running them. Why do you think it does this?\n\n\n\n\n\n\nSolution:\n\n\n\n\n\nIf a command causes something to crash or hang, it might be useful to know what that command was, in order to investigate the problem. Were the command only be recorded after running it, we would not have a record of the last command run in the event of a crash.\n\n\n\n\n\nIn practice, most people develop shell scripts by running commands at the shell prompt a few times to make sure they’re doing the right thing, then saving them in a file for re-use. This style of work allows people to recycle what they discover about their data and their workflow with one call to history and a bit of editing to clean up the output and save it as a shell script."
  },
  {
    "objectID": "materials/02-unix_intro/2.9-bonus_shell_scripts.html#nelles-pipeline-creating-a-script",
    "href": "materials/02-unix_intro/2.9-bonus_shell_scripts.html#nelles-pipeline-creating-a-script",
    "title": "2.9 Bonus: Shell Scripts",
    "section": "2.9.5 Nelle’s Pipeline: Creating a Script",
    "text": "2.9.5 Nelle’s Pipeline: Creating a Script\nNelle’s supervisor insisted that all her analytics must be reproducible. The easiest way to capture all the steps is in a script.\nFirst we return to Nelle’s data directory:\ncd ../north-pacific-gyre/2012-07-03/\nShe runs the editor and writes the following:\n# Calculate stats for data files.\nfor datafile in \"$@\"\ndo\n    echo $datafile\n    bash goostats $datafile stats-$datafile\ndone\nShe saves this in a file called do-stats.sh so that she can now re-do the first stage of her analysis by typing:\nbash do-stats.sh NENE*[AB].txt\nShe can also do this:\nbash do-stats.sh NENE*[AB].txt | wc -l\nso that the output is just the number of files processed rather than the names of the files that were processed.\nOne thing to note about Nelle’s script is that it lets the person running it decide what files to process. She could have written it as:\n# Calculate stats for Site A and Site B data files.\nfor datafile in NENE*[AB].txt\ndo\n    echo $datafile\n    bash goostats $datafile stats-$datafile\ndone\nThe advantage is that this always selects the right files: she doesn’t have to remember to exclude the ‘Z’ files. The disadvantage is that it always selects just those files — she can’t run it on all files (including the ‘Z’ files), or on the ‘G’ or ‘H’ files her colleagues in Antarctica are producing, without editing the script. If she wanted to be more adventurous, she could modify her script to check for command-line arguments, and use NENE*[AB].txt if none were provided. Of course, this introduces another tradeoff between flexibility and complexity.\n\n\n\n\n\n\nExercise 2.9.5.1: Variables in Shell Scripts\n\n\n\nIn the molecules directory, imagine you have a shell script called script.sh containing the following commands:\nhead -n $2 $1\ntail -n $3 $1\nWhile you are in the molecules directory, you type the following command:\nbash script.sh '*.pdb' 1 1\nWhich of the following outputs would you expect to see?\n\nAll of the lines between the first and the last lines of each file ending in .pdb in the molecules directory\nThe first and the last line of each file ending in .pdb in the molecules directory\nThe first and the last line of each file in the molecules directory\nAn error because of the quotes around *.pdb\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n\nThe correct answer is 2.\nThe special variables $1, $2 and $3 represent the command line arguments given to the script, such that the commands run are:\nhead -n 1 cubane.pdb ethane.pdb octane.pdb pentane.pdb propane.pdb\ntail -n 1 cubane.pdb ethane.pdb octane.pdb pentane.pdb propane.pdb\nThe shell does not expand '*.pdb' because it is enclosed by quote marks. As such, the first argument to the script is '*.pdb' which gets expanded within the script by head and tail.\n\n\n\n\n\n\n\n\n\n\n\nExercise 2.9.5.2: Find the Longest File With a Given Extension\n\n\n\nWrite a shell script called longest.sh that takes the name of a directory and a filename extension as its arguments, and prints out the name of the file with the most lines in that directory with that extension. For example:\nbash longest.sh /tmp/data pdb\nwould print the name of the .pdb file in /tmp/data that has the most lines.\n\n\n\n\n\n\nSolution:\n\n\n\n\n\n# Shell script which takes two arguments:\n#    1. a directory name\n#    2. a file extension\n# and prints the name of the file in that directory\n# with the most lines which matches the file extension.\n\nwc -l $1/*.$2 | sort -n | tail -n 2 | head -n 1\n\n\n\n\n\n\n\n\n\n\n\nExercise 2.9.5.3: Script Reading Comprehension\n\n\n\nFor this question, consider the 02_unix_intro/molecules directory once again. This contains a number of .pdb files in addition to any other files you may have created. Explain what each of the following three scripts would do when run as bash script1.sh *.pdb, bash script2.sh *.pdb, and bash script3.sh *.pdb respectively.\n# Script 1\necho *.*\n# Script 2\nfor filename in $1 $2 $3\ndo\n    cat $filename\ndone\n# Script 3\necho $@.pdb\n\n\n\n\n\n\nSolution:\n\n\n\n\n\nIn each case, the shell expands the wildcard in *.pdb before passing the resulting list of file names as arguments to the script.\nScript 1 would print out a list of all files containing a dot in their name. The arguments passed to the script are not actually used anywhere in the script.\nScript 2 would print the contents of the first 3 files with a .pdb file extension. $1, $2, and $3 refer to the first, second, and third argument respectively.\nScript 3 would print all the arguments to the script (i.e. all the .pdb files), followed by .pdb. $@ refers to all the arguments given to a shell script.\ncubane.pdb ethane.pdb methane.pdb octane.pdb pentane.pdb propane.pdb.pdb\n\n\n\n\n\n\n\n\n\n\n\nExercise 2.9.5.4: Debugging Scripts\n\n\n\nSuppose you have saved the following script in a file called do-errors.sh in Nelle’s north-pacific-gyre/2012-07-03 directory:\n# Calculate stats for data files.\nfor datafile in \"$@\"\ndo\n    echo $datfile\n    bash goostats $datafile stats-$datafile\ndone\nWhen you run it:\nbash do-errors.sh NENE*[AB].txt\nthe output is blank. To figure out why, re-run the script using the -x option:\nbash -x do-errors.sh NENE*[AB].txt\nWhat is the output showing you? Which line is responsible for the error?\n\n\n\n\n\n\nSolution:\n\n\n\n\n\nThe -x option causes bash to run in debug mode. This prints out each command as it is run, which will help you to locate errors. In this example, we can see that echo isn’t printing anything. We have made a typo in the loop variable name, and the variable datfile doesn’t exist, hence returning an empty string."
  },
  {
    "objectID": "materials/02-unix_intro/2.9-bonus_shell_scripts.html#indicating-program-to-execute-script",
    "href": "materials/02-unix_intro/2.9-bonus_shell_scripts.html#indicating-program-to-execute-script",
    "title": "2.9 Bonus: Shell Scripts",
    "section": "2.9.6 Indicating program to execute script",
    "text": "2.9.6 Indicating program to execute script\nAlthough the scripts we have created and run works perfectly well. It is a good practice to specify the program you would like to be used to execute the script. In addition to the .sh telling us it is a bash script, we need to include #!/bin/bash often as the first line before the contents of our script begin. If we had written our script in another programming language, say python, we would have included #!/usr/bin/env python to tell the console to run the script with python."
  },
  {
    "objectID": "materials/02-unix_intro/2.9-bonus_shell_scripts.html#summary-key-features-of-a-script",
    "href": "materials/02-unix_intro/2.9-bonus_shell_scripts.html#summary-key-features-of-a-script",
    "title": "2.9 Bonus: Shell Scripts",
    "section": "2.9.7 Summary: Key features of a script",
    "text": "2.9.7 Summary: Key features of a script\n\n\n\nDiagram summarising how a shell script is organised"
  },
  {
    "objectID": "materials/02-unix_intro/2.9-bonus_shell_scripts.html#credit",
    "href": "materials/02-unix_intro/2.9-bonus_shell_scripts.html#credit",
    "title": "2.9 Bonus: Shell Scripts",
    "section": "2.9.8 Credit",
    "text": "2.9.8 Credit\nInformation on this page has been adapted and modified from the following source(s):\n\nGabriel A. Devenyi (Ed.), Gerard Capes (Ed.), Colin Morris (Ed.), Will Pitchers (Ed.),Greg Wilson, Gerard Capes, Gabriel A. Devenyi, Christina Koch, Raniere Silva, Ashwin Srinath, … Vikram Chhatre. (2019, July). swcarpentry/shell-novice: Software Carpentry: the UNIX shell, June 2019 (Version v2019.06.1).\nhttps://github.com/cambiotraining/UnixIntro\nhttps://github.com/cambiotraining/unix-shell"
  },
  {
    "objectID": "materials/02-unix_intro/2.1-unix_overview.html",
    "href": "materials/02-unix_intro/2.1-unix_overview.html",
    "title": "2.1 The Unix Shell",
    "section": "",
    "text": "Teaching: 10 min || Exercises: 0 min"
  },
  {
    "objectID": "materials/02-unix_intro/2.1-unix_overview.html#overview",
    "href": "materials/02-unix_intro/2.1-unix_overview.html#overview",
    "title": "2.1 The Unix Shell",
    "section": "Overview",
    "text": "Overview\n\n\n\n\n\n\n\n\n\n\n\n\nQuestions:\n\n\n\n\nWhat is a command shell and why would I use one?\n\n\n\n\n\n\n\n\n\nLearning Objectives:\n\n\n\n\nExplain how the shell relates to the keyboard, the screen, the operating system, and users’ programs.\nExplain when and why command-line interfaces should be used instead of graphical interfaces.\n\n\n\n\n\n\n\n\n\nKey Points:\n\n\n\n\nA shell is a program whose primary purpose is to read commands and run other programs.\nThis lesson uses Bash, the default shell in many implementations of Unix.\nPrograms can be run in Bash by entering commands at the command-line prompt.\nThe shell’s main advantages are its high action-to-keystroke ratio, its support for automating repetitive tasks, and its capacity to access networked machines.\nThe shell’s main disadvantages are its primarily textual nature and how cryptic its commands and operation can be."
  },
  {
    "objectID": "materials/02-unix_intro/2.1-unix_overview.html#background",
    "href": "materials/02-unix_intro/2.1-unix_overview.html#background",
    "title": "2.1 The Unix Shell",
    "section": "2.1.1 Background",
    "text": "2.1.1 Background\nHumans and computers commonly interact in many different ways, such as through a keyboard and mouse, touch screen interfaces, or using speech recognition systems. The most widely used way to interact with personal computers is called a graphical user interface (GUI). With a GUI, we give instructions by clicking a mouse and using menu-driven interactions.\nWhile the visual aid of a GUI makes it intuitive to learn, this way of delivering instructions to a computer scales very poorly. Imagine the following task: for a literature search, you have to copy the third line of one thousand text files in one thousand different directories/folders and paste it into a single file. Using a GUI, you would not only be clicking at your desk for several hours, but you could potentially also commit an error in the process of completing this repetitive task. This is where we take advantage of the Unix shell. The Unix shell is both a command-line interface (CLI) and a scripting language, allowing such repetitive tasks to be done automatically and fast. With the proper commands, the shell can repeat tasks with or without some modification as many times as we want."
  },
  {
    "objectID": "materials/02-unix_intro/2.1-unix_overview.html#the-shell",
    "href": "materials/02-unix_intro/2.1-unix_overview.html#the-shell",
    "title": "2.1 The Unix Shell",
    "section": "2.1.2 The Shell",
    "text": "2.1.2 The Shell\nThe shell is a program where users can type commands. With the shell, it’s possible to invoke complicated programs like climate modelling software or simple commands that create an empty directory with only one line of code. The most popular Unix shell is Bash (the Bourne Again SHell — so-called because it’s derived from a shell written by Stephen Bourne). Bash is the default shell on most modern implementations of Unix and in most packages that provide Unix-like tools for Windows.\nUsing the shell will take some effort and some time to learn. While a GUI presents you with choices to select, CLI choices are not automatically presented to you, so you must learn a few commands like new vocabulary in a language you’re studying. However, unlike a spoken language, a small number of “words” (i.e. commands) gets you a long way, and we’ll cover those essential few today.\nThe grammar of a shell allows you to combine existing tools into powerful pipelines and handle large volumes of data automatically. Sequences of commands can be written into a script, improving the reproducibility of workflows.\nIn addition, the command line is often the easiest way to interact with remote machines and supercomputers. Familiarity with the shell is near essential to run a variety of specialized tools and resources including high-performance computing systems. As clusters and cloud computing systems become more popular for scientific data crunching, being able to interact with the shell is becoming a necessary skill. We can build on the command-line skills covered here to tackle a wide range of scientific questions and computational challenges.\nLet’s get started.\nWhen the shell is first opened, you are presented with a prompt, indicating that the shell is waiting for input.\n$\nThe shell typically uses $ as the prompt, but may use a different symbol. For this workshop, we will show all commands needed to be typed in a shaded box without the $ prompt. This is to allow participants to simply copy and paste commands seamlessly to aid speeding up other activities of the workshop.\nNote that after you type (or copy paste) a command, you have to press the Enter key to execute it.\n\n\n\n\n\n\nGeneral format for commands and outputs in this course\n\n\n\nthis shaded box will contain the commands for easy copy pasting\nthe output of a command (if any) will appear in this area right below\nthe line of command.\n\n\nThe prompt is followed by a text cursor, a character that indicates the position where your typing will appear. The cursor is usually a flashing or solid block, but it can also be an underscore or a pipe. You may have seen it in a text editor program, for example.\nSo let’s try our first command, ls which is short for listing. This command will list the contents of the current directory:\n\n\n\n\n\n\nls\nDesktop   Downloads    Movies    Pictures\nDocuments Library      Music     Public\n...\n\n\n\nNote that depending on your system, you may have a slightly different output.\nThroughout this workshop, we hide some of the output of commands using ellipsis ....\n\n\n\n\n\n\nCommand not found\n\n\n\nIf the shell can’t find a program whose name is the command you typed, it will print an error message such as:\n\n\n\n\n\n\nks\nks: command not found\n\n\n\nThis might happen if the command was mis-typed or if the program corresponding to that command is not installed."
  },
  {
    "objectID": "materials/02-unix_intro/2.1-unix_overview.html#nelles-pipeline-a-typical-problem",
    "href": "materials/02-unix_intro/2.1-unix_overview.html#nelles-pipeline-a-typical-problem",
    "title": "2.1 The Unix Shell",
    "section": "2.1.3 Nelle’s Pipeline: A Typical Problem",
    "text": "2.1.3 Nelle’s Pipeline: A Typical Problem\nNelle Nemo, a marine biologist, has just returned from a six-month survey of the North Pacific Gyre, where she has been sampling gelatinous marine life in the Great Pacific Garbage Patch. She has 1520 samples that she’s run through an assay machine to measure the relative abundance of 300 proteins. She needs to run these 1520 files through an imaginary program called goostats.sh she inherited. On top of this huge task, she has to write up results by the end of the month so her paper can appear in a special issue of Aquatic Goo Letters.\nThe bad news is that if she has to run goostats.sh by hand using a GUI, she will have to select and open a file 1520 times. If goostats.sh takes 30 seconds to run each file, the whole process will take more than 12 hours of Nelle’s attention. With the shell, Nelle can instead assign her computer this mundane task while she focuses her attention on writing her paper.\nThe next few lessons will explore the ways Nelle can achieve this. More specifically, they explain how she can use a command shell to run the goostats.sh program, using loops to automate the repetitive steps of entering file names, so that her computer can work while she writes her paper.\nAs a bonus, once she has put a processing pipeline together, she will be able to use it again whenever she collects more data.\nIn order to achieve her task, Nelle needs to know how to:\n\nnavigate to a file/directory\ncreate a file/directory\ncheck the length of a file\nchain commands together\nretrieve a set of files\niterate over files\nrun a shell script containing her pipeline\n\nBy the end of today, you should be able to perform all the above tasks all by yourself."
  },
  {
    "objectID": "materials/02-unix_intro/2.1-unix_overview.html#credit",
    "href": "materials/02-unix_intro/2.1-unix_overview.html#credit",
    "title": "2.1 The Unix Shell",
    "section": "2.1.4 Credit",
    "text": "2.1.4 Credit\nInformation on this page has been adapted and modified from the following source(s):\n\nGabriel A. Devenyi (Ed.), Gerard Capes (Ed.), Colin Morris (Ed.), Will Pitchers (Ed.),Greg Wilson, Gerard Capes, Gabriel A. Devenyi, Christina Koch, Raniere Silva, Ashwin Srinath, … Vikram Chhatre. (2019, July). swcarpentry/shell-novice: Software Carpentry: the UNIX shell, June 2019 (Version v2019.06.1)."
  },
  {
    "objectID": "materials/02-unix_intro/2.2-files_directories.html",
    "href": "materials/02-unix_intro/2.2-files_directories.html",
    "title": "2.2 Navigating Files and Directories",
    "section": "",
    "text": "Teaching: 40 min || Exercises: 20 min"
  },
  {
    "objectID": "materials/02-unix_intro/2.2-files_directories.html#overview",
    "href": "materials/02-unix_intro/2.2-files_directories.html#overview",
    "title": "2.2 Navigating Files and Directories",
    "section": "Overview",
    "text": "Overview\n\n\n\n\n\n\n\n\n\n\n\n\nQuestions:\n\n\n\n\nHow can I move around on my computer?\nHow can I see what files and directories I have?\nHow can I specify the location of a file or directory on my computer?\nHow can I create, copy, and delete files and directories?\nHow can I edit files?\n\n\n\n\n\n\n\n\n\nLearning Objectives:\n\n\n\n\nExplain the similarities and differences between a file and a directory.\nTranslate an absolute path into a relative path and vice versa.\nConstruct absolute and relative paths that identify specific files and directories.\nUse options and arguments to change the behaviour of a shell command.\nDemonstrate the use of tab completion, and explain its advantages.\nCreate a directory hierarchy that matches a given diagram.\nCreate files in that hierarchy using an editor or by copying and renaming existing files.\nDelete, copy and move specified files and/or directories.\n\n\n\n\n\n\n\n\n\nKey Points:\n\n\n\n\nThe file system is responsible for managing information on the disk.\nInformation is stored in files, which are stored in directories (folders).\nDirectories can also store other directories, which forms a directory tree.\ncd path changes the current working directory.\nls path prints a listing of a specific file or directory; ls on its own lists the current working directory.\npwd prints the user’s current working directory.\n/ on its own is the root directory of the whole file system.\nA relative path specifies a location starting from the current location.\nAn absolute path specifies a location from the root of the file system.\nDirectory names in a path are separated with / on Unix, but \\\\ on Windows.\n.. means ‘the directory above the current one’; . on its own means ‘the current directory’.\ncp old new copies a file.\nmkdir path creates a new directory.\nmv old new moves (renames) a file or directory.\nrm path removes (deletes) a file.\n* matches zero or more characters in a filename, so *.txt matches all files ending in .txt.\n? matches any single character in a filename, so ?.txt matches a.txt but not any.txt.\nThe shell does not have a trash bin: once something is deleted, it’s really gone.\nMost files’ names are something.extension. The extension isn’t required, and doesn’t guarantee anything, but is normally used to indicate the type of data in the file."
  },
  {
    "objectID": "materials/02-unix_intro/2.2-files_directories.html#the-file-system",
    "href": "materials/02-unix_intro/2.2-files_directories.html#the-file-system",
    "title": "2.2 Navigating Files and Directories",
    "section": "2.2.1 The file system",
    "text": "2.2.1 The file system\nThe part of the operating system responsible for managing files and directories is called the file system. It organizes our data into files, which hold information, and directories (also called “folders”), which hold files or other directories.\nSeveral commands are frequently used to create, inspect, rename, and delete files and directories. To start exploring them, we’ll go to our open shell window.\nFirst let’s find out where we are by running a command called pwd (which stands for “print working directory”). Directories are like places - at any time while we are using the shell we are in exactly one place, called our current working directory. Commands mostly read and write files in the current working directory, i.e. “here”, so knowing where you are before running a command is important. pwd shows you where you are:\n\n\n\n\n\n\npwd\n/home/ubuntu\n\n\n\nHere, the computer’s response is /home/ubuntu, which is your home directory:\n\n\n\n\n\n\nHome Directory Variation\n\n\n\nThe home directory path will look different on different operating systems. On Mac it may look like /Users/ubuntu, and on Windows it will be similar to C:\\Documents and Settings\\ubuntu or C:\\Users\\ubuntu.\n(Note that it may look slightly different for different versions of Windows, and ubuntu may be replaced with your username.) In future examples, we’ve used Linux output as the default - Mac and Windows output may differ slightly, but should be generally similar.\n\n\nTo understand what a “home directory” is, let’s have a look at how the file system as a whole is organized. For the sake of this example, we’ll be illustrating the filesystem on a typical Linux computer. After this illustration, you’ll be learning commands to explore your own filesystem, which will be constructed in a similar way, but not be exactly identical.\nOn a typical Linux computer, the filesystem looks like this:\n\n\n\nThe File System\n\n\nAt the top is the root directory that holds everything else. We refer to it using a slash character, /, on its own; this is the leading slash in /home/ubuntu.\nInside that directory are several other directories:\n\nbin (which is where some built-in programs are stored),\nusr (for miscellaneous user files),\nhome (where users’ personal directories are located),\ntmp (for temporary files that don’t need to be stored long-term), and so on.\n\nWe know that our current working directory /home/ubuntu is stored inside /home because /home is the first part of its name. Similarly, we know that /home is stored inside the root directory / because its name begins with /.\n\nSlashes\nNotice that there are two meanings for the / character. When it appears at the front of a file or directory name, it refers to the root directory. When it appears inside a name, it’s just a separator.\n\n\nUnderneath /home, we find one directory for each user with an account on the machine, in this example imhotep, larry, and ubuntu (you).\n\n\n\nHome Directories\n\n\nThe user imhotep’s files are stored in /home/imhotep, user larry’s in /home/larry, and yours in /home/ubuntu. Because you are the current user in our examples here, this is why we get /home/ubuntu as our home directory. Typically, when you open a new command prompt you will be in your home directory to start.\nNow let’s learn the command that will let us see the contents of our own filesystem. We can see what’s in our home directory by running ls, which stands for “listing”:\n\n\n\n\n\n\nls\nDocuments    Downloads    Music        Public\nDesktop      Movies       Pictures     Templates\n...\n\n\n\n(Again, your results may be slightly different depending on your operating system and how you have customized your filesystem.)\nls prints the names of the files and directories in the current directory. We can make its output more comprehensible by using the -F option (also known as a switch or a flag) , which tells ls to classify the output by adding a marker to file and directory names to indicate what they are:\n\na trailing / indicates that this is a directory\n@ indicates a link\n* indicates an executable\n\nDepending on your default options, the shell might also use colors to indicate whether each entry is a file or directory.\n\n\n\n\n\n\nls -F\nDocuments/    Downloads/    Music/        Public/\nDesktop/      Movies/       Pictures/     Templates/\n...\n\n\n\nHere, we can see that our home directory contains mostly sub-directories. Any names in your output that don’t have a classification symbol, are plain old files.\n\n\n\n\n\n\nClearing your terminal\n\n\n\nIf your screen gets too cluttered, you can clear your terminal using the clear command. You can still access previous commands using ↑ and ↓ to move line-by-line, or by scrolling in your terminal."
  },
  {
    "objectID": "materials/02-unix_intro/2.2-files_directories.html#general-syntax-of-a-shell-command",
    "href": "materials/02-unix_intro/2.2-files_directories.html#general-syntax-of-a-shell-command",
    "title": "2.2 Navigating Files and Directories",
    "section": "General syntax of a shell command",
    "text": "General syntax of a shell command\nConsider the command below as a general example of a command, which we will dissect into its component parts:\nls -F /\n\n\n\nGeneral syntax of a shell command\n\n\nls is the command, with an option -F and an argument /. We’ve already encountered options (also called switches or flags) which either start with a single dash (-) or two dashes (--), and they change the behaviour of a command. Arguments tell the command what to operate on (e.g. files and directories). Sometimes options and arguments are referred to as parameters. A command can be called with more than one option and more than one argument: but a command doesn’t always require an argument or an option.\nEach part is separated by spaces: if you omit the space between ls and -F the shell will look for a command called ls-F, which doesn’t exist. Also, capitalisation can be important: ls -r is different to ls -R. Also, ls -s is different to ls -S. Find out what each does.\nPutting all that together, our command above gives us a listing of files and directories in the root directory /. An example of the output you might get from the above command is given below:\n\n\n\n\n\n\nls -F /\nbin/  boot/  cdrom/  core  dev/  etc/  home/  home2/  initrd.img@  \ninitrd.img.old@  lib/  lib32/  lib64/  lost+found/  media/  mnt/  \nopt/  proc/  root/  run/  sbin/  snap/  srv/  sys/  tmp/  usr/  var/"
  },
  {
    "objectID": "materials/02-unix_intro/2.2-files_directories.html#getting-help",
    "href": "materials/02-unix_intro/2.2-files_directories.html#getting-help",
    "title": "2.2 Navigating Files and Directories",
    "section": "Getting help",
    "text": "Getting help\nls has lots of other options. There are two common ways to find out how to use a command and what options it accepts:\n\nWe can pass a --help option to the command, such as:\nls --help\nWe can read its manual with man, such as:\nman ls\n\n\nThe --help option\nMany bash commands, and programs that people have written that can be run from within bash, support a --help option to display more information on how to use the command or program.\n\n\n\n\n\n\nls --help\nUsage: ls [OPTION]... [FILE]...\nList information about the FILEs (the current directory by default).\nSort entries alphabetically if none of -cftuvSUX nor --sort is specified.\n\nMandatory arguments to long options are mandatory for short options too.\n  -a, --all                  do not ignore entries starting with .\n  -A, --almost-all           do not list implied . and ..\n      --author               with -l, print the author of each file\n  -b, --escape               print C-style escapes for nongraphic characters\n      --block-size=SIZE      scale sizes by SIZE before printing them; e.g.,\n                               '--block-size=M' prints sizes in units of\n                               1,048,576 bytes; see SIZE format below\n  -B, --ignore-backups       do not list implied entries ending with ~\n  -c                         with -lt: sort by, and show, ctime (time of last\n                               modification of file status information);\n                               with -l: show ctime and sort by name;\n                               otherwise: sort by ctime, newest first\n  -C                         list entries by columns\n      --color[=WHEN]         colorize the output; WHEN can be 'always' (default\n                               if omitted), 'auto', or 'never'; more info below\n  -d, --directory            list directories themselves, not their contents\n  -D, --dired                generate output designed for Emacs' dired mode\n  -f                         do not sort, enable -aU, disable -ls --color\n  -F, --classify             append indicator (one of */=>@|) to entries\n      --file-type            likewise, except do not append '*'\n      --format=WORD          across -x, commas -m, horizontal -x, long -l,\n                               single-column -1, verbose -l, vertical -C\n      --full-time            like -l --time-style=full-iso\n  -g                         like -l, but do not list owner\n      --group-directories-first\n                             group directories before files;\n                               can be augmented with a --sort option, but any\n                               use of --sort=none (-U) disables grouping\n  -G, --no-group             in a long listing, don't print group names\n  -h, --human-readable       with -l and/or -s, print human readable sizes\n                               (e.g., 1K 234M 2G)\n      --si                   likewise, but use powers of 1000 not 1024\n...   ...                    ...     \n\n\n\n\n\n\n\n\n\nUnsupported command-line options\n\n\n\nIf you try to use an option (flag) that is not supported, ls and other commands will usually print an error message similar to:\n\n\n\n\n\n\nls -j\nls: invalid option -- 'j'\nTry 'ls --help' for more information.\n\n\n\n\n\n\n\nThe man command\nThe other way to learn about ls is to type\nman ls\nThis will turn your terminal into a page with a description of the ls command and its options and, if you’re lucky, some examples of how to use it.\nTo navigate through the man pages, you may use ↑ and ↓ to move line-by-line, or try B and Spacebar to skip up and down by a full page. To search for a character or word in the man pages, use / followed by the character or word you are searching for. Sometimes a search will result in multiple hits. If so, you can move between hits using N (for moving forward) and Shift+N (for moving backward).\nTo quit the man pages, press Q.\n\n\n\n\n\n\nManual pages on the web\n\n\n\nOf course there is a third way to access help for commands: Searching the internet via your web browser. When using internet search, including the phrase unix man page in your search query will help to find relevant results.\nGNU provides links to its manuals including the core GNU utilities, which covers many commands introduced within this lesson.\n\n\n\n\n\n\n\n\nExercise 2.2.1.1: Exploring More ls Flags\n\n\n\nYou can also use two options at the same time. What does the command ls do when used with the -l option? What about if you use both the -l and the -h option?\nSome of its output is about properties that we do not cover in this lesson (such as file permissions and ownership), but the rest should be useful nevertheless.\n\n\n\n\n\n\nSolution:\n\n\n\n\n\nThe -l option makes ls use a long listing format, showing not only the file/directory names but also additional information such as the file size and the time of its last modification. If you use both the -h option and the -l option, this makes the file size “human readable”, i.e. displaying something like 5.3K instead of 5369. \n\n\n\n\n\n\n\n\n\n\n\nExercise 2.2.1.2: Listing in Reverse Chronological Order\n\n\n\nBy default, ls lists the contents of a directory in alphabetical order by name. The command ls -t lists items by time of last change instead of alphabetically. The command ls -r lists the contents of a directory in reverse order. Which file is displayed last when you combine the -t and -r options? Hint: You may need to use the -l option to see the last changed dates.\n\n\n\n\n\n\nSolution:\n\n\n\n\n\nThe most recently changed file is listed last when using -rtl. This can be very useful for finding your most recent edits or checking to see if a new output file was written."
  },
  {
    "objectID": "materials/02-unix_intro/2.2-files_directories.html#exploring-other-directories",
    "href": "materials/02-unix_intro/2.2-files_directories.html#exploring-other-directories",
    "title": "2.2 Navigating Files and Directories",
    "section": "2.2.2 Exploring Other Directories",
    "text": "2.2.2 Exploring Other Directories\n\nNot only can we use ls on the current working directory, but we can use it to list the contents of a different directory. Let’s take a look at our Desktop directory by running ls -F Desktop, i.e., the command ls with the -F option and the argument Desktop. The argument Desktop tells ls that we want a listing of something other than our current working directory:\n\n\n\n\n\n\nls -F Desktop\n\n...\nworkshop_files_Bact_Genomics_2023/\n...\n\n\n\nNote that if a directory named Desktop does not exist in your current working directory, this command will return an error. Typically, a Desktop directory exists in your home directory, which we assume is the current working directory of your bash shell.\nYour output should be a list of all the files and sub-directories in your Desktop directory, including the workshop_files_Bact_Genomics_2023 directory. On many systems, the command line Desktop directory is the same as your GUI Desktop. Take a look at your Desktop to confirm that your output is accurate.\nAs you may now see, using a bash shell is strongly dependent on the idea that your files and directories are organized in a hierarchical file system. Organizing things hierarchically in this way helps us keep track of our work: it’s possible to put hundreds of files in our home directory, just as it’s possible to pile hundreds of printed papers on our desk, but it’s a self-defeating strategy.\nNow that we know the workshop_files_Bact_Genomics_2023 directory is located in our Desktop directory, we can do two things.\nFirst, we can look at its contents, using the same strategy as before, passing a directory name as an argument to ls:\n\n\n\n\n\n\nls -F Desktop/workshop_files_Bact_Genomics_2023\n01_intro_WGS/    02_unix_intro/    03_file_formats/    04_QC/    05_mapping/\n...\n\n\n\nSecond, we can also change our location to that directory, so we are no longer located in our home directory.\nThe command to change locations is cd followed by a directory name to change our working directory. cd stands for “change directory”, which is a bit misleading: the command doesn’t change the directory, it changes the shell’s idea of what directory we are in.\nLet’s say we want to move to the 02_unix_intro directory we saw above. We can use the following series of commands to get there:\ncd Desktop\ncd workshop_files_Bact_Genomics_2023\ncd 02_unix_intro\nThese commands will move us from our home directory onto our Desktop, then into the workshop_files_Bact_Genomics_2023 directory, then into the 02_unix_intro directory. You will notice that cd doesn’t print anything. This is normal. Many shell commands will not output anything to the screen when successfully executed. But if we run pwd after it, we can see that we are now in /home/ubuntu/Desktop/workshop_files_Bact_Genomics_2023/02_unix_intro. If we run ls without arguments now, it lists the contents of /home/ubuntu/Desktop/workshop_files_Bact_Genomics_2023/02_unix_intro, because that’s where we are now:\n\n\n\n\n\n\npwd\n/home/ubuntu/Desktop/workshop_files_Bact_Genomics_2023/02_unix_intro\n\n\n\n\n\n\n\n\n\nls -F\n\namino-acids.txt   elements/     pdb/            salmon.txt\nanimals.txt       morse.txt     planets.txt     sunspot.txt\n\n\n\nWe now know how to go down the directory tree, but how do we go up? We might try the following:\n\n\n\n\n\n\nError\n\n\n\ncd workshop_files_Bact_Genomics_2023\n-bash: cd: workshop_files_Bact_Genomics_2023: No such file or directory\n\n\nBut we get an error! Why is this?\nWith our methods so far, cd can only see sub-directories inside your current directory. There are different ways to see directories above your current location; we will start with the simplest.\nThere is a shortcut in the shell to move up one directory level that looks like this:\ncd ..\n.. is a special directory name meaning “the directory containing this one”, or more succinctly, the parent of the current directory. Sure enough, if we run pwd after running cd .., we’re back in /home/ubuntu/Desktop/workshop_files_Bact_Genomics_2023:\n\n\n\n\n\n\npwd\n/home/ubuntu/Desktop/workshop_files_Bact_Genomics_2023\n\n\n\nThe special directory .. doesn’t usually show up when we run ls. If we want to display it, we can give ls the -a option:\n\n\n\n\n\n\nls -F -a\n\n./    01_intro_WGS/    03_file_formats/    05_mapping/\n../   02_unix_intro/   04_QC/              \n\n\n\n-a stands for “show all”; it forces ls to show us file and directory names that begin with ., such as .. (which, if we’re in /home/ubuntu, refers to the /home directory)\nAs you can see, it also displays another special directory that’s just called ., which means “the current working directory”. It may seem redundant to have a name for it, but we will see some uses for it soon.\n\n\n\n\n\n\nNote\n\n\n\nNote that in most command line tools, multiple options can be combined with a single - and no spaces between the options: ls -F -a is equivalent to ls -Fa.\n\n\n\n\n\n\n\n\nOther Hidden Files\n\n\n\nIn addition to the hidden directories .. and ., you may also see a file called .bashrc. This file usually contains shell configuration settings. You may also see other files and directories beginning with .. These are usually files and directories that are used to configure different programs on your computer. The prefix . is used to prevent these configuration files from cluttering the terminal when a standard ls command is used.\n\n\nThese three commands are the basic commands for navigating the filesystem on your computer: pwd, ls and cd. Let’s explore some variations on those commands.\nWhat happens if you type cd on its own, without giving a directory?\ncd\nHow can you check what happened? pwd gives us the answer!\n\n\n\n\n\n\npwd\n/Users/name_of_user\n\n\n\nIt turns out that cd without an argument will return you to your home directory, which is great if you’ve got lost in your own filesystem.\nLet’s try returning to the 02_unix_intro directory from before. Last time, we used three commands, but we can actually string together the list of directories to move to 02_unix_intro in one step:\ncd Desktop/workshop_files_Bact_Genomics_2023/02_unix_intro\nCheck that we’ve moved to the right place by running pwd and ls -F\nIf we want to move up one level from the 02_unix_intro directory, we could use cd ... But there is another way to move to any directory, regardless of your current location.\nSo far, when specifying directory names, or even a directory path (as above), we have been using relative paths. When you use a relative path with a command like ls or cd, it tries to find that location from where we are, rather than from the root of the file system.\nHowever, it is possible to specify the absolute path to a directory by including its entire path from the root directory, which is indicated by a leading slash. The leading / tells the computer to follow the path from the root of the file system, so it always refers to exactly one directory, no matter where we are when we run the command.\nThis allows us to move to our workshop_files_Bact_Genomics_2023 directory from anywhere on the filesystem (including from inside 02_unix_intro). To find the absolute path we are looking for, we can use pwd and then extract the piece we need to move to workshop_files_Bact_Genomics_2023.\n\n\n\n\n\n\npwd\n/home/ubuntu/Desktop/workshop_files_Bact_Genomics_2023/02_unix_intro\n\n\n\ncd /home/ubuntu/Desktop/workshop_files_Bact_Genomics_2023\nRun pwd and ls -F to ensure that we are in the directory we expect.\n\n\n\n\n\n\nTwo More Shortcuts\n\n\n\nThe shell interprets the tilde (~) character at the start of a path to mean “the current user’s home directory”. For example, for your home directory,/home/ubuntu, then ~/Desktop/workshop_files_Bact_Genomics_2023/02_unix_intro is equivalent to /home/ubuntu/Desktop/workshop_files_Bact_Genomics_2023/02_unix_intro. This only works if it is the first character in the path: here/there/~/elsewhere is not here/there/home/ubuntu/elsewhere.\nAnother shortcut is the dash (-) character. cd will translate - into the previous directory I was in, which is faster than having to remember, then type, the full path. This is a very efficient way of moving back and forth between two directories – i.e. if you execute cd - twice, you end up back in the starting directory.\nThe difference between cd .. and cd - is that the former brings you up, while the latter brings you back.\n\nTry it! First navigate to ~/Desktop/workshop_files_Bact_Genomics_2023 (you should already be there).\ncd ~/Desktop/workshop_files_Bact_Genomics_2023\nThen cd into the exercise-data/creatures directory\ncd 02_unix_intro/bacteria_rpob\nNow if you run\ncd -\nyou’ll see you’re back in ~/Desktop/workshop_files_Bact_Genomics_2023. Run cd - again and you’re back in ~/Desktop/workshop_files_Bact_Genomics_2023/02_unix_intro/bacteria_rpob\n\n\n\n\n\n\n\n\nTab completion\n\n\n\nSometimes file and directory names get too long and it’s tedious to have to type the full name for example when moving with cd. We can let the shell do most of the work > through what is called tab completion. Let’s say we are in the /home/ubuntu/Desktop/workshop_files_Bact_Genomics_2023/02_unix_intro/ and we type:\nls nor\nand then press Tab or ↹(the tab key on the keyboard), the shell automatically completes the directory name:\nls north-pacific-gyre/\nIf we press Tab or ↹ again, Bash will add 2012-07-03/ to the command, since it’s the only possible completion (NB. this may not be the case if there is another directory in there). Pressing Tab or ↹ again does nothing, since there are 19 possibilities; pressing Tab or ↹ twice brings up a list of all the files, and so on. This is very useful in practise and we will see more of it later.\n\n\n\n\n\n\n\n\nExercise 2.2.2.1: Absolute vs Relative Paths\n\n\n\nStarting from /home/amanda/data, which of the following commands could Amanda use to navigate to her home directory, which is /home/amanda?\n\ncd .\ncd /\ncd /home/amanda\ncd ../..\ncd ~\ncd home\ncd ~/data/..\ncd\ncd ..\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n\n\nNo: . stands for the current directory.\nNo: / stands for the root directory.\nYes: This is an example of using the full absolute path.\nNo: this goes up two levels, i.e. ends in /home.\nYes: ~ stands for the user’s home directory, in this case /home/amanda.\nNo: this would navigate into a directory home in the current directory if it exists.\nYes: unnecessarily complicated, but correct.\nYes: shortcut to go back to the user’s home directory.\nYes: goes up one level.\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2.2.2.2: Relative Path Resolution\n\n\n\nUsing the filesystem diagram below, if pwd displays /Users/thing, what will ls -F ../backup display?\n\n../backup: No such file or directory\n2012-12-01 2013-01-08 2013-01-27\n2012-12-01/ 2013-01-08/ 2013-01-27/\noriginal/ pnas_final/ pnas_sub/\n\n\n\n\nFile System for Challenge Questions\n\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n\n\nNo: there is a directory backup in /Users.\nNo: this is the content of Users/thing/backup, but with .. we asked for one level further up.\nNo: see previous explanation.\nYes: ../backup/ refers to /Users/backup/.\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2.2.2.3: ls Reading Comprehension\n\n\n\nUsing the filesystem diagram below, if pwd displays /Users/backup, and -r tells ls to display things in reverse order, what command(s) will result in the following output:\npnas_sub/ pnas_final/ original/\n\n\n\nFile System for Challenge Questions\n\n\n\nls pwd\nls -r -F\nls -r -F /Users/backup\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n\n\nNo: pwd is not the name of a directory.\nYes: ls without directory argument lists files and directories in the current directory.\nYes: uses the absolute path explicitly."
  },
  {
    "objectID": "materials/02-unix_intro/2.2-files_directories.html#creating-directories",
    "href": "materials/02-unix_intro/2.2-files_directories.html#creating-directories",
    "title": "2.2 Navigating Files and Directories",
    "section": "2.2.3 Creating directories",
    "text": "2.2.3 Creating directories\nWe now know how to explore files and directories, but how do we create them in the first place?\n\nStep 1: see where we are and what we already have\nLet’s go back to our workshop_files_Bact_Genomics_2023 directory on the Desktop and use ls -F to see what it contains:\n\n\n\n\n\n\npwd\n/home/ubuntu/Desktop/workshop_files_Bact_Genomics_2023\n\n\n\n\n\n\n\n\n\nls -F\n01_intro_WGS/    02_unix_intro/    03_file_formats/    04_QC/    05_mapping/\n...\n\n\n\n\n\nStep 2: Create a directory\nLet’s create a new directory called thesis using the command mkdir thesis (which has no output):\nmkdir thesis\nAs you might guess from its name, mkdir means “make directory”. Since thesis is a relative path (i.e., does not have a leading slash, like /what/ever/thesis), the new directory is created in the current working directory:\n\n\n\n\n\n\nls -F\n\n01_intro_WGS/    02_unix_intro/    03_file_formats/    04_QC/    05_mapping/\nthesis/\n...\n\n\n\n\nTwo ways of doing the same thing\nUsing the shell to create a directory is no different than using a file explorer. If you open the current directory using your operating system’s graphical file explorer, the thesis directory will appear there too. While the shell and the file explorer are two different ways of interacting with the files, the files and directories themselves are the same.\n\n\nGood names for files and directories\nComplicated names of files and directories can make your life painful when working on the command line. Here we provide a few useful tips for the names of your files.\n\n\n\n\n\n\n1. Don’t use spaces.\n\n\n\nSpaces can make a name more meaningful, but since spaces are used to separate arguments on the command line it is better to avoid them in names of files and directories. You can use - or _ instead (e.g. north-pacific-gyre/ rather than north pacific gyre/).\n\n\n\n\n\n\n\n\n2. Don’t begin the name with - (dash).\n\n\n\nCommands treat names starting with - as options.\n\n\n\n\n\n\n\n\n3. Stick with letters, numbers, . (period or ‘full stop’), - (dash) and _ (underscore).\n\n\n\nMany other characters have special meanings on the command line. We will learn about some of these during this lesson. There are special characters that can cause your command to not work as expected and can even result in data loss.\n\n\nIf you need to refer to names of files or directories that have spaces or other special characters, you should surround the name in quotes (\"\").\n\nSince we’ve just created the thesis directory, there’s nothing in it yet:\nls -F thesis\n\n\nStep 3: Create a text file\nLet’s change our working directory to thesis using cd, then run a text editor called Nano to create a file called firstdraft.txt:\ncd thesis\nnano firstdraft.txt\n\nWhich Editor?\nWhen we say, “nano is a text editor,” we really do mean “text”: it can only work with plain character data, not tables, images, or any other human-friendly media. We use it in examples because it is one of the least complex text editors. However, because of this trait, it may not be powerful enough or flexible enough for the work you may need to do after this workshop.\nOn Unix systems (such as Linux and Mac OS X), many programmers use Emacs or Vim (both of which require more time to learn), or a graphical editor such as Gedit. On Windows, you may wish to use Notepad++. Windows also has a built-in editor called notepad that can be run from the command line in the same way as nano for the purposes of this lesson.\nNo matter what editor you use, you will need to know where it searches for and saves files. If you start it from the shell, it will (probably) use your current working directory as its default location. If you use your computer’s start menu, it may want to save files in your desktop or documents directory instead. You can change this by navigating to another directory the first time you “Save As…”\n\nLet’s type in a few lines of text. Once we’re happy with our text, we can press Ctrl+O (press the Ctrl or Control key and, while holding it down, press the O key) to write our data to disk (we’ll be asked what file we want to save this to: press Return or Enter to accept the suggested default of firstdraft.txt).\n\n\n\nNano in Action\n\n\nOnce our file is saved, we can use Ctrl-X to quit the editor and return to the shell. Note that pressing Ctrl-X without previously saving the file can achieve both aim of saving and exiting. You can try this out.\n\nControl, Ctrl, or ^ Key\nThe Control key is also called the Ctrl key. There are various ways in which using the Control key may be described. For example, you may see an instruction to press the Control key and, while holding it down, press the X key, described as any of:\n\nControl-X\nControl+X\nCtrl-X\nCtrl+X\n^X\nC-x\n\nIn nano, along the bottom of the screen you’ll see ^G Get Help ^O WriteOut. This means that you can use Control-G to get help and Control-O to save your file.\n\nnano doesn’t leave any output on the screen after it exits, but ls now shows that we have created a file called firstdraft.txt:\n\n\n\n\n\n\nls\nfirstdraft.txt\n\n\n\n\n\n\n\n\n\nExercise 2.2.3.1: Creating Files a Different Way\n\n\n\nWe have seen how to create text files using the nano editor. Now, try the following command:\ntouch my_file.txt\n\nWhat did the touch command do? When you look at your current directory using the GUI file explorer, does the file show up?\nUse ls -l to inspect the files. How large is my_file.txt?\nWhen might you want to create a file this way?\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n\n\nThe touch command generates a new file called my_file.txt in your current directory. You can observe this newly generated file by typing ls at the command line prompt. my_file.txt can also be viewed in your GUI file explorer.\nWhen you inspect the file with ls -l, note that the size of my_file.txt is 0 bytes. In other words, it contains no data. If you open my_file.txt using your text editor it is blank.\nSome programs do not generate output files themselves, but instead require that empty files have already been generated. When the program is run, it searches for an existing file to populate with its output. The touch command allows you to efficiently generate a blank text file to be used by such programs.\n\n\n\n\n\n\n\nWhat’s In A Name?\nYou may have noticed that all of the files in our data directory are named “something dot something”, and in this part of the lesson, we always used the extension .txt. This is just a convention: we can call a file mythesis or almost anything else we want. However, most people use two-part names most of the time to help them (and their programs) tell different kinds of files apart. The second part of such a name is called the filename extension, and indicates what type of data the file holds: .txt signals a plain text file, .pdf indicates a PDF document, .cfg is a configuration file full of parameters for some program or other, .png is a PNG image, and so on.\nThis is just a convention, albeit an important one. Files contain bytes: it’s up to us and our programs to interpret those bytes according to the rules for plain text files, PDF documents, configuration files, images, and so on.\nNB. Naming a PNG image of a whale as whale.mp3 doesn’t somehow magically turn it into a recording of whalesong, though it might cause the operating system to try to open it with a music player when someone double-clicks it."
  },
  {
    "objectID": "materials/02-unix_intro/2.2-files_directories.html#moving-files-and-directories",
    "href": "materials/02-unix_intro/2.2-files_directories.html#moving-files-and-directories",
    "title": "2.2 Navigating Files and Directories",
    "section": "2.2.4 Moving files and directories",
    "text": "2.2.4 Moving files and directories\nReturning to the workshop_files_Bact_Genomics_2023 directory,\ncd ~/Desktop/workshop_files_Bact_Genomics_2023/\nIn our thesis directory we have a file firstdraft.txt which isn’t a particularly informative name, so let’s change the file’s name using mv, which is short for “move”:\nmv thesis/firstdraft.txt thesis/second_draft.txt\nThe first argument tells mv what we are “moving”, while the second is where it’s to go. In this case, we are moving thesis/firstdraft.txt to thesis/seconddrafts.txt, which has the same effect as renaming the file. Sure enough, ls shows us that thesis now contains one file called second_draft.txt:\n\n\n\n\n\n\nls thesis\nsecond_draft.txt\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nOne has to be careful when specifying the target file name, since mv will silently overwrite any existing file with the same name, which could lead to data loss.\n\n\nAn additional option, mv -i (or mv --interactive), can be used to make mv ask you for confirmation before overwriting.\nNote that mv also works on directories.\nLet’s move second_draft.txt into the current working directory. We use mv once again, but this time we’ll just use the name of a directory as the second argument to tell mv that we want to keep the filename, but put the file somewhere new. (This is why the command is called “move”.) In this case, the directory name we use is the special directory name . that we mentioned earlier.\nmv thesis/second_draft.txt .\nThe effect is to move the file from the directory it was in to the current working directory (.). ls now shows us that thesis is empty:\nls thesis\nFurther, ls with a filename or directory name as an argument only lists that file or directory. We can use this to see that second_draft.txt is still in our current directory:\n\n\n\n\n\n\nls second_draft.txt\nsecond_draft.txt"
  },
  {
    "objectID": "materials/02-unix_intro/2.2-files_directories.html#copying-files-and-directories",
    "href": "materials/02-unix_intro/2.2-files_directories.html#copying-files-and-directories",
    "title": "2.2 Navigating Files and Directories",
    "section": "2.2.5 Copying files and directories",
    "text": "2.2.5 Copying files and directories\nThe cp command works very much like mv, except it copies a file instead of moving it. We can check that it did the right thing using ls with two paths as arguments — like most Unix commands, ls can be given multiple paths at once:\n\n\n\n\n\n\ncp second_draft.txt thesis/third_draft.txt\nls second_draft.txt thesis/third_draft.txt\nsecond_draft.txt   thesis/third_draft.txt\n\n\n\nWe can also copy a directory and all its contents by using the recursive option -r, e.g. to back up a directory:\ncp -r thesis thesis_backup\nWe can check the result by listing the contents of both the thesis and thesis_backup directory:\n\n\n\n\n\n\nls thesis thesis_backup\nthesis:\nthird_draft.txt\n\nthesis_backup:\nthird_draft.txt\n\n\n\n\n\n\n\n\n\nExercise 2.2.5.1: Renaming Files\n\n\n\nSuppose that you created a plain-text file in your current directory to contain a list of the statistical tests you will need to do to analyze your data, and named it: statstics.txt After creating and saving this file you realize you misspelled the filename! You want to correct the mistake, which command could you use to do so?\n\ncp statstics.txt statistics.txt\nmv statstics.txt statistics.txt\nmv statstics.txt .\ncp statstics.txt .\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n\n\nNo. While this would create a file with the correct name, the incorrectly named file still exists in the directory and would need to be deleted.\nYes, this would work to rename the file.\nNo, the period(.) indicates where to move the file, but does not provide a new file name; identical file names cannot be created.\nNo, the period(.) indicates where to copy the file, but does not provide a new file name; identical file names cannot be created.\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2.2.5.2: Moving and Copying\n\n\n\nWhat is the output of the closing ls command in the sequence shown below? You can try the commands starting from /home/ubuntu/Desktop/workshop_files_Bact_Genomics_2023/02_unix_intro/mycobacteria_rpob.\npwd\n/home/ubuntu/Desktop/workshop_files_Bact_Genomics_2023/02_unix_intro/mycobacteria_rpob\nls\nMycobacterium_tuberculosis_H37Rv_rpob.fasta\nmkdir recombine\nmv Mycobacterium_tuberculosis_H37Rv_rpob.fasta recombine/\ncp recombine/Mycobacterium_tuberculosis_H37Rv_rpob.fasta ../Mycobacterium_tuberculosis_H37Rv_rpob-saved.fasta\nls\n\nMycobacterium_tuberculosis_H37Rv_rpob-saved.fasta recombine\nrecombine\nMycobacterium_tuberculosis_H37Rv_rpob.fasta recombine\nMycobacterium_tuberculosis_H37Rv_rpob-saved.fasta\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n\n\nWe start in the /home/ubuntu/Desktop/workshop_files_Bact_Genomics_2023/02_unix_intro/mycobacteria_rpob directory, and create a new folder called recombine.\nThe second line moves (mv) the file Mycobacterium_tuberculosis_H37Rv_rpob.fasta to the new folder (recombine).\nThe third line makes a copy of the file we just moved.\n\nThe tricky part here is where the file was copied to. Recall that .. means “go up a level”, so the copied file is now in /home/ubuntu/Desktop/workshop_files_Bact_Genomics_2023/02_unix_intro. - Notice that .. is interpreted with respect to the current working directory, not with respect to the location of the file being copied. So, the only thing that will show using ls (in /home/ubuntu/Desktop/workshop_files_Bact_Genomics_2023/02_unix_intro/mycobacteria_rpob) is the recombine folder.\n\nNo, see explanation above. Mycobacterium_tuberculosis_H37Rv_rpob-saved.fasta is located in /home/ubuntu/Desktop/workshop_files_Bact_Genomics_2023/02_unix_intro\nYes\nNo, see explanation above. Mycobacterium_tuberculosis_H37Rv_rpob.fasta is located at /home/ubuntu/Desktop/workshop_files_Bact_Genomics_2023/02_unix_intro/mycobacteria_rpob/recombine\nNo, see explanation above. Mycobacterium_tuberculosis_H37Rv_rpob-saved.fasta is located at /home/ubuntu/Desktop/workshop_files_Bact_Genomics_2023/02_unix_intro"
  },
  {
    "objectID": "materials/02-unix_intro/2.2-files_directories.html#removing-files-and-directories",
    "href": "materials/02-unix_intro/2.2-files_directories.html#removing-files-and-directories",
    "title": "2.2 Navigating Files and Directories",
    "section": "2.2.6 Removing files and directories",
    "text": "2.2.6 Removing files and directories\nReturning to the workshop_files_Bact_Genomics_2023 directory, let’s tidy up this directory by removing the second_draft.txt file we created. The Unix command we will use for this is rm (short for ‘remove’):\nrm second_draft.txt\nWe can confirm the file has gone using ls:\n\n\n\n\n\n\nls second_draft.txt\nls: cannot access 'second_draft.txt': No such file or directory\n\n\n\nYou can also check by simply executing ls\nls\n\n\n\n\n\n\nDeleting Is Forever\n\n\n\nThe Unix shell doesn’t have a trash bin that we can recover deleted files from (though most graphical interfaces to Unix do). Instead, when we delete files, they are unlinked from the file system so that their storage space on disk can be recycled. Tools for finding and recovering deleted files do exist, but there is no guarantee they will work in any particular situation, since the computer may recycle the file’s disk space right away.\n\n\n\n\n\n\n\n\nExercise 2.2.6.1: Using rm Safely\n\n\n\nWhat happens when we execute rm -i thesis_backup/third_draft.txt? Why would we want this protection when using rm?\n\n\n\n\n\n\nSolution:\n\n\n\n\n\nrm: remove regular file 'thesis_backup/third_draft.txt'? y\nThe -i option will prompt before (every) removal (use Y to confirm deletion or N to keep the file). - The Unix shell doesn’t have a trash bin, so all the files removed will disappear forever. - By using the -i option, we have the chance to check that we are deleting only the files that we want to remove.\n\n\n\n\n\nIf we try to remove the thesis directory using rm thesis, we get an error message:\n\n\n\n\n\n\nrm thesis\nrm: cannot remove `thesis': Is a directory\n\n\n\nThis happens because rm by default only works on files, not directories.\nrm can remove a directory and all its contents if we use the recursive option -r, and it will do so without any confirmation prompts:\nrm -r thesis\nGiven that there is no way to retrieve files deleted using the shell, rm -r should be used with great caution (you might consider adding the interactive option rm -r -i)."
  },
  {
    "objectID": "materials/02-unix_intro/2.2-files_directories.html#operations-with-multiple-files-and-directories",
    "href": "materials/02-unix_intro/2.2-files_directories.html#operations-with-multiple-files-and-directories",
    "title": "2.2 Navigating Files and Directories",
    "section": "2.2.7 Operations with multiple files and directories",
    "text": "2.2.7 Operations with multiple files and directories\nOftentimes one needs to copy or move several files at once. This can be done by providing a list of individual filenames, or specifying a naming pattern using wildcards.\n\n\n\n\n\n\nExercise 2.2.7.1: Copy with Multiple Filenames\n\n\n\nFor this exercise, you can test the commands in the workshop_files_Bact_Genomics_2023/02_unix_intro directory. In the example below, what does cp do when given several filenames and a directory name?\nmkdir backup\ncp nucleotides.txt bacteria.txt backup/\nIn the example below, what does cp do when given three or more file names?\n\n\n\n\n\n\nls -F\n\nnucleotides.txt  bacteria.txt  backup/  elements/  morse.txt  \npdb/  planets.txt  salmon.txt  sunspot.txt\n\n\n\ncp nucleotides.txt bacteria.txt morse.txt \n\n\n\n\n\n\nSolution:\n\n\n\n\n\nIf given more than one file name followed by a directory name (i.e. the destination directory must be the last argument), cp copies the files to the named directory. If given three file names, cp throws an error such as the one below, because it is expecting a directory name as the last argument.\ncp: target ‘morse.txt’ is not a directory\n\n\n\n\n\n\nUsing wildcards for accessing multiple files at once\n\nWildcards\n\n\n\n\n\n\n*\n\n\n\n* is a wildcard, which matches zero or more characters. Let’s consider the workshop_files_Bact_Genomics_2023/02_unix_intro/molecules directory: *.pdb matches ethane.pdb, propane.pdb, and every file that ends with ‘.pdb’. On the other hand, p*.pdb only matches pentane.pdb and propane.pdb, because the ‘p’ at the front only matches filenames that begin with the letter ‘p’.\n\n\n\n\n\n\n\n\n?\n\n\n\n? is also a wildcard, but it matches exactly one character. So ?ethane.pdb would match methane.pdb whereas *ethane.pdb matches both ethane.pdb, and methane.pdb.\n\n\nWildcards can be used in combination with each other e.g. ???ane.pdb matches three characters followed by ane.pdb, giving cubane.pdb  ethane.pdb  octane.pdb.\nWhen the shell sees a wildcard, it expands the wildcard to create a list of matching filenames before running the command that was asked for. As an exception, if a wildcard expression does not match any file, Bash will pass the expression as an argument to the command as it is. For example typing ls *.pdf in the molecules directory (which contains only files with names ending with .pdb) results in an error message that there is no file called *.pdf.\nHowever, generally commands like wc and ls see the lists of file names matching these expressions, but not the wildcards themselves. It is the shell, not the other programs, that deals with expanding wildcards, and this is an example of orthogonal design.\n\n\n\n\n\n\nExercise 2.2.7.2: List filenames matching a pattern\n\n\n\nWhen run in the molecules directory, which ls command(s) will produce this output?\nethane.pdb   methane.pdb\n\nls *t*ane.pdb\nls *t?ne.*\nls *t??ne.pdb\nls ethane.*\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n\nThe answer is 3.\n1. shows all files whose names contain zero or more characters (*) followed by the letter t, then zero or more characters (*) followed by ane.pdb. This gives ethane.pdb  methane.pdb  octane.pdb  pentane.pdb.\n2. shows all files whose names start with zero or more characters (*) followed by the letter t, then a single character (?), then ne. followed by zero or more characters (*). This will give us octane.pdb and pentane.pdb but doesn’t match anything which ends in thane.pdb.\n3. fixes the problems of option 2 by matching two characters (??) between t and ne. This is the solution.\n4. only shows files starting with ethane..\n\n\n\n\n\n\n\n\n\n\n\nExercise 2.2.7.3: More on Wildcards\n\n\n\nLook into the exp directory in ~/Desktop/workshop_files_Bact_Genomics_2023/02_unix_intro/exp that has the following structure:\nexp\n├── 2015-10-23-calibration.txt\n├── 2015-10-23-dataset1.txt\n├── 2015-10-23-dataset2.txt\n├── 2015-10-23-dataset_overview.txt\n├── 2015-10-26-calibration.txt\n├── 2015-10-26-dataset1.txt\n├── 2015-10-26-dataset2.txt\n├── 2015-10-26-dataset_overview.txt\n├── 2015-11-23-calibration.txt\n├── 2015-11-23-dataset1.txt\n├── 2015-11-23-dataset2.txt\n├── 2015-11-23-dataset_overview.txt\nSuppose you want to:\n\nbackup all dataset file to backup/datasets and all calibration files to backup/calibration\ncopy all the dataset files created on the 23rd to send_to_bob/all_datasets_created_on_a_23rd and all November files (calibration) and dataset) to send_to_bob/all_november_files so you can send to a colleague.\n\nWhich commands would you use to do that?\nThe resulting directory structure should look like this\nexp\n├── 2015-10-23-calibration.txt\n├── 2015-10-23-dataset1.txt\n├── 2015-10-23-dataset2.txt\n├── 2015-10-23-dataset_overview.txt\n├── 2015-10-26-calibration.txt\n├── 2015-10-26-dataset1.txt\n├── 2015-10-26-dataset2.txt\n├── 2015-10-26-dataset_overview.txt\n├── 2015-11-23-calibration.txt\n├── 2015-11-23-dataset1.txt\n├── 2015-11-23-dataset2.txt\n├── 2015-11-23-dataset_overview.txt\n├── backup\n│   ├── calibration\n│   │   ├── 2015-10-23-calibration.txt\n│   │   ├── 2015-10-26-calibration.txt\n│   │   └── 2015-11-23-calibration.txt\n│   └── datasets\n│       ├── 2015-10-23-dataset1.txt\n│       ├── 2015-10-23-dataset2.txt\n│       ├── 2015-10-23-dataset_overview.txt\n│       ├── 2015-10-26-dataset1.txt\n│       ├── 2015-10-26-dataset2.txt\n│       ├── 2015-10-26-dataset_overview.txt\n│       ├── 2015-11-23-dataset1.txt\n│       ├── 2015-11-23-dataset2.txt\n│       └── 2015-11-23-dataset_overview.txt\n└── send_to_bob\n    ├── all_datasets_created_on_a_23rd\n    │   ├── 2015-10-23-dataset1.txt\n    │   ├── 2015-10-23-dataset2.txt\n    │   ├── 2015-10-23-dataset_overview.txt\n    │   ├── 2015-11-23-dataset1.txt\n    │   ├── 2015-11-23-dataset2.txt\n    │   └── 2015-11-23-dataset_overview.txt\n    └── all_november_files\n        ├── 2015-11-23-calibration.txt\n        ├── 2015-11-23-dataset1.txt\n        ├── 2015-11-23-dataset2.txt\n        └── 2015-11-23-dataset_overview.txt\n\n\n\n\n\n\nSolution:\n\n\n\n\n\n\nSolution is scheduled to be released after class. \n\n\n\n\n\n\n\n\n\n\n\nExercise 2.2.7.4: Reproduce a folder structure\n\n\n\nYou’re starting a new experiment, and would like to duplicate the directory structure from your previous experiment so you can add new data.\nAssume that the previous experiment is in a folder called ‘2016-05-18’, which contains a data folder that in turn contains folders named raw and processed that contain data files. The goal is to copy the folder structure of the 2016-05-18-data folder into a folder called 2016-05-20 so that your final directory structure looks like this:\n2016-05-20/\n└── data\n    ├── processed\n    └── raw\nWhich of the following set of commands would achieve this objective? What would the other commands do? Try them out in the workshop_files_Bact_Genomics_2023/02_unix_intro directory.\n\n1.\nmkdir 2016-05-20\nmkdir 2016-05-20/data\nmkdir 2016-05-20/data/processed\nmkdir 2016-05-20/data/raw\n\n\n2.\nmkdir 2016-05-20\ncd 2016-05-20\nmkdir data\ncd data\nmkdir raw processed\n\n\n3.\nmkdir 2016-05-20/data/raw\nmkdir 2016-05-20/data/processed\n\n\n4.\nmkdir 2016-05-20\ncd 2016-05-20\nmkdir data\nmkdir raw processed\n\n\n\n\n\n\nSolution:\n\n\n\n\n\nThe first two sets of commands achieve this objective. The first set uses relative paths to create the top level directory before the subdirectories.\nThe third set of commands will give an error because mkdir won’t create a subdirectory of a non-existent directory: the intermediate level folders must be created first.\nThe final set of commands generates the ‘raw’ and ‘processed’ directories at the same level as the ‘data’ directory."
  },
  {
    "objectID": "materials/02-unix_intro/2.2-files_directories.html#credit",
    "href": "materials/02-unix_intro/2.2-files_directories.html#credit",
    "title": "2.2 Navigating Files and Directories",
    "section": "2.2.8 Credit",
    "text": "2.2.8 Credit\nInformation on this page has been adapted and modified from the following source(s):\n\nGabriel A. Devenyi (Ed.), Gerard Capes (Ed.), Colin Morris (Ed.), Will Pitchers (Ed.),Greg Wilson, Gerard Capes, Gabriel A. Devenyi, Christina Koch, Raniere Silva, Ashwin Srinath, … Vikram Chhatre. (2019, July). swcarpentry/shell-novice: Software Carpentry: the UNIX shell, June 2019 (Version v2019.06.1).\nhttps://github.com/cambiotraining/UnixIntro"
  },
  {
    "objectID": "materials/02-unix_intro/2.8-bonus_loops.html",
    "href": "materials/02-unix_intro/2.8-bonus_loops.html",
    "title": "2.8 Bonus: Loops",
    "section": "",
    "text": "Teaching: 40 min || Exercises: 10 min"
  },
  {
    "objectID": "materials/02-unix_intro/2.8-bonus_loops.html#overview",
    "href": "materials/02-unix_intro/2.8-bonus_loops.html#overview",
    "title": "2.8 Bonus: Loops",
    "section": "Overview",
    "text": "Overview\n\n\n\n\n\n\n\n\n\n\n\n\nQuestions:\n\n\n\n\nHow can I perform the same actions on many different files?\n\n\n\n\n\n\n\n\n\nLearning Objectives:\n\n\n\n\nWrite a loop that applies one or more commands separately to each file in a set of files.\nTrace the values taken on by a loop variable during execution of the loop.\nExplain the difference between a variable’s name and its value.\nExplain why spaces and some punctuation characters shouldn’t be used in file names.\nDemonstrate how to see what commands have recently been executed.\nRe-run recently executed commands without retyping them.\n\n\n\n\n\n\n\n\n\nKey Points:\n\n\n\n\nA for loop repeats commands once for everything in a list.\nEvery for loop needs a variable to refer to the thing it is currently operating on.\nUse $name to expand a variable (i.e., get its value). ${name} can also be used.\nDo not use spaces, quotes, or wildcard characters such as ’*’ or ‘?’ in filenames, as it complicates variable expansion.\nGive files consistent names that are easy to match with wildcard patterns to make it easy to select them for looping.\nUse the up-arrow key to scroll up through previous commands to edit and repeat them.\nUse Ctrl-R to search through the previously entered commands.\nUse history to display recent commands, and !number to repeat a command by number.\nYou can use the echo command to do a dry-run of the commands in the loop to check what they would do, but without actually running them.\nTwo other commands that can be useful when looping through files are:\n\ndirname to extract the directory name from a path.\nbasename to extract the file name from a path. Using basename  <path>  <suffix> will return the file name without the specified suffix (this is useful to extract the file name without the extension)."
  },
  {
    "objectID": "materials/02-unix_intro/2.8-bonus_loops.html#background",
    "href": "materials/02-unix_intro/2.8-bonus_loops.html#background",
    "title": "2.8 Bonus: Loops",
    "section": "2.8.1 Background",
    "text": "2.8.1 Background\nLoops are a programming construct which allow us to repeat a command or set of commands for each item in a list. As such they are key to productivity improvements through automation. Similar to wildcards and tab completion, using loops also reduces the amount of typing required (and hence reduces the number of typing mistakes).\nSuppose we have several hundred genome data files named basilisk.dat, minotaur.dat, and unicorn.dat. For this example, we’ll use the 02_unix_intro/creatures sub-directory which only has three example files, but the principles can be applied to many more files at once.\nThe structure of these files is the same: the common name, classification, and updated date are presented on the first three lines, with DNA sequences on the following lines. Let’s look at the files:\nhead -n 5 basilisk.dat minotaur.dat unicorn.dat\nWe would like to print out the classification for each species, which is given on the second line of each file. For each file, we would need to execute the command head -n 2 and pipe this to tail -n 1. We’ll use a loop to solve this problem, but first let’s look at the general form of a loop."
  },
  {
    "objectID": "materials/02-unix_intro/2.8-bonus_loops.html#the-for-loop",
    "href": "materials/02-unix_intro/2.8-bonus_loops.html#the-for-loop",
    "title": "2.8 Bonus: Loops",
    "section": "2.8.2 The for loop",
    "text": "2.8.2 The for loop\nThe general synthax for the for loop is:\nfor thing in list_of_things\ndo\n    operation_using $thing    # Indentation within the loop is not required, but aids legibility\ndone\nand we can apply this to our example like this:\n\n\n\n\n\n\nfor filename in basilisk.dat minotaur.dat unicorn.dat\ndo\n    head -n 2 $filename | tail -n 1\ndone\nCLASSIFICATION: basiliscus vulgaris\nCLASSIFICATION: bos hominus\nCLASSIFICATION: equus monoceros\n\n\n\n\n\n\n\n\n\nFollow the Prompt\n\n\n\nThe shell prompt changes from $ to > and back again as we were typing in our loop. The second prompt, >, is different to remind us that we haven’t finished typing a complete command yet. A semicolon, ;, can be used to separate two commands written on a single line.\n\n\nWhen the shell sees the keyword for, it knows to repeat a command (or group of commands) once for each item in a list. Each time the loop runs (called an iteration), an item in the list is assigned in sequence to the variable, and the commands inside the loop are executed, before moving on to the next item in the list. Inside the loop, we call for the variable’s value by putting $ in front of it. The $ tells the shell interpreter to treat the variable as a variable name and substitute its value in its place, rather than treat it as text or an external command.\nIn this example, the list is three filenames: basilisk.dat, minotaur.dat, and unicorn.dat. Each time the loop iterates, it will assign a file name to the variable filename and run the head command. The first time through the loop, $filename is basilisk.dat. The interpreter runs the command head on basilisk.dat and pipes the first two lines to the tail command, which then prints the second line of basilisk.dat. For the second iteration, $filename becomes minotaur.dat. This time, the shell runs head on monotaur.dat and pipes the first two lines to the tail command, which then prints the second line of monotaur.dat. For the third iteration, $filename becomes unicorn.dat, so the shell runs the head command on that file, and tail on the output of that. Since the list was only three items, the shell exits the for loop.\n\n\n\n\n\n\nSame Symbols, Different Meanings\n\n\n\nHere we see > being used as a shell prompt, whereas > is also used to redirect output. Similarly, $ is used as a shell prompt, but, as we saw earlier, it is also used to ask the shell to get the value of a variable.\nIf the shell prints > or $ then it expects you to type something, and the symbol is a prompt.\nIf you type > or $ yourself, it is an instruction from you that the shell should redirect output or get the value of a variable.\n\n\nWhen using variables it is also possible to put the names into curly braces to clearly delimit the variable name: $filename is equivalent to ${filename}, but is different from ${file}name. You may find this notation in other people’s programs.\nWe have called the variable in this loop filename in order to make its purpose clearer to human readers. The shell itself doesn’t care what the variable is called; if we wrote this loop as:\nfor x in basilisk.dat minotaur.dat unicorn.dat\ndo\n    head -n 2 $x | tail -n 1\ndone\nor:\nfor temperature in basilisk.dat minotaur.dat unicorn.dat\ndo\n    head -n 2 $temperature | tail -n 1\ndone\nit would work exactly the same way.\nDon’t do this. Programs are only useful if people can understand them, so meaningless names (like x) or misleading names (like temperature) increase the odds that the program won’t do what its readers think it does.\nIn the above examples, the variables (thing, filename, x and temperature) could have been given any other name, as long as it is meaningful to both the person writing the code and the person reading it.\nAlso note that loops can be used for other things than filenames, like a list of numbers or a subset of data.\n\n\n\n\n\n\nExercise 2.8.2.1: Write your own loop\n\n\n\nHow would you write a loop that echoes all 10 numbers from 0 to 9? Solution\n\n\n\n\n\n\nSolution:\n\n\n\n\n\nfor loop_variable in 0 1 2 3 4 5 6 7 8 9\ndo\n    echo $loop_variable\ndone\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n\n\n\n\n\n\n\n\nExercise 2.8.2.2: Variables in Loops\n\n\n\nThis exercise refers to the 02_unix_intro/molecules sub-directory. ls gives the following output:\n\n\n\n\n\n\ncubane.pdb  ethane.pdb  methane.pdb  octane.pdb  pentane.pdb  propane.pdb\n\n\n\nWhat is the output of the following code?\nfor datafile in *.pdb\ndo\n    ls *.pdb\ndone\nNow, what is the output of the following code?\nfor datafile in *.pdb\ndo\n    ls $datafile\ndone\nWhy do these two loops give different outputs?\n\n\n\n\n\n\nSolution:\n\n\n\n\n\nThe first code block gives the same output on each iteration through the loop. Bash expands the wildcard *.pdb within the loop body (as well as before the loop starts) to match all files ending in .pdb and then lists them using ls. The expanded loop would look like this:\n\n\n\n\n\n\nfor datafile in cubane.pdb ethane.pdb methane.pdb octane.pdb pentane.pdb\npropane.pdb\ndo\n    ls cubane.pdb ethane.pdb methane.pdb octane.pdb pentane.pdb  propane.pdb\ndone\ncubane.pdb  ethane.pdb  methane.pdb  octane.pdb  pentane.pdb  propane.pdb\ncubane.pdb  ethane.pdb  methane.pdb  octane.pdb  pentane.pdb  propane.pdb\ncubane.pdb  ethane.pdb  methane.pdb  octane.pdb  pentane.pdb  propane.pdb\ncubane.pdb  ethane.pdb  methane.pdb  octane.pdb  pentane.pdb  propane.pdb\ncubane.pdb  ethane.pdb  methane.pdb  octane.pdb  pentane.pdb  propane.pdb\ncubane.pdb  ethane.pdb  methane.pdb  octane.pdb  pentane.pdb  propane.pdb\n\n\n\nThe second code block lists a different file on each loop iteration. The value of the datafile variable is evaluated using $datafile,and then listed using ls.\n\n\n\n\n\n\ncubane.pdb\nethane.pdb\nmethane.pdb\noctane.pdb\npentane.pdb\npropane.pdb\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2.8.2.3: Limiting Sets of Files\n\n\n\nWhat would be the output of running the following loop in the 02_unix_intro/molecules sub-directory?\nfor filename in c*\ndo\n   ls $filename\ndone\n\nNo files are listed.\nAll files are listed.\nOnly cubane.pdb, octane.pdb and pentane.pdb are listed.\nOnly cubane.pdb is listed.\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n\n4 is the correct answer. * matches zero or more characters, so any file name starting with the letter c, followed by zero or more other characters will be matched.\n\n\n\nHow would the output differ from using this command instead?\nfor filename in *c*\ndo\n   ls $filename\ndone\n\nThe same files would be listed.\nAll the files are listed this time.\nNo files are listed this time.\nThe files cubane.pdb and octane.pdb will be listed.\nOnly the file octane.pdb will be listed.\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n\n4 is the correct answer. * matches zero or more characters, so file name with zero or more characters before a letter c and zero or more characters after the letter c will be matched.\n\n\n\n\n\n\n\n\n\n\n\nExercise 2.8.2.4: Saving to a File in a Loop - Part One\n\n\n\nIn the 02_unix_intro/molecules sub-directory, what is the effect of this loop?\nfor alkanes in *.pdb\ndo\n    echo $alkanes\n    cat $alkanes > alkanes.pdb\ndone\n\nPrints cubane.pdb, ethane.pdb, methane.pdb, octane.pdb, pentane.pdb and propane.pdb, and the text from propane.pdb will be saved to a file called alkanes.pdb.\nPrints cubane.pdb, ethane.pdb, and methane.pdb, and the text from all three files would be concatenated and saved to a file called alkanes.pdb.\nPrints cubane.pdb, ethane.pdb, methane.pdb, octane.pdb, and pentane.pdb, and the text from propane.pdb will be saved to a file called alkanes.pdb.\nNone of the above.\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n\n\nThe text from each file in turn gets written to the alkanes.pdb file. However, the file gets overwritten on each loop iteration, so the final content of alkanes.pdb is the text from the propane.pdb file.\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2.8.2.5: Saving to a File in a Loop - Part Two\n\n\n\nAlso in the 02_unix_intro/molecules sub-directory, what would be the output of the following loop?\nfor datafile in *.pdb\ndo\n    cat $datafile >> all.pdb\ndone\n\nAll of the text from cubane.pdb, ethane.pdb, methane.pdb, octane.pdb, and pentane.pdb would be concatenated and saved to a file called all.pdb.\nThe text from ethane.pdb will be saved to a file called all.pdb.\nAll of the text from cubane.pdb, ethane.pdb, methane.pdb, octane.pdb, pentane.pdb and propane.pdb would be concatenated and saved to a file called all.pdb.\nAll of the text from cubane.pdb, ethane.pdb, methane.pdb, octane.pdb, pentane.pdb and propane.pdb would be printed to the screen and saved to a file called all.pdb.\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n\n3 is the correct answer. >> appends to a file, rather than overwriting it with the redirected output from a command. Given the output from the cat command has been redirected, nothing is printed to the screen.\n\n\n\n\n\nLet’s continue with our example in the 02_unix_intro/creatures sub-directory. Here’s a slightly more complicated loop:\nfor filename in *.dat\ndo\n    echo $filename\n    head -n 100 $filename | tail -n 20\ndone\nThe shell starts by expanding *.dat to create the list of files it will process. The loop body then executes two commands for each of those files. The first command, echo, prints its command-line arguments to standard output. For example:\n\n\n\n\n\n\necho hello there\nprints:\nhello there\n\n\n\nIn this case, since the shell expands $filename to be the name of a file, echo $filename prints the name of the file. Note that we can’t write this as:\nfor filename in *.dat\ndo\n    $filename\n    head -n 100 $filename | tail -n 20\ndone\nbecause then the first time through the loop, when $filename expanded to basilisk.dat, the shell would try to run basilisk.dat as a program. Finally, the head and tail combination selects lines 81-100 from whatever file is being processed (assuming the file has at least 100 lines).\n\nDealing with Spaces in Names\nSpaces are used to separate the elements of the list that we are going to loop over. If one of those elements contains a space character, we need to surround it with quotes, and do the same thing to our loop variable. Suppose our data files are named:\nred dragon.dat\npurple unicorn.dat\nTo loop over these files, we would need to add double quotes like this:\nfor filename in \"red dragon.dat\" \"purple unicorn.dat\"\ndo\n    head -n 100 \"$filename\" | tail -n 20\ndone\nIt is simpler to avoid using spaces (or other special characters) in filenames.\nThe files above don’t exist, so if we run the above code, the head command will be unable to find them, however the error message returned will show the name of the files it is expecting:\n\n\n\n\n\n\n\n\n\n\n\n\nError\n\n\n\nhead: cannot open ‘red dragon.dat’ for reading: No such file or directory\nhead: cannot open ‘purple unicorn.dat’ for reading: No such file or directory\n\n\nTry removing the quotes around $filename in the loop above to see the effect of the quote marks on spaces. Note that we get a result from the loop command for unicorn.dat when we run this code in the creatures directory:\n\n\n\n\n\n\nhead: cannot open ‘red’ for reading: No such file or directory\nhead: cannot open ‘dragon.dat’ for reading: No such file or directory\nhead: cannot open ‘purple’ for reading: No such file or directory\nCGGTACCGAA\nAAGGGTCGCG\nCAAGTGTTCC\n...\n\n\n\n\n\n\n\n\nbacking up files with cp\nWe would like to modify each of the files in 02_unix_intro/creatures, but also save a version of the original files, naming the copies original-basilisk.dat and original-unicorn.dat. We can’t use:\ncp *.dat original-*.dat\nbecause that would expand to:\ncp basilisk.dat minotaur.dat unicorn.dat original-*.dat\nThis wouldn’t back up our files, instead we get an error:\n\n\n\n\n\n\nError\n\n\n\ncp: target `original-*.dat' is not a directory\n\n\nThis problem arises when cp receives more than two inputs. When this happens, it expects the last input to be a directory where it can copy all the files it was passed. Since there is no directory named original-*.dat in the creatures directory we get an error.\nInstead, we can use a loop:\nfor filename in *.dat\ndo\n    cp $filename original-$filename\ndone\nThis loop runs the cp command once for each filename. The first time, when $filename expands to basilisk.dat, the shell executes:\ncp basilisk.dat original-basilisk.dat\nThe second time, the command is:\ncp minotaur.dat original-minotaur.dat\nThe third and last time, the command is:\ncp unicorn.dat original-unicorn.dat\nSince the cp command does not normally produce any output, it’s hard to check that the loop is doing the correct thing. However, we learned earlier how to print strings using echo, and we can modify the loop to use echo to print our commands without actually executing them. As such we can check what commands would be run in the unmodified loop.\nThe following diagram shows what happens when the modified loop is executed, and demonstrates how the judicious use of echo is a good debugging technique.\n\n\n\nFor Loop in Action\n\n\n\n\nNelle’s Pipeline: Processing Files\nNelle is now ready to process her data files using goostats — a shell script written by her supervisor. This calculates some statistics from a protein sample file, and takes two arguments:\n\nan input file (containing the raw data)\nan output file (to store the calculated statistics)\n\nSince she’s still learning how to use the shell, she decides to build up the required commands in stages. Her first step is to make sure that she can select the right input files — remember, these are ones whose names end in ‘A’ or ‘B’, rather than ‘Z’. Starting from her home directory, Nelle types:\n\n\n\n\n\n\ncd north-pacific-gyre/2012-07-03\nfor datafile in NENE*[AB].txt\ndo\n    echo $datafile\ndone\nNENE01729A.txt\nNENE01729B.txt\nNENE01736A.txt\n...\nNENE02043A.txt\nNENE02043B.txt\n\n\n\nHer next step is to decide what to call the files that the goostats analysis program will create. Prefixing each input file’s name with ‘stats’ seems simple, so she modifies her loop to do that:\n\n\n\n\n\n\nfor datafile in NENE*[AB].txt\ndo\n    echo $datafile stats-$datafile\ndone\nNENE01729A.txt stats-NENE01729A.txt\nNENE01729B.txt stats-NENE01729B.txt\nNENE01736A.txt stats-NENE01736A.txt\n...\nNENE02043A.txt stats-NENE02043A.txt\nNENE02043B.txt stats-NENE02043B.txt\n\n\n\nShe hasn’t actually run goostats yet, but now she’s sure she can select the right files and generate the right output filenames.\nTyping in commands over and over again is becoming tedious, though and Nelle is worried about making mistakes, so instead of re-entering her loop, she presses the up arrow. In response, the shell redisplays the whole loop on one line (using semi-colons to separate the pieces):\nfor datafile in NENE*[AB].txt; do echo $datafile stats-$datafile; done\nUsing the left arrow key, Nelle backs up and changes the command echo to bash goostats:\nfor datafile in NENE*[AB].txt; do bash goostats $datafile stats-$datafile; done\nWhen she presses Enter, the shell runs the modified command. However, nothing appears to happen — there is no output. After a moment, Nelle realizes that since her script doesn’t print anything to the screen any longer, she has no idea whether it is running, much less how quickly. She kills the running command by typing Ctrl-C, uses up-arrow to repeat the command, and edits it to read:\nfor datafile in NENE*[AB].txt; do echo $datafile; bash goostats $datafile stats-$datafile; done\n\n\n\n\n\n\nBeginning and End\n\n\n\nWe can move to the beginning of a line in the shell by typing Ctrl-a and to the end using Ctrl-e.\n\n\nWhen she runs her program now, it produces one line of output every five seconds or so:\n\n\n\n\n\n\nNENE01729A.txt\nNENE01729B.txt\nNENE01736A.txt\n\n\n\n1518 times 5 seconds, divided by 60, tells her that her script will take about two hours to run. As a final check, she opens another terminal window, goes into north-pacific-gyre/2012-07-03, and uses cat stats-NENE01729B.txt to examine one of the output files. It looks good, so she decides to get some coffee and catch up on her reading.\n\n\n\n\n\n\nThose Who Know History Can Choose to Repeat It\n\n\n\nAnother way to repeat previous work is to use the history command to get a list of the last few hundred commands that have been executed, and then to use !123 (where ‘123’ is replaced by the command number) to repeat one of those commands. For example, if Nelle types this:\n\n\n\n\n\n\nhistory | tail -n 5\n  456  ls -l NENE0*.txt\n  457  rm stats-NENE01729B.txt.txt\n  458  bash goostats NENE01729B.txt stats-NENE01729B.txt\n  459  ls -l NENE0*.txt\n  460  history\n\n\n\nthen she can re-run goostats on NENE01729B.txt simply by typing !458.\n\n\n:::::\n\n\n\n\n\n\nOther History Commands\n\n\n\nThere are a number of other shortcut commands for getting at the history. - Ctrl-R enters a history search mode ‘reverse-i-search’ and finds the most recent command in your history that matches the text you enter next. Press Ctrl-R one or more additional times to search for earlier matches. - !! retrieves the immediately preceding command (you may or may not find this more convenient than using the up-arrow) - !$ retrieves the last word of the last command. That’s useful more often than you might expect: after bash goostats NENE01729B.txt stats-NENE01729B.txt, you can type less !$ to look at the file stats-NENE01729B.txt, which is quicker than doing up-arrow and editing the command-line.\n\n\n\n\nPerforming a Dry Run\nA loop is a way to do many things at once — or to make many mistakes at once if it does the wrong thing. One way to check what a loop would do is to echo the commands it would run instead of actually running them. Suppose we want to preview the commands the following loop will execute without actually running those commands:\nfor file in *.pdb\ndo\n  analyze $file > analyzed-$file\ndone\n\n\n\n\n\n\nExercise 2.8.2.6: Dry run with echo\n\n\n\nWhat is the difference between the two loops below, and which one would we want to run?\n# Version 1\nfor file in *.pdb\ndo\n  echo analyze $file > analyzed-$file\ndone\n# Version 2\nfor file in *.pdb\ndo\n  echo \"analyze $file > analyzed-$file\"\ndone\n\n\n\n\n\n\nSolution:\n\n\n\n\n\nThe second version is the one we want to run. This prints to screen everything enclosed in the quote marks, expanding the loop variable name because we have prefixed it with a dollar sign.\nThe first version redirects the output from the command echo analyze $file to a file, analyzed-$file. A series of files is generated: analyzed-cubane.pdb, analyzed-ethane.pdb etc.\nTry both versions for yourself to see the output! Be sure to open the analyzed-*.pdb files to view their contents.\n\n\n\n\n\n\n\nNested loops\n\n\n\n\n\n\nExercise 2.8.2.7: Nested Loops\n\n\n\nSuppose we want to set up up a directory structure to organize some experiments measuring reaction rate constants with different compounds and different temperatures. What would be the result of the following code:\nfor species in cubane ethane methane\ndo\n    for temperature in 25 30 37 40\n    do\n        mkdir $species-$temperature\n    done\ndone\n\n\n\n\n\n\nSolution:\n\n\n\n\n\nWe have a nested loop, i.e. contained within another loop, so for each species in the outer loop, the inner loop (the nested loop) iterates over the list of temperatures, and creates a new directory for each combination.\nTry running the code for yourself to see which directories are created!\n\n\n\n\n\n\n\n\n\nAlternate solution\n\n\n\n\n\nWe can modify the code so that our command is quoted in an echo command: echo \"mkdir $MOLECULE-$TEMP\".\nThis would be the output:\nmkdir cubane-25\nmkdir cubane-30\nmkdir cubane-37\nmkdir cubane-40\nmkdir ethane-25\nmkdir ethane-30\nmkdir ethane-37\nmkdir ethane-40\nmkdir methane-25\nmkdir methane-30\nmkdir methane-37\nmkdir methane-40\n\n\n\n\n\n\n\n\n\n\n\nExercise 2.8.2.8: Creating a .csv Input File\n\n\n\nThis is an (optional) advanced exercise.\nLet’s consider the files in the 02_unix_intro/sequencing directory (note: it’s not important if you don’t know what sequencing is!). This directory contains the results of an experiment where several samples were processed in two runs of the sequencing machine (run1/ and run2/). For each sample, there are two input files, which have suffix _1.fq.gz and _2.fq.gz.\nThe researcher analysing these files needs to produce a CSV file, which will be used as input to a pipeline, and the format of this file should be:\nsample,input1,input2\nsampleA,run1/sampleA_1.fq.gz,run1/sampleA_2.fq.gz\nsampleB,run1/sampleB_1.fq.gz,run1/sampleB_2.fq.gz\nsampleC,run1/sampleC_1.fq.gz,run1/sampleC_2.fq.gz\nsampleD,run1/sampleD_1.fq.gz,run1/sampleD_2.fq.gz\nsampleE,run2/sampleE_1.fq.gz,run2/sampleE_2.fq.gz\nsampleF,run2/sampleF_1.fq.gz,run2/sampleF_2.fq.gz\nWrite a script that produces this file.\n\n\nHints - don’t hesitate to look at these tips, it’s a challenging exercise!. Don’t worry if you don’t understand anything, you will learn more about scripts in the next chapter\n\n\nUse a for loop to iterate through each sample (remember that each sample has two input files).\nYou can combine multiple wildcards in a path, for example ls run*/*_1.fq.gz would list all files in folders starting with the word “run” and all files within those folders ending in “_1.fq.gz”.\nThe command dirname can be used to extract the directory name from a path. For example: dirname run1/sampleA_1.fq.gz would return “run1” as the result.\nConversely, the command basename can be used to extract the name of a file from a path. For example: basename run1/sampleA_1.fq.gz would return “sampleA_1.fq.gz”. Further, you can also remove a suffix at the end of the name by passing it as a second argument to basename: basename  run1/sampleA_1.fq.gz  _1.fq.gz would only return “sampleA”.\nYou can store the result of a command in a variable with the syntax NAME=$(command). For example, DIR=$(dirname run1/sampleA_1.fq.gz) would create a variable called $DIR storing the result of the command, i.e. “run1”.\n\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n\nBased on all the hints given, here is a script that would achieve the desired result:\n#!/bin/bash\n\n# first create a file with the CSV header (column names)\necho \"sample,input1,input2\" > samplesheet.csv\n\n# for each file ending with `_1.fq.gz` (so we only process each sample once)\nfor FILE in run*/*_1.fq.gz\ndo\n  # extract the directory name of the file\n  DIR=$(dirname $FILE)\n\n  # extract the prefix basename of the file\n  BASE=$(basename $FILE _1.fq.gz)\n\n  # append the name of the sample, and each input file path\n  echo \"${BASE},${DIR}/${BASE}_1.fq.gz,${DIR}/${BASE}_2.fq.gz\" >> samplesheet.csv\ndone\nThis can be incredibly useful, especially for bioinformatic applications, when you may have to process hundreds of samples using standard pipelines. You will later find out the importance of this in module 07"
  },
  {
    "objectID": "materials/02-unix_intro/2.8-bonus_loops.html#credit",
    "href": "materials/02-unix_intro/2.8-bonus_loops.html#credit",
    "title": "2.8 Bonus: Loops",
    "section": "2.8.3 Credit",
    "text": "2.8.3 Credit\nInformation on this page has been adapted and modified from the following source(s):\n\nGabriel A. Devenyi (Ed.), Gerard Capes (Ed.), Colin Morris (Ed.), Will Pitchers (Ed.),Greg Wilson, Gerard Capes, Gabriel A. Devenyi, Christina Koch, Raniere Silva, Ashwin Srinath, … Vikram Chhatre. (2019, July). swcarpentry/shell-novice: Software Carpentry: the UNIX shell, June 2019 (Version v2019.06.1).\nhttps://github.com/cambiotraining/UnixIntro\nhttps://github.com/cambiotraining/unix-shell"
  },
  {
    "objectID": "materials/02-unix_intro/2.6-vscode_overview.html",
    "href": "materials/02-unix_intro/2.6-vscode_overview.html",
    "title": "2.6 Visual Studio Code",
    "section": "",
    "text": "Teaching: 20 min || Exercises: 0 min"
  },
  {
    "objectID": "materials/02-unix_intro/2.6-vscode_overview.html#overview",
    "href": "materials/02-unix_intro/2.6-vscode_overview.html#overview",
    "title": "2.6 Visual Studio Code",
    "section": "Overview",
    "text": "Overview\n\n\n\n\n\n\n\n\n\n\n\n\nQuestions:\n\n\n\n\nHow do we download and install VS Code?\nHow do we communicate with our desktop in VS Code?\nCan I access a terminal and create a script in VS Code?\nHow do we connect to and communicate with remote servers with VS Code?\n\n\n\n\n\n\n\n\n\nLearning Objectives:\n\n\n\n\nHow to download and install VS Code\nOpening a folder in VS Code\nCreate a new script in VS Code\nOpen a Terminal in VS Code\nConnect to and communicate with remote servers in VS Code\n\n\n\n\n\n\n\n\n\nKey Points:\n\n\n\n\nYou can seamlessly create and edit scripts in VS Code.\nVS Code allows you to easily establish communication with remote servers and transfer files between your desktop and a remote server."
  },
  {
    "objectID": "materials/02-unix_intro/2.6-vscode_overview.html#introduction",
    "href": "materials/02-unix_intro/2.6-vscode_overview.html#introduction",
    "title": "2.6 Visual Studio Code",
    "section": "2.6.1 Introduction",
    "text": "2.6.1 Introduction\n\nVisual Studio Code (or VS Code) is a free, fully-featured programming open-source text/code editor available for all major platforms. It is available for Windows, Linux and macOS.\nOne of the strengths of this text editor is the wide range of extensions it offers. One of those extensions is called Remote SHH and allows us to connect to a remote computer (via ssh) and edit files as if they were on our own computer."
  },
  {
    "objectID": "materials/02-unix_intro/2.6-vscode_overview.html#download",
    "href": "materials/02-unix_intro/2.6-vscode_overview.html#download",
    "title": "2.6 Visual Studio Code",
    "section": "2.6.2 Download",
    "text": "2.6.2 Download\nYou can download VS Code from the URL: https://code.visualstudio.com/download by selecting the right platform.\n\n\n\nVS Code download page\n\n\nTo download VS code click on the respective icons, depending on your operating system."
  },
  {
    "objectID": "materials/02-unix_intro/2.6-vscode_overview.html#installation",
    "href": "materials/02-unix_intro/2.6-vscode_overview.html#installation",
    "title": "2.6 Visual Studio Code",
    "section": "2.6.3 Installation",
    "text": "2.6.3 Installation\nWe give brief instructions of how to install VS Code on each major operating system. For more detailed instructions, you may visit the VS Code documentation page.\n\nWindows\n\nOpen the folder where you have downloaded VS code.\nYou will find a installer (VSCodeUserSetup-{version}.exe). Run the installer and follow the setup steps. It will take a few minutes.\nImportant: After installation, open VS Code and go to “File > Preferences > Settings”. In the search box at the top of the settings, search for “EOL”. The top hit should give you an option called “EOL” with the default value “auto”. Change this to “” (this will ensure that the files you edit on Windows are compatible with the Linux operating system).\n\n\n\nmacOS\n\nOpen the folder where you have downloaded VS code. It will be a .zip file.\nExtract the zip contents.\nAfter extracting you will see an app Visual Studio Code.app. Drag Visual Studio Code.app to the Applications folder, so that you will open this using the macOS Launchpad.\n\n\n\nLinux\n\nFor Debian and Ubuntu based distribution, download the .deb file and then follow below instructions for installation.\n\nOpen up the terminal\nMove to the directory where you have downloaded the .deb file.\nRun the following command to install\n\nsudo apt install ./<file-name>.deb\n\nReplace  with the name of the vs-code file you have downloaded.\nIf you’re on an older Linux distribution, you will need to run this instead:\nsudo dpkg -i <file-name>.deb\nsudo apt-get install -f # Install dependencies\n\nIf you have downloaded the .rpm file, then follow below instructions for installation\n\nOpen up the terminal\nMove to the directory where you have downloaded the .rpm file.\nRun the following command to install\n\nsudo dnf install <file-name>.rpm\n\nReplace  with the name of the vs-code file you have downloaded."
  },
  {
    "objectID": "materials/02-unix_intro/2.6-vscode_overview.html#open-a-folder-in-vs-code",
    "href": "materials/02-unix_intro/2.6-vscode_overview.html#open-a-folder-in-vs-code",
    "title": "2.6 Visual Studio Code",
    "section": "2.6.4 Open a folder in VS Code",
    "text": "2.6.4 Open a folder in VS Code\nIf you open VS code for the first time it will look like the following:\n\n\n\nNo folder/directory is open in VS Code\n\n\nYou can open any folder/directory into VS Code in the following ways:\n\nOn the VS Code menu bar, choose File > Open Folder….\nThen browse to the location where you have the folder/directory. Select the folder/directory (don’t double click) and then select Select Folder to open the folder into VS Code.\n\nIf a folder/directory is open then it will look like the following:\n\n\n\nFolder/directory open in VS Code\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf only a file is open on VS Code, you will see that the colour of the bottom bar is purple. If a folder/directory is, then the colour will be blue."
  },
  {
    "objectID": "materials/02-unix_intro/2.6-vscode_overview.html#create-a-new-script-in-vs-code",
    "href": "materials/02-unix_intro/2.6-vscode_overview.html#create-a-new-script-in-vs-code",
    "title": "2.6 Visual Studio Code",
    "section": "2.6.5 Create a new script in VS Code",
    "text": "2.6.5 Create a new script in VS Code\nAssuming you have opened a folder/directory in VS Code. On the left side of the VS Code window, you will see a pane called Explorer. Under Explorer, you will find the folder’s name you have opened.\n\n\n\nVS Code Explorer Pane\n\n\nIf you hover over the folder’s name, you will see four icons appear.\n\n\n\nVS Code hover over folder’s name\n\n\nStarting from the left, they are for:\n\nCreate New File within the current folder.\nCreate New Folder within the current folder.\nRefresh explorer.\nCollapse explorer.\n\nTo create a new file click on the first icon (New File), and then type the name of the file with an extension of .sh and press enter.\n\n\n\nVS Code click on new file\n\n\nIn the centre, you will see that a new tab is open. You will see the title of this tab is the same as the name of your file.\n\n\n\nVS Code new file with file name of my-first-script.sh\n\n\nNow you can begin your coding. Write the following code and save the file:\necho Hello World\nAfter then, you need to save this file. You can save the file by pressing ctrl (on macOS cmd) + S.\n\n\n\nVS Code - My first bash script"
  },
  {
    "objectID": "materials/02-unix_intro/2.6-vscode_overview.html#open-a-terminal-in-vs-code",
    "href": "materials/02-unix_intro/2.6-vscode_overview.html#open-a-terminal-in-vs-code",
    "title": "2.6 Visual Studio Code",
    "section": "2.6.6 Open a terminal in VS Code",
    "text": "2.6.6 Open a terminal in VS Code\nVS Code comes with an integrated terminal that conveniently starts at the root of your workspace (this will be clear below). There are a few different ways to open the integrated terminal:\n\nUsing Keyboard shortcut: Press Ctrl+ ` (ctrl + backtick character).\nUsing VS Code menu bar: View > Terminal.\n\nYou will see that the integrated terminal opens at the bottom.\n\n\n\nVS Code with an integrated terminal\n\n\nTo learn more about the integrated terminal visit https://code.visualstudio.com/docs/editor/integrated-terminal"
  },
  {
    "objectID": "materials/02-unix_intro/2.6-vscode_overview.html#connecting-to-an-hpc",
    "href": "materials/02-unix_intro/2.6-vscode_overview.html#connecting-to-an-hpc",
    "title": "2.6 Visual Studio Code",
    "section": "2.6.7 Connecting to an HPC",
    "text": "2.6.7 Connecting to an HPC\n\nConfiguring Visual Studio Code for connecting to HPC\nWe will use an extension called “Remote-SSH”. To install the extension (see Figure):\n\nClick the “Extensions” button on the side bar (or use Ctrl + Shift + X)\nIn the search box type “remote ssh” and choose the “Remote - SSH” extension\nClick the “Install” button in the window that opens\n\n\n\n\nInstalling Remote-SSH extension in VS Code\n\n\n\n\nConnect VS Code to the HPC\nFollow the instructions below to connect VS Code to the HPC.\n\nClick the “Open Remote Window” green button on the bottom left corner.\nClick “Connect to Host…” in the popup menu that opens.\nClick “+ Add New SSH Host…”.\nType your username and HPC hostname in the same way you do with ssh.\nSelect SSH configuration file to save this information for the future. Select the first file listed in the popup menu (a file in your user’s home under .ssh/config).\nA menu pops open on the bottom right informing the host was added to the configuration file. Click “Connect”.\nYou may be asked what kind of platform you are connecting to. HPC environments always run on Linux.\nThe first time you connect to a host you will also be asked if you trust this computer. You can answer “Continue”.\nFinally, you will be asked for your password. Once you are connected the green button on the bottom-left corner should change to indicate you are ssh’d into the HPC\nTo open a folder on the HPC, use the left-hand “Explorer” and click “Open Folder”\nType the path to the folder on the HPC from where you want to work from and press OK\n\nYou may be asked for your password again. The first time you connect to a folder you will also be asked “Do you trust the authors of the files in this folder?”, to which you can answer “Yes, I trust the authors”.\n\n\n\n\n\nSteps to connect to a remote server with VS Code\n\n\nOnce you are connected to the HPC in this way, you can edit files and even create new files and folders on the HPC filesystem."
  },
  {
    "objectID": "materials/02-unix_intro/2.6-vscode_overview.html#credit",
    "href": "materials/02-unix_intro/2.6-vscode_overview.html#credit",
    "title": "2.6 Visual Studio Code",
    "section": "2.6.8 Credit",
    "text": "2.6.8 Credit\nInformation on this page has been adapted and modified from the following source(s):\n\nhttps://github.com/cambiotraining/sars-cov-2-genomics\nhttps://github.com/cambiotraining/hpc-intro"
  },
  {
    "objectID": "materials/02-unix_intro/2.5-using_remote_resources.html",
    "href": "materials/02-unix_intro/2.5-using_remote_resources.html",
    "title": "2.5 Using remote resources",
    "section": "",
    "text": "Teaching: 15 min || Exercises: 5 min"
  },
  {
    "objectID": "materials/02-unix_intro/2.5-using_remote_resources.html#overview",
    "href": "materials/02-unix_intro/2.5-using_remote_resources.html#overview",
    "title": "2.5 Using remote resources",
    "section": "Overview",
    "text": "Overview\n\n\n\n\n\n\n\n\n\n\n\n\nQuestions:\n\n\n\n\nHow can I work on the unix shell of a remote computer?\nHow can I move files between computers?\nHow can I access web resources using the command line?\nHow do I mount a directory from another computer onto my local filesystem?\nWhen might different remote access tools be more appropriate for a task?\n\n\n\n\n\n\n\n\n\nLearning Objectives:\n\n\n\n\nKnow how to securely connect and work from a remote computer.\nKnow how to copy files to/from a remote computer.\n\n\n\n\n\n\n\n\n\nKey Points:\n\n\n\n\nThe ssh program can be used to securely login to a remote server. The general command is ssh username@remote, where username is the user’s name on the remote machine and remote is the name of that machine (sometimes in the form of an IP address)\nscp a method of copying files using the ssh protocol. This is used to copy files to/from a remote server from your local machine. The syntax for this command is ssh username@remote:path_to_remote_file path_on_local_machine to copy a file from the remote machine to the local machine or ssh path_on_local_machine username@remote:path_to_remote_file vice-versa.\nsshfs a method of using the ssh protocol to connect your local filesystem directly to a remote filesystem as if they were connected\nrsync a different method of file transfer which only copies files that have changed\nwget a command which can download files from http links as if it were in a browser\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nIf you are a self-learner (rather than attending one of our workshops), you will need an account on a remote server to follow this section. You will need to adjust the commands shown on this page to match your username and hostname on that server."
  },
  {
    "objectID": "materials/02-unix_intro/2.5-using_remote_resources.html#background",
    "href": "materials/02-unix_intro/2.5-using_remote_resources.html#background",
    "title": "2.5 Using remote resources",
    "section": "2.5.1 Background",
    "text": "2.5.1 Background\nLet’s take a closer look at what happens when we use the shell on a desktop or laptop computer. The first step is to log in so that the operating system knows who we are and what we’re allowed to do. We do this by typing our username and password; the operating system checks those values against its records, and if they match, runs a shell for us.\nAs we type commands, the 1’s and 0’s that represent the characters we’re typing are sent from the keyboard to the shell. The shell displays those characters on the screen to represent what we type, and then, if what we typed was a command, the shell executes it and displays its output (if any).\nWhat if we want to run some commands on another machine, such as the Noguchi server in the Prince Alwaleed building that manages our database of field results? To do this, we have to first log in to that machine. We call this a remote login.\nIn order for us to be able to login, the remote computer must be running a remote login server and we will run a client program that can talk to that server. The client program passes our login credentials to the remote login server and, if we are allowed to login, that server then runs a shell for us on the remote computer.\nOnce our local client is connected to the remote server, everything we type into the client is passed on, by the server, to the shell running on the remote computer. That remote shell runs those commands on our behalf, just as a local shell would, then sends back output, via the server, to our client, for our computer to display.\nThis phenomenon works just like accessing your email. In fact, if you have used an email address to send mails then you have unknowingly used a remote server. Have you wondered where all your email messages are sitting? Your guess is as good as mine!"
  },
  {
    "objectID": "materials/02-unix_intro/2.5-using_remote_resources.html#the-ssh-protocol",
    "href": "materials/02-unix_intro/2.5-using_remote_resources.html#the-ssh-protocol",
    "title": "2.5 Using remote resources",
    "section": "2.5.2 The ssh protocol",
    "text": "2.5.2 The ssh protocol\nThe Secure SHell (SSH) is a protocol which allows us to send secure encrypted information across an unsecured network, like the internet. The underlying protocol supports a number of commands we can use to move information of different types in different ways. The simplest and most straightforward is the ssh command which facilitates a remote login session connecting our local user and shell to any remote user we have permission to access.\nssh sshuser@remote-machine\nThe first argument specifies the location of the remote machine (by IP address or a URL) as well as the user we want to connect to seperated by an @ sign.  For the purpose of this course we’ve set up a container on our cluster for you to connect to the address remote-machine is actually a special kind of URL that only your computer will understand. In real life this would normally be an address on the internet which can be access from anywhere or at least an institutional local area network.\n\n\n\n\n\n\nThe authenticity of host '[192.168.1.59]:2231 ([192.168.1.59]:2231)' \ncan't be established.\nRSA key fingerprint is SHA256:4X1kUMDOG021U52XDL2U56GFIyC+S5koImofnTHvALk.\nAre you sure you want to continue connecting (yes/no)?\n\n\n\nWhen you connect to a computer for the first time you should see a warning like the one above. This signifies that the computer is trying to prove it’s identity by sending a fingerprint which relates to a key that only it knows. Depending on the security of the server you are connecting to they might distribute the fingerprint ahead of time for you to compare and advise you to double check it in case it changes at a later log on. In our case it is safe to type yes .\nsshuser@192.168.1.59's password: ********\nNow you are prompted for a password. Enter the appropriate password and hit enter.\n    sshuser@remote_machine:~$\nYou should now have a prompt very similar to the one you started with but with a new username and computer hostname. Take a look around with the ls command and you should see that your new session has its own completely independent filesystem. Unfortunately None of the files are particularly relevant to us. Let’s change that, but first we need to go back to our original computer’s shell. Use the key combination Ctrl + D (Ctrl+D) on an empty command prompt to log out."
  },
  {
    "objectID": "materials/02-unix_intro/2.5-using_remote_resources.html#moving-files",
    "href": "materials/02-unix_intro/2.5-using_remote_resources.html#moving-files",
    "title": "2.5 Using remote resources",
    "section": "2.5.3 Moving files",
    "text": "2.5.3 Moving files\nssh has a simple file copying counterpart called secure copy scp which uses all the same methods for authentication and encryption but focuses on moving files between computers in a similar manner to the cp command we learnt about before.\nMaking sure we’re in the 02_unix_intro directory let’s copy the bacteria.txt file to the remote machine\ncd ~/Desktop/workshop_files_Bact_Genomics_2023/02_unix_intro\nscp bacteria.txt sshuser@remote-machine:/home/user\n\n\n\n\n\n\nNote\n\n\n\nYou will be prompted to provide your login credentials while attempting to copy to/from a remote source.\n\n\nThe format of the command should be quite familiar when comparing to the cp command for local copying. The last two arguments specify the source and the destination of the copy respectively. The difference comes in that any remote locations involved in the copy must be preceded by the username@IP syntax used in the ssh command previously. The first half tells scp how to access the computer and the second half tells it where in the filesystem to operate, these two segments are separated by a : .\n\n\n\n\n\n\nA successful copy should produce something like this\nbacteria.txt                                     100%   86   152.8KB/s   00:00\n\n\n\nNow it looks like we’ve copied the file, but we should check.\nEstablishing a whole ssh session just to run one command might be a bit cumbersome. Instead we can tell ssh all the commands it needs to run at the same time we connect by adding an extra argument to the end. ssh will automatically disconnect after it completes the full command string.\n\n\n\n\n\n\nssh sshuser@remote-machine \"ls /home/user/*.txt\"\nbacteria.txt\n\n\n\nSuccess!\n\n\n\n\n\n\nExercise 2.5.3.1: How do we get files back from the remote server?\n\n\n\nUsing two commands we’ve uploaded a file to our server and checked it was there\nscp bacteria.txt sshuser@remote-machine:/home/user\nssh sshuser@remote-machine \"ls /home/user/*.txt\"\nMore often we might want to have the server do some work on our files and then get them back. We can use exactly the same syntax structure with different arguments.\nTry and make a change to bacteria.txt (perhaps add some text to the end with the echo command and >>) in the remote location and then retrieve the changed file. Remember to change the name of the file as you already have a bacteria.txt in your directory.\nIf you’re having trouble check man scp for more information about how to structure the command\n\n\n\n\n\n\nSolution:\n\n\n\n\n\nssh sshuser@remote-machine \"echo all done >> bacteria.txt\"\nscp sshuser@remote-machine:bacteria.txt changed_bacteria.txt"
  },
  {
    "objectID": "materials/02-unix_intro/2.5-using_remote_resources.html#managing-multiple-remote-files",
    "href": "materials/02-unix_intro/2.5-using_remote_resources.html#managing-multiple-remote-files",
    "title": "2.5 Using remote resources",
    "section": "2.5.4 Managing multiple remote files",
    "text": "2.5.4 Managing multiple remote files\nSometimes we need to manage a large number of files across two locations, often maintaining a specific directory structure. scp can handle this with it’s -r flag. Just like cp this puts the command in recursive mode allowing it to copy entire directories of files\n\n\n\n\n\n\nscp -r sshuser@remote-machine:/home/user/workshop_files_Bact_Genomics_2023/02_unix_intro .\nTB_H37Rv_truncated.fasta                        100%  150KB   1.9MB/s   00:00    \nsample4_run2.fastq                              100%  372     7.0KB/s   00:00    \nsample3_run2.fastq                              100%  372     7.6KB/s   00:00    \nsample1_run2.fastq                              100%  370     8.4KB/s   00:00    \nsample5_run2.fastq                              100%  372     7.5KB/s   00:00    \nsample2_run2.fastq                              100%  370    12.7KB/s   00:00    \nG26832.tsv                                      100% 1008KB   7.1MB/s   00:00    \nG26832.tsv                                      100% 1008KB   5.7MB/s   00:00    \nG26832.embl                                     100%   12MB  10.2MB/s   00:01    \nG26832.gff3                                     100% 5698KB   9.6MB/s   00:00    \ncontigs.gfa                                     100% 4314KB   9.3MB/s   00:00    \nG26832_R1.fastq.gz                              100%  218MB  10.6MB/s   00:20    \nG26832.fna                                      100% 4329KB   9.7MB/s   00:00    \nG26832.mlst.tsv                                 100%   89     1.3KB/s   00:00    \nG26832.txt                                      100%  392     8.1KB/s   00:00    \nG26832.ffn                                      100% 4005KB   9.4MB/s   00:00    \n...    \n\n\n\n\n\n\n\n\n\nNote\n\n\n\nDid you realise the format of the above command was different from the previous scp command? Here, we copied files from the remote server unto our local machine. Check in your current directory if you will find the files you just copied.\n\n\nHowever scp isn’t always the best tool to use for managing this kind of operation.\nWhen you run scp it copies the entirety of every single file you specify. For an initial copy this is probably what you want, but if you change only a few files and want to synchronize the server copy to keep up with your changes it wouldn’t make sense to copy the entire directory structure again.\nFor this scenario rsync can be an excellent tool."
  },
  {
    "objectID": "materials/02-unix_intro/2.5-using_remote_resources.html#rsync",
    "href": "materials/02-unix_intro/2.5-using_remote_resources.html#rsync",
    "title": "2.5 Using remote resources",
    "section": "2.5.5 rsync",
    "text": "2.5.5 rsync\nFirst lets add some new files to our 02_unix_intro directory using the touch command. This command does nothing but create an empty file or update the timestamp of an existing file.\ntouch newfile1 newfile2 newfile3\nNow we have everything set up we can issue the rsync command to sync our two directories.\nrsync -avz 02_unix_intro sshuser@remote-machine:/home/user/workshop_files_Bact_Genomics_2023/02_unix_intro\nThe -v flag/option stands for verbose and outputs a lot of status information during the transfer. This is helpful when running the command interactively but should generally be removed when writing scripts.\nThe -z flag/option means that the data will be compressed in transit. This is good practice depending on the speed of your connection (although in our case it is a little unnecessary).\nWhilst we’ve used rsync in mostly the same way as we did scp it has many more customisation options as we can observe on the man page\nman rsync\n-a is useful for preserving filesystem metadata regarding the files being copied. This is particularly relevant for backups or situations where you intend to sync a copy back over your original at a later point.\n--exclude and --include can be used to more granularly control which files are copied.\nFinally --delete is very useful when you want to maintain an exact copy of the source including the deletion of files only present in the destination. Let’s tidy up our 02_unix_intro directory with this option.\n\n\n\n\n\n\nExercise 2.5.5.1: How to remove files using rsync --delete\n\n\n\nFirst we should manually delete our local copies of the empty files we created.\nrm newfile*\nconsulting the output of man rsync synchronize the local folder with the remote folder again. This time causing the remote copies of newfile* to be deleted.\nkeep in mind that for rsync a path with a trailing / means the contents of a directory rather than the directory itself. Think about how using the --delete flag could make it very dangerous to make this mistake.\nDon’t worry too much though, you can always upload a new copy of the data.\n\n\n\n\n\n\nSolution:>\n\n\n\n\n\nrsync -avz --delete 02_unix_intro sshuser@remote-machine:/home/user/workshop_files_Bact_Genomics_2023/02_unix_intro"
  },
  {
    "objectID": "materials/02-unix_intro/2.5-using_remote_resources.html#sshfs",
    "href": "materials/02-unix_intro/2.5-using_remote_resources.html#sshfs",
    "title": "2.5 Using remote resources",
    "section": "2.5.6 SshFs",
    "text": "2.5.6 SshFs\nSshfs is another way of using the same ssh protocol to share files in a slightly different way. This software allows us to connect the file system of one machine with the file system of another using a “mount point”. Let’s start by making a directory in /home/ubuntu/Desktop/workshop_files_Bact_Genomics_2023/02_unix_intro to act as this mount point. Convention tells us to call it mnt.\nmkdir /home/ubuntu/Desktop/workshop_files_Bact_Genomics_2023/02_unix_intro/mnt\nNow we can run the sshfs command\nsshfs sshuser@remote-machine:/home/user /home/ubuntu/Desktop/workshop_files_Bact_Genomics_2023/02_unix_intro/mnt/\nIt looks fairly similar to the previous copying commands. The first argument is a remote source, the second argument is a local destination. The difference is that now whenever we interact with our local mount point it will be as if we were interacting with the remote filesystem starting at the directory we specified /home/sshuser.\n\n\n\n\n\n\ncd /home/ubuntu/Desktop/workshop_files_Bact_Genomics_2023/02_unix_intro/mnt/\nls -l\n\ntotal 5824\n-rw-r--r--@  1 user  staff  1626227 Nov 23 12:13 G26832.gff3.gz\n-rw-r--r--@  1 user  staff  1031772 Nov 23 12:27 G26832.tsv\n-rw-r--r--@  1 user  staff     3249 Nov 22 17:23 MTB_H37Rv_selected_proteins.fasta.gz\n-rw-r--r--@  1 user  staff   153474 Nov 22 12:10 MTB_H37Rv_truncated.fasta\n-rw-r--r--@  1 user  staff      784 Nov 22 17:59 TBNmA041_Species_firstset.csv\n-rw-r--r--@  1 user  staff      869 Nov 22 17:59 TBNmA041_Species_secondset.csv\n-rw-r--r--@  1 user  staff    37336 Nov 22 13:35 TBNmA041_annotation_truncated_1.gff3\n-rw-r--r--@  1 user  staff    37336 Nov 22 13:35 TBNmA041_annotation_truncated_1.tsv\n-rw-r--r--@  1 user  staff    53321 Nov 22 13:37 TBNmA041_annotation_truncated_2.gff3\n-rw-r--r--@  1 user  staff      290 Nov 22 16:20 bacteria.txt\ndrwxr-xr-x@  9 user  staff      288 Nov 22 15:25 bacteria_rpob\ndrwxr-xr-x@ 10 user  staff      320 Nov 22 15:58 molecules\n-rw-r--r--@  1 user  staff      554 Oct 12 14:59 morse.txt\ndrwxr-xr-x@  4 user  staff      128 Nov 21 17:44 mycobacteria_rpob\ndrwxr-xr-x@  4 user  staff      128 Nov 21 16:09 north-pacific-gyre\n-rw-r--r--@  1 user  staff      141 Nov 21 18:03 nucleotides.txt\ndrwxr-xr-x@  7 user  staff      224 Nov 22 13:03 sequencing_run1\ndrwxr-xr-x@  7 user  staff      224 Nov 22 13:04 sequencing_run2\n\n\n\nThe files shown are not on the local machine at all but we can still access and edit them. Much like the concept of a shared network drive in Windows.\nThis approach is particularly useful when you need to use a program which isn’t available on the remote server to edit or visualize files stored remotely. Keep in mind however, that every action taken on these files is being encrypted and sent over the network. Using this approach for computationally intense tasks could substantially slow down the time to completion.\nIt’s also worth noting that this isn’t the only way to mount remote directories in linux. Protocols such as nfs and samba are actually more common and may be more appropriate for a given use case. Sshfs has the advantage of running over ssh so it require very little set up on the remote computer."
  },
  {
    "objectID": "materials/02-unix_intro/2.5-using_remote_resources.html#wget---accessing-web-resources",
    "href": "materials/02-unix_intro/2.5-using_remote_resources.html#wget---accessing-web-resources",
    "title": "2.5 Using remote resources",
    "section": "2.5.7 wget - Accessing web resources",
    "text": "2.5.7 wget - Accessing web resources\nwget is in a different category of command compared to the others in this section. It is mainly for accessing resources that can be downloaded over http(s) and doesn’t have a mechanism for uploading/pushing files.\nWhilst this tool can be customised to do a wide range of tasks, at its simplest it can be used to download datasets for processing at the start of a processing pipeline.\nThe data for this course can be downloaded as follows\n\n\n\n\n\n\n\n\n\n\n\nwget https://www.dropbox.com/sh/wgya3rngcl9nngm/AACYjpWnNSmqez_hw1kf0nDPa?dl=0 -O course_data.zip\n--2022-11-23 14:20:41--  https://www.dropbox.com/sh/ggrruxq2w1zb3gn/AAD7lzWEuQlLISC_ZWqRybJia?dl=0\nResolving www.dropbox.com (www.dropbox.com)... 162.125.64.18\nConnecting to www.dropbox.com (www.dropbox.com)|162.125.64.18|:443... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: /sh/raw/ggrruxq2w1zb3gn/AAD7lzWEuQlLISC_ZWqRybJia [following]\n--2022-11-23 14:20:42--  https://www.dropbox.com/sh/raw/ggrruxq2w1zb3gn/AAD7lzWEuQlLISC_ZWqRybJia\nReusing existing connection to www.dropbox.com:443.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://uc2998e7e38ee57258e41fcd6c69.dl.dropboxusercontent.com/zip_download_get/BUvw0Y7LsejbFNGF_CUwWx9ZoNLvYxLa58t5BMxPei6Asg_wxPm2W3Eb7v9EH4deBFDd6nh8VlQwFLrJoeMq1d0bZIHAa_80V9DTaoKZXsgukQ# [following]\n--2022-11-23 14:20:47--  https://uc2998e7e38ee57258e41fcd6c69.dl.dropboxusercontent.com/zip_download_get/BUvw0Y7LsejbFNGF_CUwWx9ZoNLvYxLa58t5BMxPei6Asg_wxPm2W3Eb7v9EH4deBFDd6nh8VlQwFLrJoeMq1d0bZIHAa_80V9DTaoKZXsgukQ\nResolving uc2998e7e38ee57258e41fcd6c69.dl.dropboxusercontent.com (uc2998e7e38ee57258e41fcd6c69.dl.dropboxusercontent.com)... 162.125.64.15\nConnecting to uc2998e7e38ee57258e41fcd6c69.dl.dropboxusercontent.com (uc2998e7e38ee57258e41fcd6c69.dl.dropboxusercontent.com)|162.125.64.15|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 46293314 (44M) [application/zip]\nSaving to: ‘course_data.zip’\n\ncourse_data.zip                    100%[===============================================================>]  44.15M  9.96MB/s    in 5.5s    \n\n2022-11-23 14:20:54 (8.03 MB/s) - ‘course_data.zip’ saved [46293314/46293314]\n\n\n\nFor large files or sets of files there are also a few useful flags/options\n\n-b Places the task in the background and writes all output into a log file\n-c can be used to resume a download that has failed mid-way\n-i can take a file with a list of URLs for downloading\n\nWhere tools like wget shine in particular is in using URLs generated by web-based REST APIs like the one offered by Ensembl .\nWe can use bash to combine strings and form a valid query URL that meets our requirements.\nwget https://rest.ensembl.org/genetree/member/symbol/homo_sapiens/BRCA2?prune_taxon=9526;content-type=text/x-orthoxml%2Bxml;prune_species=cow\n\n\n\n\n\n\nExercise 2.5.7.1: When should we use which tool?\n\n\n\nUltimately there is no hard right answer to this and any tool that works, works. That said, can you think of a scenario where you think each of the following might be the best choice.\nscp rsync wget\n\n\n\n\n\n\nSolution:\n\n\n\n\n\nscp - Any time you’re copying a file or set of files once and in one direction scp makes sense\nrsync - This is a good choice when you need to have a local copy of the whole dataset but there is also relatively frequent communication to update the source/destination. It also makes sense when the dataset gets too large to transfer regularly as a whole. If your rsync use is getting complex consider separating the more dynamic files for sophisticated version control with something like git\nwget - Whenever there is a single, canonical, mostly unchanging, web-based source for a piece of data wget is a good choice. Downloading all the data required for a script at the top with a series of wget commands can be good practice to organise your process. If you find wget limiting for this purpose a similar command called curl can be slightly more customisable for use in programming\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nAlthough using these resources is very good especially being able to include them in scripts, currently, there are GUI applications that can enable you communicate just as good with remote servers. We will look at the vscode in our next lesson."
  },
  {
    "objectID": "materials/02-unix_intro/2.5-using_remote_resources.html#credit",
    "href": "materials/02-unix_intro/2.5-using_remote_resources.html#credit",
    "title": "2.5 Using remote resources",
    "section": "2.5.8 Credit",
    "text": "2.5.8 Credit\nInformation on this page has been adapted and modified from the following source(s):\n\nhttps://github.com/cambiotraining/UnixIntro\nhttps://github.com/cambiotraining/unix-shell"
  },
  {
    "objectID": "materials/02-unix_intro/2.4-combining_commands.html",
    "href": "materials/02-unix_intro/2.4-combining_commands.html",
    "title": "2.4 Combining Commands",
    "section": "",
    "text": "Teaching: 20 min || Exercises: 10 min"
  },
  {
    "objectID": "materials/02-unix_intro/2.4-combining_commands.html#overview",
    "href": "materials/02-unix_intro/2.4-combining_commands.html#overview",
    "title": "2.4 Combining Commands",
    "section": "Overview",
    "text": "Overview\n\n\n\n\n\n\n\n\n\n\n\n\nQuestions:\n\n\n\n\nHow can I combine existing commands to do new things?\n\n\n\n\n\n\n\n\n\nLearning Objectives:\n\n\n\n\nRedirect a command’s output to a file.\nProcess a file instead of keyboard input using redirection.\nConstruct command pipelines with two or more stages.\nExplain what usually happens if a program or pipeline isn’t given any input to process.\n\n\n\n\n\n\n\n\n\nKey Points:\n\n\n\n\nOther useful commands to manipulate text and which are often used together in pipes include:\n\ncat displays the contents of its inputs.\nhead displays the first 10 lines of its input.\ntail displays the last 10 lines of its input.\nsort used to order the input lines alphabetically (default) or numerically (-n option).\nwc counts lines, words, and characters in its inputs.\ncut to extract parts of a file that are separated by a delimiter.\nuniq to obtain only unique lines from its input. The -c option can also be used to count how often each of those unique lines occur.\n\ncommand > file redirects a command’s output to a file (overwriting any existing content).\ncommand >> file appends a command’s output to a file.\n< operator redirects input to a command\nThe | pipe allows to chain several commands together.\nfirst | second is a pipeline: the output of the first command is used as the input to the second.\nThe best way to use the shell is to use pipes to combine simple single-purpose programs (filters)."
  },
  {
    "objectID": "materials/02-unix_intro/2.4-combining_commands.html#a-few-more-basic-commands",
    "href": "materials/02-unix_intro/2.4-combining_commands.html#a-few-more-basic-commands",
    "title": "2.4 Combining Commands",
    "section": "2.4.1 A few more basic commands",
    "text": "2.4.1 A few more basic commands\nLet’s go over and perform some basic commands we have already encountered, but this time, we may go deeper and also introduce new commands.\n\nPerforming counts with wc\nWe’ll start with a directory called molecules that contains six files describing some simple organic molecules. The .pdb extension indicates that these files are in Protein Data Bank format, a simple text format that specifies the type and position of each atom in the molecule.\n\n\n\n\n\n\nls molecules\ncubane.pdb    ethane.pdb    methane.pdb\noctane.pdb    pentane.pdb   propane.pdb\n\n\n\nLet’s go into that directory with cd and run the command wc *.pdb. wc is the “word count” command: it counts the number of lines, words, and characters in files (from left to right, in that order).\nThe * in *.pdb matches zero or more characters, so the shell turns *.pdb into a list of all .pdb files in the current directory:\n\n\n\n\n\n\ncd molecules\nwc *.pdb\n  20  156  1158  cubane.pdb\n  12  84   622   ethane.pdb\n   9  57   422   methane.pdb\n  30  246  1828  octane.pdb\n  21  165  1226  pentane.pdb\n  15  111  825   propane.pdb\n 107  819  6081  total\n\n\n\nIf we run wc -l instead of just wc, the output shows only the number of lines per file:\n\n\n\n\n\n\nwc -l *.pdb\n  20  cubane.pdb\n  12  ethane.pdb\n   9  methane.pdb\n  30  octane.pdb\n  21  pentane.pdb\n  15  propane.pdb\n 107  total\n\n\n\n\n\n\n\n\n\nWhy Isn’t It Doing Anything?\n\n\n\nWhat happens if a command is supposed to process a file, but we don’t give it a filename? For example, what if we type:\nwc -l\nbut don’t type *.pdb (or anything else) after the command? Since it doesn’t have any filenames, wc assumes it is supposed to process input given at the command prompt, so it just sits there and waits for us to give it some data interactively. From the outside, though, all we see is it sitting there: the command doesn’t appear to do anything.\nIf you make this kind of mistake, you can escape out of this state by holding down the control key (Ctrl) and typing the letter C once and letting go of the Ctrl key. Ctrl+C\n\n\nWe can also use -w to get only the number of words, or -c to get only the number of characters.\nWhich of these files contains the fewest lines? It’s an easy question to answer when there are only six files, but what if there were 6000? Our first step toward a solution is to run the command:\nwc -l *.pdb > lengths.txt\nThe greater than symbol, >, tells the shell to redirect the command’s output to a file instead of printing it to the screen. (This is why there is no screen output: everything that wc would have printed has gone into the file lengths.txt instead.) The shell will create the file if it doesn’t exist. If the file exists, it will be silently overwritten, which may lead to data loss and thus requires some caution. ls lengths.txt confirms that the file exists:\n\n\n\n\n\n\nls lengths.txt\nlengths.txt\n\n\n\n\n\nPrinting file content to screen with cat\nWe have already encountered the cat command in our previous lesson. We will go a little deeper here and see how its useful when combining commands.\nWe can now send the content of lengths.txt to the screen using cat lengths.txt. The cat command gets its name from “concatenate” i.e. join together, and it prints the contents of files one after another. There’s only one file in this case, so cat just shows us what it contains:\n\n\n\n\n\n\ncat lengths.txt\n  20  cubane.pdb\n  12  ethane.pdb\n   9  methane.pdb\n  30  octane.pdb\n  21  pentane.pdb\n  15  propane.pdb\n 107  total\n\n\n\n\n\n\n\n\n\nOutput Page by Page\n\n\n\nWe’ll continue to use cat in this lesson, for convenience and consistency, but it has the disadvantage that it always dumps the whole file onto your screen. More useful in practice is the command less, which you use with less lengths.txt. This displays a screenful of the file, and then stops. You can go forward one screenful by pressing the spacebar, or back one by pressing b. Press q to quit.\n\n\n\n\nSorting with the sort command\nNow let’s use the sort command to sort its contents.\n\n\n\n\n\n\nWhat Does sort -n Do?\n\n\n\nIf we run sort on a file containing the following lines:\n10\n2\n19\n22\n6\nthe output is:\n\n\n\n\n\n\n10\n19\n2\n22\n6\n\n\n\nIf we run sort -n on the same input, we get this instead:\n\n\n\n\n\n\n2\n6\n10\n19\n22\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe -n option specifies a numerical rather than an alphanumerical sort.\n\n\n\n\nWe will also use the -n option to specify that the sort is numerical instead of alphanumerical. This does not change the file; instead, it sends the sorted result to the screen:\n\n\n\n\n\n\nsort -n lengths.txt\n  9  methane.pdb\n 12  ethane.pdb\n 15  propane.pdb\n 20  cubane.pdb\n 21  pentane.pdb\n 30  octane.pdb\n107  total\n\n\n\nWe can put the sorted list of lines in another temporary file called sorted-lengths.txt by putting > sorted-lengths.txt after the command, just as we used > lengths.txt to put the output of wc into lengths.txt. Once we’ve done that, we can run another command called head to get the first few lines in sorted-lengths.txt:\n\n\n\n\n\n\nsort -n lengths.txt > sorted-lengths.txt\nhead -n 1 sorted-lengths.txt\n  9  methane.pdb\n\n\n\nUsing -n 1 with head tells it that we only want the first line of the file; -n 20 would get the first 20, and so on. Since sorted-lengths.txt contains the lengths of our files ordered from least to greatest, the output of head must be the file with the fewest lines.\n\n\nRedirecting to the same file >>\nIt’s a very bad idea to try redirecting the output of a command that operates on a file to the same file. For example:\nsort -n lengths.txt > lengths.txt\nDoing something like this may give you incorrect results and/or delete the contents of lengths.txt.\n\nWhat Does >> Mean?\nWe have seen the use of >, but there is a similar operator >> which works slightly different. We’ll learn about the differences between these two operators by printing some strings. We can use the echo command to print strings e.g.\n\n\n\n\n\n\necho The echo command prints text\nThe echo command prints text\n\n\n\nNow test the commands below to reveal the difference between the two operators:\n(Try executing each command twice in a row and then examining the output files)\necho hello > testfile01.txt\nand:\necho hello >> testfile02.txt\n\n\n\n\n\n\nNote\n\n\n\nIn the first example with >, the string “hello” is written to testfile01.txt, but the file gets overwritten each time we run the command.\nWe see from the second example that the >> operator also writes “hello” to a file (in this casetestfile02.txt), but appends the string to the file if it already exists (i.e. when we run it for the second time).\n\n\n\n\n\n\n\n\nExercise 2.4.1.1: Appending Data\n\n\n\nWe have already met the head command, which prints lines from the start of a file. tail is similar, but prints lines from the end of a file instead.\nConsider the file bacteria.txt. After these commands, select the answer that corresponds to the file bacteria-subset.txt:\nhead -n 3 bacteria.txt > bacteria-subset.txt\ntail -n 2 bacteria.txt >> bacteria-subset.txt\n\nThe first three lines of bacteria.txt\nThe last two lines of bacteria.txt\nThe first three lines and the last two lines of bacteria.txt\nThe second and third lines of bacteria.txt\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n\nOption 3 is correct.\nFor option 1 to be correct we would only run the head command.\nFor option 2 to be correct we would only run the tail command.\nFor option 4 to be correct we would have to pipe the output of head into tail -n 2 by doing head -n 3 bacteria.txt | tail -n 2 > bacteria-subset.txt\n\n\n\n\n\nIf you think this is confusing, that’s fine: even once you understand what wc, sort, and head do, all those intermediate files make it hard to follow what’s going on.\n\n\n\n\n\n\nNow that we know a few basic commands, we can finally look at the shell’s most powerful feature: the ease with which it lets us combine existing programs in new ways."
  },
  {
    "objectID": "materials/02-unix_intro/2.4-combining_commands.html#the-pipe",
    "href": "materials/02-unix_intro/2.4-combining_commands.html#the-pipe",
    "title": "2.4 Combining Commands",
    "section": "2.4.2 The | Pipe",
    "text": "2.4.2 The | Pipe\nThe way we can combine commands together is using a pipe, which uses the special operator |.\nFrom our previous exercise, we can make it easier to understand by running sort and head together:\n\n\n\n\n\n\nsort -n lengths.txt | head -n 1\n  9  methane.pdb\n\n\n\nThe vertical bar, |, between the two commands is called a pipe. It tells the shell that we want to use the output of the command on the left as the input to the command on the right.\nNothing prevents us from chaining pipes consecutively. That is, we can for example send the output of wc directly to sort, and then the resulting output to head. Thus we first use a pipe to send the output of wc to sort:\n\n\n\n\n\n\nwc -l *.pdb | sort -n\n   9 methane.pdb\n  12 ethane.pdb\n  15 propane.pdb\n  20 cubane.pdb\n  21 pentane.pdb\n  30 octane.pdb\n 107 total\n\n\n\nAnd now we send the output of this pipe, through another pipe, to head, so that the full pipeline becomes:\n\n\n\n\n\n\nwc -l *.pdb | sort -n | head -n 1\n   9  methane.pdb\n\n\n\nThis is exactly like a mathematician nesting functions like log(3x) and saying “the log of three times x”. In our case, the calculation is “head of sort of line count of *.pdb”.\nThe redirection and pipes used in the last few commands are illustrated below:\n\n\n\nRedirects and Pipes\n\n\n\n\n\n\n\n\nExercise 2.4.2.1: Piping Commands Together\n\n\n\nIn our current directory, we want to find the 3 files which have the least number of lines. Which command listed below would work?\n\nwc -l * > sort -n > head -n 3\nwc -l * | sort -n | head -n 1-3\nwc -l * | head -n 3 | sort -n\nwc -l * | sort -n | head -n 3\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n\nOption 4 is the solution. The pipe character | is used to connect the output from one command to the input of another. > is used to redirect standard output to a file. Try it in the molecules/ directory!\n\n\n\n\n\n\n\n\n\n\n\nExercise 2.4.2.2: An example pipeline: Checking Files\n\n\n\nThere are 17 files from an assay in the ~/Desktop/workshop_files_Bact_Genomics_2023/02_unix_intro/north-pacific-gyre/2012-07-03 directory. Suppose you want to do some quick sanity checks on the content of the files. You know that the files are supposed to have 300 lines. Starting by moving to that directory,\n\nHow would you check if there are any files with fewer than 300 lines in the directory?\nHow would you check if there are any files with more than 300 lines in the directory?\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n\n\nwc -l *.txt | sort -n | head -n 5. You can report the number of lines of all the text files in the directory, sort them, and then get the top (if any files have fewer than 300 lines they will appear here).\nwc -l *.txt | sort -n -r | head -n 5. Same as above but now we want to sort the files in reverse order.\n\n\n\n\n\n\nThis idea of linking programs together is why Unix has been so successful. Instead of creating enormous programs that try to do many different things, Unix programmers focus on creating lots of simple tools that each do one job well, and that work well with each other. This programming model is called “pipes and filters”.\nWe’ve already seen pipes; a filter is a program like wc or sort that transforms a stream of input into a stream of output. Almost all of the standard Unix tools can work this way: unless told to do otherwise, they read from standard input, do something with what they’ve read, and write to standard output.\nThe key is that any program that reads lines of text from standard input and writes lines of text to standard output can be combined with every other program that behaves this way as well. You can and should write your programs this way so that you and other people can put those programs into pipes to multiply their power.\n\n\n\n\n\n\nExercise 2.4.2.3: Pipe Reading Comprehension\n\n\n\nA file called bacteria.txt (in the 02_unix_intro folder) contains the following data:\n2023-01-24,Escherichia_coli\n2023-01-24,Mycobacteria_tuberculosis\n2023-01-24,Salmonella_enterica\n2023-01-24,Staphylococcus_aureus\n2023-01-25,Streptococcus_pneumoniae\n2023-01-25,Pseudomonas_aeruginosa\n2023-01-25,Haemophilus_influenzae \n2023-01-26,Carsonella_ruddii\n2023-01-26,Escherichia_coli\nWhat text passes through each of the pipes and the final redirect in the pipeline below?\ncat bacteria.txt | head -n 5 | tail -n 3 | sort -r > final.txt\nHint: build the pipeline up one command at a time to test your understanding\n\n\n\n\n\n\nSolution:\n\n\n\n\n\nThe head command extracts the first 5 lines from bacteria.txt. Then, the last 3 lines are extracted from the previous 5 by using the tail command. With the sort -r command those 3 lines are sorted in reverse order and finally, the output is redirected to a file final.txt. The content of this file can be checked by executing cat final.txt. The file should contain the following lines:\n2023-01-25,Streptococcus_pneumoniae\n2023-01-24,Staphylococcus_aureus\n2023-01-24,Salmonella_enterica\n\n\n\n\n\n\n\n\n\n\n\nExercise 2.4.2.4: Pipe Construction\n\n\n\nFor the file bacteria.txt from the previous exercise, consider the following command:\n\n\n\n\n\n\ncut -d , -f 2 bacteria.txt\nThe cut command is used to remove or “cut out” certain sections of each line in the file. The optional -d flag is used to define the delimiter. A delimiter is a character that is used to separate each line of text into columns. The default delimiter is Tab, meaning that the cut command will automatically assume that values in different columns will be separated by a tab. The -f flag is used to specify the field (column) to cut out. The command above uses the -d option to split each line by comma, and the -f option to print the second field in each line, to give the following output:\nEscherichia_coli\nMycobacteria_tuberculosis\nSalmonella_enterica\nStaphylococcus_aureus\nStreptococcus_pneumoniae\nPseudomonas_aeruginosa\nHaemophilus_influenzae \nCarsonella_ruddii\nEscherichia_coli\n\n\n\nThe uniq command filters out adjacent matching lines in a file. How could you extend this pipeline (using uniq and another command) to find out what bacteria the file contains (without any duplicates in their names)?\n\n\n\n\n\n\nSolution:\n\n\n\n\n\ncut -d , -f 2 bacteria.txt | sort | uniq\n\n\n\n\n\n\n\n\n\n\n\nAwk: a more powerful tool for text processing\n\n\n\nWe have seen the cut command that allows the selection of columns in tabular data. If you need more powerful manipulation of tabular data you can use the command awk, which permits more powerful operations (selection, calculations etc.) on columns. For more complex operations, however, we recommend going to your favourite programming language!\n\n\n\n\n\n\n\n\nExercise 2.4.2.5: Which Pipe?\n\n\n\nThe file bacteria.txt contains 9 lines of data formatted as follows:\n\n\n\n\n\n\n2023-01-24,Escherichia_coli\n2023-01-24,Mycobacteria_tuberculosis\n2023-01-24,Salmonella_enterica\n2023-01-24,Staphylococcus_aureus\n2023-01-25,Streptococcus_pneumoniae\n...\n\n\n\nThe uniq command has a -c option which gives a count of the number of times a line occurs in its input. Assuming your current directory is 02_unix_intro/, what command would you use to produce a table that shows the total count of each type of animal in the file?\n\nsort bacteria.txt | uniq -c\nsort -t, -k2,2 bacteria.txt | uniq -c\ncut -d, -f 2 bacteria.txt | uniq -c\ncut -d, -f 2 bacteria.txt | sort | uniq -c\ncut -d, -f 2 bacteria.txt | sort | uniq -c | wc -l\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n\nOption 4. is the correct answer. If you have difficulty understanding why, try running the commands, or sub-sections of the pipelines (make sure you are in the 02_unix_intro/ directory).\n\n\n\n\n\n\n\n\n\n\n\nExercise 2.4.2.6: Filtering by patterns\n\n\n\ngrep is another command that searches for patterns in text. Patterns could be simple text or a combination of text and the wildcard characters we have seen before like ? and *. Like > other commands we have seen grep can be used on multiple files. For example if we wanted to find all occurrences of name in all the text files we could write:\ngrep \"name\" *.txt\nUsing the bacteria.txt file suppose we wanted to copy all the Escherichia_coli dates to a separate file Escherichia_coli.txt. Which combination of commands would achieve this?\n\n\n\n\n\n\nSolution:\n\n\n\n\n\ngrep “Escherichia_coli” bacteria.txt | cut -d, -f 1 > Escherichia_coli.txt"
  },
  {
    "objectID": "materials/02-unix_intro/2.4-combining_commands.html#more-exercises-on-pipe",
    "href": "materials/02-unix_intro/2.4-combining_commands.html#more-exercises-on-pipe",
    "title": "2.4 Combining Commands",
    "section": "2.4.3 More exercises on | Pipe",
    "text": "2.4.3 More exercises on | Pipe\n\n\n\n\n\n\nExercise 2.4.3.1: Pipe Comprehension\n\n\n\nIf you had the following two text files:\ncat bacteria_1.txt\nEscherichia_coli\nMycobacteria_tuberculosis\nSalmonella_enterica\nMycobacteria_tuberculosis\ncat bacteria_2.txt\nEscherichia_coli\nStaphylococcus_aureus\nMycobacteria_tuberculosis\nStreptococcus_pneumoniae\nWhat would be the result of the following command?\ncat bacteria*.txt | head -n 6 | tail -n 1\n\n\n\n\n\n\nSolution:\n\n\n\n\n\nThe result would be “Staphylococcus_aureus”. Here is a diagram illustrating what happens in each of the three steps:\n                    Escherichia_coli\n                    Mycobacteria_tuberculosis                 Escherichia_coli                        \ncat bacteria*.txt   Salmonella_enterica         head -n 6     Mycobacteria_tuberculosis    tail -n 1\n----------------->  Mycobacteria_tuberculosis  ----------->   Salmonella_enterica        ----------->  Staphylococcus_aureus\n                    Escherichia_coli                          Mycobacteria_tuberculosis\n                    Staphylococcus_aureus                     Escherichia_coli\n                    Mycobacteria_tuberculosis                 Staphylococcus_aureus\n                    Streptococcus_pneumoniae                 \n\ncat bacteria*.txt would combine the content of both files, and then\nhead -n 6 would print the first six lines of the combined file, and then\ntail -n 1 would return the last line of this output.\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2.4.3.2: zcat and grep\n\n\n\nIn the 02_unix_intro/ directory, you will find a file named MTB_H37Rv_selected_proteins.fasta.gz. This is a file that contains the amino acid sequences of some proteins from Mycobacterium tuberculosis in a text-based format known as FASTA. However, this file is compressed using an algorithm known as GZip, which is indicated by the file extension .gz.\nTo look inside compressed files, you can use an alternative to cat called zcat (the ‘z’ at the beginning indicates it will work on zipped files).\n\nUse zcat together with less to look inside this file.\n\n\nHint\n\nRemember you can press Q to exit the less program.\n\nThe content of this file may look a little strange, if you’re not familiar with the FASTA file format. But basically, each protein sequence name starts with the > symbol. Combine zcat with grep to extract the sequence names only. How many proteins are in the file?\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n\nTask 1\nThe following command allows us to “browse” through the content of this file:\nzcat MTB_H37Rv_selected_proteins.fasta.gz | less\nNB. Mac users may have to use this version of the command zcat < TB_H37Rv_selected_proteins.fasta.gz | less to read compressed files by directing the file to zcat with <.\nWe can use ↑ and ↓ to move line-by-line or the Page Up and Page Down keys to move page-by-page. You can exit less by pressing Q (for “quit”). This will bring you back to the console.\n\nTask 2\nWe can look at the sequences’ names by running:\nzcat MTB_H37Rv_selected_proteins.fasta.gz | grep \">\"\n>gyrB locus_tag=\"Rv0005\"\n>gyrA locus_tag=\"Rv0006\"\n>pknB locus_tag=\"Rv0014c\"\n>pknA locus_tag=\"Rv0015c\"\n>pbpA locus_tag=\"Rv0016c\"\n>whiB5 locus_tag=\"Rv0022c\"\n>rpoB locus_tag=\"Rv0667\"\n>rpoA locus_tag=\"Rv3457c\"\n>inhA locus_tag=\"Rv1484\"\n>esxT locus_tag=\"Rv3444c\"\nWe could further count how many sequences, by piping this output to wc:\nzcat < MTB_H37Rv_selected_proteins.fasta.gz | grep \">\" | wc -l\n10\n\n\n\n\n\n\nMore on Cut, Sort, Unique & Count\nLet’s now explore a few more useful commands to manipulate text that can be combined to quickly answer useful questions about data we may generate later on in this course.\nLet’s start with the command cut, which is used to extract sections from each line of its input. For example, let’s say we wanted to retrieve only the second field (or column) of our CSV files in the 02_unix_intro directory, which contains the species classification taxonomy_id of reads obtained from sequencing the sample `TBNmA041:\n\n\n\n\n\n\ncat TBNmA041*.csv | cut -d \",\" -f 2\ntaxonomy_id\n1773\n78331\n1764\n701042\n339268\n1767\n222805\n1768\n722731\n1545728\n...\n\n\n\nThe two options used with this command are:\n\n-d defines the delimiter used to separate different parts of the line. Because this is a CSV file, we use the comma as our delimiter. The tab is used as the default delimiter.\n-f defines the field or part of the line we want to extract. In our case, we want the second field (or column) of our CSV file. It’s worth knowing that you can specify more than one field, so for example if you had a CSV file with more columns and wanted columns 3 and 7 you could set -d 3,7.\n\nThe next command we will explore is called sort, which sorts the lines of its input alphabetically (default) or numerically (if using the -n option). Let’s combine it with our previous command to see the result:\nRemember we have to add the sort option -n to achieve the required output.\n\n\n\n\n\n\ncat TBNmA041*.csv | cut -d \",\" -f 2 | sort -n\ntaxonomy_id\n1764\n1764\n1767\n1768\n1769\n1773\n1773\n1781\n1784\n29311\n78331\n78331\n212767\n...\n\n\n\nYou can see that the output is now sorted as desired.\nThe cut command is often used in conjunction with another command: uniq. This command returns the unique lines in its input. Importantly, it only works as intended if the input is sorted. That’s why it’s often used together with sort.\nLet’s see it in action, by continuing building our command:\n\n\n\n\n\n\ncat TBNmA041*.csv | cut -d \",\" -f 2 | sort -n | uniq\ntaxonomy_id\n1764\n1767\n1768\n1769\n1773\n1781\n1784\n29311\n78331\n212767\n222805\n339268\n482462\n701042\n722731\n1545728\n1682113\n1879023\n1936029\n\n\n\nWe can see that now the output is de-duplicated, so only unique values are returned. And so, with a few simple commands, we’ve answered a very useful question from our data: what are the unique species in our collection of samples?\n\n\n\nIllustration of the sort + uniq commands by Julia Evans\n\n\n\n\n\n\n\n\nExercise 2.4.3.3: Sort & Count I\n\n\n\nLet’s continue working on our command:\ncat TBNmA041*.csv | cut -d \",\" -f 2 | sort -n | uniq\nAs you saw, this output also returns a line called “taxonomy_id”. This was part of the header (or column name) of our CSV file, which is not really useful to have in our output.\nLet’s try and solve that problem, and also ask the question of how frequent each of these species are in our data.\n\nLooking at the help page for grep (grep --help or man grep), see if you can find an option to invert a match, i.e. to return the lines that do not match a pattern. Can you think of how to include this in our pipeline to remove that line from our output?\n\n\nHint\n\nThe option to invert a match with grep is -v. grep -v \"taxonomy_id\" would return the lines that do not match the word “taxonomy_id”.\n\nThe uniq command has an option called -c. Try adding that option to the command and infer what it does (or look at uniq --help).\nFinally, produce a sorted table of counts for each of our taxonomic id in descending order (the most common species at the top).\n\n\nHint\n\nThe sort command has an option to order the output in reverse order: -r.\n\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n\nTask 1\nLooking at the help of this function with grep --help, we can find the following option:\n -v, --invert-match        select non-matching lines\nSo, we can continue working on our pipeline by adding another step at the end:\n\n\n\n\n\n\ncat TBNmA041*.csv | cut -d \",\" -f 2 | sort -n | uniq | grep -v \"taxonomy_id\"\n1764\n1767\n1768\n1769\n1773\n1781\n1784\n29311\n...\n\n\n\nThis now removes the line that matched the word “taxonomy_id”.\n\nTask 2\nLooking at the help for uniq --help, we can see that:\n -c, --count           prefix lines by the number of occurrences\nSo, if we add this option, we will get the count of how many times each unique line appears in our data:\n\n\n\n\n\n\ncat TBNmA041*.csv | cut -d \",\" -f 2 | sort -n | uniq -c | grep -v \"taxonomy_id\"\n   2 1764\n   1 1767\n   1 1768\n   1 1769\n   2 1773\n   1 1781\n   1 1784\n   1 29311\n   2 78331\n   1 212767\n   1 222805\n...   \n\n\n\n\nTask 3\nNow that we’ve counted each of our species, we can again sort this result, this time by the counts value, by adding another sort step at the end:\n\n\n\n\n\n\ncat TBNmA041*.csv | cut -d \",\" -f 2 | sort -n | uniq -c | grep -v \"taxonomy_id\" | sort -r\n   2 78331\n   2 701042\n   2 482462\n   2 339268\n   2 1936029\n   2 1773\n   2 1764\n   2 1682113\n   2 1545728\n   1 722731\n   1 29311\n   1 222805\n   1 212767\n   1 1879023\n   1 1784\n   1 1781\n   1 1769\n   1 1768\n   1 1767\n\n\n\nWe used the option -r, which from the help page sort --help, says:\n  -r, --reverse               reverse the result of comparisons\n\n\n\n\n\n\n\n\n\n\n\nExercise 2.4.3.4: Sort & Count II\n\n\n\nThis is an (optional) advanced exercise.\nIn the 02_unix_intro/ directory, you will find two files named G26832.gff3.gz and G26832.tsv. We have already encountered these type of files and hopefully are familiar with its contents. The .gff is a file containing the locations and other information of genes in the Mycobacterium tuberculosis genome in a standard text-based format called GFF.\nThis is a tab-delimited file, where the 3rd column contains information about the kind of annotated feature, the 4th and 5th columns contains the start and end positions on the contig and the 7th column contains the strand information. You can investigate its content with zcat G26832.gff3.gz | less -S. We use zcat because the file is compressed (we can tell from its extension ending with .gz)\nExtracts from this file have been converted to a more friendly file format .tsv. The .tsv file also contain similar information but this time, with the gene names spelt out distinctly in column 7. The .tsv file can easily be open using excel or libre office. For the tasks below, you may want to use the .tsv file to carry out the analysis.\nUsing a combination of the commands we have seen so far:\n\nCount how many occurrences of each feature (2nd column in .tsv) there is in the file.\nHow many times does the gene “menH” occur in the genomic?\nWhich gene has the highest number of occurrences in the genome?\n\nHow many distinct trna’s can you identify in the bacteria genome?\nWhich of these occurs more than once in the genome?\nDo you see any unusual trna? What do you think their roles are?\n\n\n\n\nHint\n\n\nStart by investigating the content of the file with cat G26832.tsv | head or open with a GUI tool. You will notice the first few lines of the file contain comments starting with # symbol. You should remove these lines before continuing.\nCheck the help for grep to remind yourself what the option is to return lines not matching a pattern.\nRemember that the cut program uses tab as its default delimiter.\nTo see all trna’s you can use grep \"_trna\".\n\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n\nTask 1\nWe could use:\n\n\n\n\n\n\ncat G26832.tsv | grep -v \"#\" | cut -f 2 | sort | uniq -c\n4041 cds\n   1 crispr\n  21 ncRNA\n  13 ncRNA-region\n   1 oriC\n   3 rRNA\n   1 sorf\n  46 tRNA\n   1 tmRNA\n\n\n\n\nWe use grep to remove the first few lines of the file that start with # character.\nWe use cut to extract the third “field” (column) of the file. Because it’s a tab-delimited file, we don’t need to specify a delimiter with cut, as that is the default.\nWe use sort to order the features in alphabetical order.\nFinally, uniq is used to return the unique values as well as count their occurrence (with the -c option).\n\n\nTask 2\nThe answer is 14. We could use the following command by investigating column 7:\ncat G26832.tsv | grep \"menH\" | cut -f 7 | wc -l\n\nTask 3\nThe gene with the highest occurrence in the genome is acrR and it occurs 38 times.\nWe can find this out with the command below:\ncat G26832.tsv | grep -v \"#\" | cut -f 7 | sort | uniq -c | sort -r | head -n 20\n\nTask 4\nTask 4a For task 4a, we will simply count the number of unique trna in the genome using the command below:\ncat G26832.tsv | grep \"_trna\" | cut -f 7 | sort | uniq -c | wc -l\nWe observe 22 unique trna counts.\nTask 4b For task 4b, we can identify the trna’s that occur more than once in the genome by simply adding another sort to the previous command instead of the wc -l.\nWe can do this with the command below:\ncat G26832.tsv | grep \"_trna\" | cut -f 7 | sort | uniq -c | sort -r\n   5 Ser_trna\n   5 Leu_trna\n   4 Arg_trna\n   3 Val_trna\n   3 Thr_trna\n   3 Pro_trna\n   3 Gly_trna\n   3 Ala_trna\n   2 Lys_trna\n   2 Glu_trna\n   2 Gln_trna\n...\nThere are 11 trna's that occur more than once in the genome.\nTask 4c fMet_trna and Ile2_trna may be unusual to you. Google and find out what they do.\n\n\n\n\n\n\n\n\n\n\n\nsort: Alphabetically or Numerically?\n\n\n\n\n\nNote that, by default the sort command will order input lines alphabetically. So, for example, if it received this as input:\n10\n2\n1\n20\nThe result of sorting would be:\n1\n10\n2\n20\nBecause that’s the alphabetical order of those characters. We can use the option sort -n to make sure it sorts these as numbers, in which case the output would be as expected:\n1\n2\n10\n20\nBut, you may be asking yourself: why did it work with the output of uniq without specifying -n?\nThis is because the output of uniq left-aligns all the numbers by prefixing the smaller numbers with a space, such as this:\n10\n 2\n 1\n20\nAnd because the space character comes first in the computer’s “alphabet”, we don’t actually need to use the -n option.\nHere’s the main message: always use the -n option if you want things that look like numbers to be sorted numerically (if the input doesn’t look like a number, then sort will just order them alphabetically instead)."
  },
  {
    "objectID": "materials/02-unix_intro/2.4-combining_commands.html#credit",
    "href": "materials/02-unix_intro/2.4-combining_commands.html#credit",
    "title": "2.4 Combining Commands",
    "section": "2.4.4 Credit",
    "text": "2.4.4 Credit\nInformation on this page has been adapted and modified from the following source(s):\n\nGabriel A. Devenyi (Ed.), Gerard Capes (Ed.), Colin Morris (Ed.), Will Pitchers (Ed.),Greg Wilson, Gerard Capes, Gabriel A. Devenyi, Christina Koch, Raniere Silva, Ashwin Srinath, … Vikram Chhatre. (2019, July). swcarpentry/shell-novice: Software Carpentry: the UNIX shell, June 2019 (Version v2019.06.1).\nhttps://github.com/cambiotraining/UnixIntro\nhttps://github.com/cambiotraining/unix-shell"
  },
  {
    "objectID": "materials/03-file_formats/3.1-file_formats.html",
    "href": "materials/03-file_formats/3.1-file_formats.html",
    "title": "3.1 Common File Formats",
    "section": "",
    "text": "Teaching: 20 min || Exercises: 10 min"
  },
  {
    "objectID": "materials/03-file_formats/3.1-file_formats.html#overview",
    "href": "materials/03-file_formats/3.1-file_formats.html#overview",
    "title": "3.1 Common File Formats",
    "section": "Overview",
    "text": "Overview\n\n\n\n\n\n\n\n\n\n\n\n\nQuestions:\n\n\n\n\nWhat are the various file formats I may encounter while carrying out my bioinformatics analysis\nWhat information do I get from each file type\nWhich file type should I look for specifically to address my needs\n\n\n\n\n\n\n\n\n\nLearning Objectives:\n\n\n\n\nTo know and identify the various file formats used in bioinformatics\nTo know which file type to select to address appropriate needs\n\n\n\n\n\n\n\n\n\nKey Points:\n\n\n\n\nThere are several file types generated along a bioinformatics pipeline. Each containing varying depth of information on the sequence.\nThe common file types I may encounter in this course are:\n\nCSV and TSV\nFASTA\nFAST5\nFASTQ\nSAM, BAM and CRAM\nVCF\nBED\nGFF\nNEWICK\n\nThe various file formats are usually designed to allow for processing tools to easily parse the data"
  },
  {
    "objectID": "materials/03-file_formats/3.1-file_formats.html#background",
    "href": "materials/03-file_formats/3.1-file_formats.html#background",
    "title": "3.1 Common File Formats",
    "section": "3.1.1 Background",
    "text": "3.1.1 Background\n\n\n\n\n\n\nWhat is a file format?\n\n\n\nA file format is a way for computers (and humans) to standardize how data is organized.\n\n\nIn this chapter, our aim will be to go over the most common bioinformatics file formats you will come across. We will however not go into much details, but to simply help you become familiarized with the different file types that you may encounter in the course and are frequently used to store biological data.\nThe heading of each file format links to a wiki page with more details about the format.\nGenerally, files can be classified into two categories: text files and binary files.\n\nText files can be opened with standard text editors, and manipulated using command-line tools (such as head, less, grep, cat, etc.). However, many of the standard files listed in this page can be opened with specific software that displays their content in a more user-friendly way. For example, the NEWICK format is used to store phylogenetic trees and, although it can be opened in a text editor, it is better used with a software such as FigTree to visualise the tree as a graph. Very often, text files may be compressed to save storage space. A common compression format used in bioinformatics is gzip which has extension .gz. Many bioinformatic tools support compressed files. For example, FASTQ files (used to store NGS sequencing data) are often compressed with format .fq.gz.\nBinary files are often used to store data more efficiently. This helps improves efficiency for computers and are usually in a non-human readable binary format. You’ll see some binary files have a corresponding “index” file which is useful for searching. Typically, specific tools need to be used with those files. For example, the BAM format is used to store sequences aligned to a reference genome and can be manipulated with dedicated software such as samtools.\n\nMore generally, the extensions (last set of letters after the final . in a file name) your find in file types are also used to indicate which algorithm/program can be used to open the file."
  },
  {
    "objectID": "materials/03-file_formats/3.1-file_formats.html#csv-and-tsv",
    "href": "materials/03-file_formats/3.1-file_formats.html#csv-and-tsv",
    "title": "3.1 Common File Formats",
    "section": "3.1.2 CSV and TSV",
    "text": "3.1.2 CSV and TSV\nCSV stands for Comma-Separated Values\nTSV on the other hand stands for Tab-Separated Values\nCSV and TSV are text files. They store tabular data in a text file with the file extensions: .csv and .tsv respectively. It is also possible to store them with .txt extension and the data wll work normally. As the names suggest, for CSV files there is a comma , between each value, whereas for TSV files there is a tab between each value.\n\nBelow is a sample .csv text. You will see this being used more frequently in other chapters.\n\n\n\n\n\n\nsample,fastq_1,fastq_2\nTBNmA041,/Genomes/TBNmA041_R1.fastq.gz,/Genomes/TBNmA041_R2.fastq.gz\nTBNmA050,/Genomes/TBNmA050_R1.fastq.gz,/Genomes/TBNmA050_R2.fastq.gz\nTBNmA064,/Genomes/TBNmA064_R1.fastq.gz,/Genomes/TBNmA064_R2.fastq.gz\nTBNmA067,/Genomes/TBNmA067_R1.fastq.gz,/Genomes/TBNmA067_R2.fastq.gz\nTBNmA074,/Genomes/TBNmA074_R1.fastq.gz,/Genomes/TBNmA074_R2.fastq.gz\nTBNmA077,/Genomes/TBNmA077_R1.fastq.gz,/Genomes/TBNmA077_R2.fastq.gz\n\n\n\nThe simplicity of these formats allows researchers to easily exchange data among computers, a term known as portability.\nThese files can be opened with spreadsheet programs (such as Microsoft Excel). They can also be created from spreadsheet programs by going to File > Save As… and select “CSV (Comma delimited)” or “Text (Tab delimited)” as the file format.\n\n\n\nA .csv file opened separately with excel (upper) and a text editor (lower)"
  },
  {
    "objectID": "materials/03-file_formats/3.1-file_formats.html#fasta",
    "href": "materials/03-file_formats/3.1-file_formats.html#fasta",
    "title": "3.1 Common File Formats",
    "section": "3.1.3 FASTA",
    "text": "3.1.3 FASTA\nFASTA (pronounced “fast-A”) format is a simple type of format that bioinformaticians use to represent either DNA or protein sequences. FASTA files are therefore text files and stores nucleotide or amino acid sequences. It is written in text format, allowing for processing tools to easily parse the data. Within the basic FASTA file, there are two lines per sequence\n\n\nthe identifier (comments, annotations)\n\n\nthe sequence itself\n\n\nThe top line holds information pertaining to the sequence below. It is preceded by with a >. Without this informative first line, we just have a raw format. As a general rule, it is recommended that each line of sequence be shorter than 80 characters. The file extensions for FASTA are .fa or .fas or .fasta\nA FASTA file containing a single nucleotide sequence might look like this:\n\n\n\n\n\n\n>G26832_filtered: example fasta sequence\nttgaccgatgaccccggttcaggcttcaccacagtgtggaacgcggtcgtctccgaactt\naacggcgaccctaaggttgacgacggacccagcagtgatgctaatctcagcgctccgctg\nacccctcagcaaagggcttggctcaatctcgtccagccattgaccatcgtcgaggggttt\ngctctgttatccgtgccgagcagctttgtccaaaacgaaatcgagcgccatctgcgggcc\nccgattaccgacgctctcagccgccgactcggacatcagatccaactcggggtccgcatc\n\n\n\n… and one containing a protein sequence may look like this:\n\n\n\n\n\n\n>PROTEIN_SEQUENCE_1\nMTEITAAMVKELRESTGAGMMDCKNALSETNGDFDKAVQLLREKGLGKAAKKADRLAAEG\nLVSVKVSDDFTIAAMRPSYLSYEDLDMTFVENEYKALVAELEKENEERRRLKDPNKPEHK\nIPQFASRKQLSDAILKEAEEKIKEELKAQGKPEKIWDNIIPGKMNSFIADNSQLDSKLTL\nMGQFYVMDDKKTVEQVIAEKEKEFGGKIKIVEFICFEVGEGLEKKTEDFAAEVAAQL\n\n\n\n\nSpecific filename extensions\nThe generic form of FASTA file has the .fas extension. For more specific types, we can use the following:\n\n\n\n\n\n\n\n\nExtension\nMeaning\nNotes\n\n\n\n\nfna\nFASTA nucleic acid\nUsed generically to specify nucleic acids\n\n\nffn\nFASTA nucleotide coding regions\nContains coding regions for a genome\n\n\nfaa\nFASTA amino acid\nContains amino acid sequences. A multiple protein fasta file can have the more specific extension mpfa\n\n\nfrn\nFASTA non-coding RNA\nContains non-coding RNA regions for a genome, in DNA alphabet e.g. tRNA, rRNA"
  },
  {
    "objectID": "materials/03-file_formats/3.1-file_formats.html#fast5",
    "href": "materials/03-file_formats/3.1-file_formats.html#fast5",
    "title": "3.1 Common File Formats",
    "section": "3.1.4 FAST5",
    "text": "3.1.4 FAST5\nFAST5 is a binary file. More specifically, it is a Hierarchical Data Format (HDF5) file. used by Nanopore platforms to store the called sequences (in FASTQ format) as well as the raw electrical signal data from the pore. It’s file extension is .fast5."
  },
  {
    "objectID": "materials/03-file_formats/3.1-file_formats.html#fastq",
    "href": "materials/03-file_formats/3.1-file_formats.html#fastq",
    "title": "3.1 Common File Formats",
    "section": "3.1.5 FASTQ",
    "text": "3.1.5 FASTQ\nThe FASTA format is extremely simple with just two lines per sequence — the first is for the description, the other for the raw sequence. As this format is nicely simple, it does not tell us the Quality of each nucleotide or protein. Your guess is as good as mine, yes! the Q in FASTQ (pronounced “fast-Q”) stands for the Quality of each nucleotide or amino acid in a sequence. FASTQ is therefore a text file, but often compressed with gzip and stores sequences and their quality scores. The quality score is encoded with a single ASCII character. The file extensions for FASTQ are .fq or .fastq (compressed as .fq.gz or .fastq.gz) Just like the FASTA, the first line in a FASTQ file contains the sequence identifier with an optional description, however, the line begins with an @ character rather than a > character.\nA FASTQ file containing a single sequence might look like any of these:\n\n\n\n\n\n\n@VH00616:1:AAAM55THV:1:1101:65456:1000 2:N:0:CAGCAATCGT+ATACATCACA\nGTATCGAGCTGGGATCGGCCGCGGATCTAGACGGTCTGCTGGCCCGGATGCGGGCGACCGACATTCACGT\n+\nCCCCCCCCCCCCCCCCCCCCCCCCCCCC;CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC\n\n\n\n\n\n\n\n\n\n@VH00616:1:AAAM55THV:1:1101:58564:1038 2:N:0:CAGCAATCGT+ATACATCACA\nGTGTCAATGAACGCGCTTGGTGTGGCCCAACCGCATTCGGACGCCACGGACGTGACGTCGTGGCGCTCGG\n+\n!''*((((***+))%%%++)(%%%%).1***-+*''))**55CCF>>>>>>CCCCCCC65)**55CCF>>\n\n\n\nThe second line contains the actual sequences of the read which is separated from the fourth (Quality) line by a + on the third line.\nThe fourth line encodes the quality scores per each base call. This line must have the same length as the sequence in line 2\nThe byte representing quality runs from 0x21 (lowest quality; ‘!’ in ASCII) to 0x7e (highest quality; ‘~’ in ASCII). Here are the quality value characters in left-to-right increasing order of quality (ASCII):\n\n\n\n\n\n\n!\"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]\n^_`abcdefghijklmnopqrstuvwxyz{|}~"
  },
  {
    "objectID": "materials/03-file_formats/3.1-file_formats.html#sam-bam-and-cram",
    "href": "materials/03-file_formats/3.1-file_formats.html#sam-bam-and-cram",
    "title": "3.1 Common File Formats",
    "section": "3.1.6 SAM, BAM and CRAM",
    "text": "3.1.6 SAM, BAM and CRAM\nBefore we talk about SAM, BAM and CRAM, we must discuss the software, SAMtools, from which these formats originate.\n\nSAMtools\nSAMtools is a suite of utilities that allow for efficient post-processing of short DNA sequence read alignments. The program includes several command line programs such as view, sort, and index that allow for next-generation sequence data processing. You will come across this in later chapters.\n\n\nSAM format\nThe name SAM comes from Sequence Alignment/MAP. It is a text file with the file extensions .sam. In addition to regular sequence reads, SAM includes alignment data that link short reads to a reference sequence. This makes SAM files the choice of format when visualizing short read sequences in genome browsers such as IGV (Integrated Genome Viewer).\n\n\nBAM and CRAM\nBAM refers to Binary Alignment Map. As the name suggest, its a binary file. It is same as a SAM file but compressed in binary form with the file extension .bam. The SAM format is simple to parse, generate and check for errors. However, its large file size (usually in GB on average) gets in the way of efficiency. Thus, researchers found a way to compress it into a binary format without losing the ability to manipulate it. BAM contains indexable representation of nucleotide sequence alignments, allowing for intensive data processing in production pipelines.\nCRAM is a restructured version of its binary version, with column-orientation.\n\n\nWhat Information is in SAM & BAM\nSAM files and BAM files contain the same information, but in a different format. Both SAM & BAM files contain an optional header section (beginning with the @ symbol) followed by the alignment section.\n\n\n\n\n\n\nIllustration of the composition of a SAM file\n\n\n\n@SQ     SN:MTB_anc      LN:4411532\n@PG     ID:bwa  PN:bwa  VN:0.7.17-r1188 CL:bwa mem -t 4 MTB_ANC_unwrapped.fasta G26832_R1.fastq.gz G26832_R2.fastq.gz\nD00535:103:CD2W4ANXX:8:2209:4278:1980   73      MTB_anc 4112828 60      101M    =       4112828 0       CCCGATCGGGTGAGATCACGAAGCCGGTGCCCTCCAACACTTTCTGGCATCTGGGTGCCAGGCTGCGGATTTTGACGACACTTGGCTCGGTGGCCGCCACC     B@BB=EBB/EG/01?DB:E/9/9EGG/EB9@FG09:110=CDGGCC11:CFFE@BCG=CE1F0BF9EG//B0@@/9/C>//;F000E09CDDDDD/.CDDG     NM:i:0  MD:Z:101        AS:i:101        XS:i:0\n\n\nThe header section may contain information about the entire file and additional information for alignments. The alignments then associate themselves with specific header information.\nThe alignment section contains the information for each sequence about where/how it aligns to the reference genome.\nAlignment sections have 11 mandatory fields, as well as a variable number of optional fields\n\n\n\nCol\nField\nType\nBrief description\n\n\n\n\n1\nQNAME\nString\nQuery template NAME\n\n\n2\nFLAG\nInt\nbitwise FLAG\n\n\n3\nRNAME\nString\nReferences sequence NAME\n\n\n4\nPOS\nInt\n1- based leftmost mapping POSition\n\n\n5\nMAPQ\nInt\nMAPping Quality\n\n\n6\nCIGAR\nString\nCIGAR string\n\n\n7\nRNEXT\nString\nRef. name of the mate/next read\n\n\n8\nPNEXT\nInt\nPosition of the mate/next read\n\n\n9\nTLEN\nInt\nobserved Template LENgth\n\n\n10\nSEQ\nString\nsegment SEQuence\n\n\n11\nQUAL\nString\nASCII of Phred-scaled base QUALity+33\n\n\n\nThe bitwise FLAG, provides the following information: * are there multiple fragments? * are all fragments properly aligned? * is this fragment unmapped? * is the next fragment unmapped? * is this query the reverse strand? * is the next fragment the reverse strand? * is this the 1st fragment? * is this the last fragment? * is this a secondary alignment? * did this read fail quality controls? * is this read a PCR or optical duplicate?\n\n\n\nOverview of the contents of a SAM/BAM file"
  },
  {
    "objectID": "materials/03-file_formats/3.1-file_formats.html#vcf",
    "href": "materials/03-file_formats/3.1-file_formats.html#vcf",
    "title": "3.1 Common File Formats",
    "section": "3.7 VCF",
    "text": "3.7 VCF\nVCF stands for Variant Calling Format. As you may already guess from the name, yes!, it is a text file but often compressed with gzip that stores gene sequence variations (SNP/Indel variants). It’s file extension is .vcf. It contains a header with metadata preceded by a ## string. Best practices with VCF files recommend describing INFO, FILTER, and FORMAT entries used in the body within the header.\nFollowing the header is the body, made up of 8 mandatory tab separated columns, one for each identifier and an unlimited number of optional columns that may be used to record other information about the sample(s). When additional columns are used, the first optional column is used to describe the format of the data in the columns that follow.\n\nThe columns of a VCF\n\n\n\n\n\n\n\n\n\nName\nBrief description\n\n\n\n\n1\nCHROM\nThe name of the sequence (typically a chromosome) on which the variation is being called. This sequence is usually known as ‘the reference sequence’, i.e. the sequence against which the given sample varies.\n\n\n2\nPOS\nThe 1-based position of the variation on the given sequence.\n\n\n3\nID\nThe identifier of the variation, e.g. a dbSNP rs identifier, or if unknown a “.”. Multiple identifiers should be separated by semi-colons without white-space.\n\n\n4\nREF\nThe reference base (or bases in the case of an indel) at the given position on the given reference sequence.\n\n\n5\nALT\nThe list of alternative alleles at this position.\n\n\n6\nQUAL\nA quality score associated with the inference of the given alleles.\n\n\n7\nFILTER\nA flag indicating which of a given set of filters the variation has failed or PASS if all the filters were passed successfully.\n\n\n8\nINFO\nAn extensible list of key-value pairs (fields) describing the variation. See below for some common fields. Multiple fields are separated by semicolons with optional values in the format: =[,data].\n\n\n9\nFORMAT\nAn (optional) extensible list of fields for describing the samples. See below for some common fields.\n\n\n+\nSAMPLEs\nFor each (optional) sample described in the file, values are given for the fields listed in FORMAT.\n\n\n\n\n\n\n\n\n\nA typical content of a VCF file\n\n\n\n##fileformat=VCFv4.2\n##FILTER=<ID=PASS,Description=\"All filters passed\">\n##bcftoolsVersion=1.14+htslib-1.14\n##bcftoolsCommand=mpileup --fasta-ref MTB_ANC_unwrapped.fasta --min-BQ 20 --annotate FORMAT/AD,FORMAT/ADF,FORMAT/ADR,FORMAT/DP,FORMAT/SP,INFO/AD,INFO/ADF,INFO/ADR G26832_sorted.bam\n##reference=file://MTB_ANC_unwrapped.fasta\n##contig=<ID=MTB_anc,length=4411532>\n##ALT=<ID=*,Description=\"Represents allele(s) other than observed.\">\n##INFO=<ID=INDEL,Number=0,Type=Flag,Description=\"Indicates that the variant is an INDEL.\">\n##INFO=<ID=IDV,Number=1,Type=Integer,Description=\"Maximum number of raw reads supporting an indel\">\n##INFO=<ID=IMF,Number=1,Type=Float,Description=\"Maximum fraction of raw reads supporting an indel\">\n##INFO=<ID=DP,Number=1,Type=Integer,Description=\"Raw read depth\">\n##INFO=<ID=VDB,Number=1,Type=Float,Description=\"Variant Distance Bias for filtering splice-site artefacts in RNA-seq data (bigger is better)\",Version=\"3\">\n##INFO=<ID=RPBZ,Number=1,Type=Float,Description=\"Mann-Whitney U-z test of Read Position Bias (closer to 0 is better)\">\n##INFO=<ID=MQBZ,Number=1,Type=Float,Description=\"Mann-Whitney U-z test of Mapping Quality Bias (closer to 0 is better)\">\n##INFO=<ID=BQBZ,Number=1,Type=Float,Description=\"Mann-Whitney U-z test of Base Quality Bias (closer to 0 is better)\">\n##INFO=<ID=MQSBZ,Number=1,Type=Float,Description=\"Mann-Whitney U-z test of Mapping Quality vs Strand Bias (closer to 0 is better)\">\n##INFO=<ID=SCBZ,Number=1,Type=Float,Description=\"Mann-Whitney U-z test of Soft-Clip Length Bias (closer to 0 is better)\">\n##INFO=<ID=FS,Number=1,Type=Float,Description=\"Phred-scaled p-value using Fisher's exact test to detect strand bias\">\n##INFO=<ID=SGB,Number=1,Type=Float,Description=\"Segregation based metric.\">\n##INFO=<ID=MQ0F,Number=1,Type=Float,Description=\"Fraction of MQ0 reads (smaller is better)\">\n##FORMAT=<ID=PL,Number=G,Type=Integer,Description=\"List of Phred-scaled genotype likelihoods\">\n##FORMAT=<ID=DP,Number=1,Type=Integer,Description=\"Number of high-quality bases\">\n##FORMAT=<ID=SP,Number=1,Type=Integer,Description=\"Phred-scaled strand bias P-value\">\n##FORMAT=<ID=AD,Number=R,Type=Integer,Description=\"Allelic depths (high-quality bases)\">\n##FORMAT=<ID=ADF,Number=R,Type=Integer,Description=\"Allelic depths on the forward strand (high-quality bases)\">\n##FORMAT=<ID=ADR,Number=R,Type=Integer,Description=\"Allelic depths on the reverse strand (high-quality bases)\">\n##INFO=<ID=AD,Number=R,Type=Integer,Description=\"Total allelic depths (high-quality bases)\">\n##INFO=<ID=ADF,Number=R,Type=Integer,Description=\"Total allelic depths on the forward strand (high-quality bases)\">\n##INFO=<ID=ADR,Number=R,Type=Integer,Description=\"Total allelic depths on the reverse strand (high-quality bases)\">\n##FORMAT=<ID=GT,Number=1,Type=String,Description=\"Genotype\">\n##INFO=<ID=AC,Number=A,Type=Integer,Description=\"Allele count in genotypes for each ALT allele, in the same order as listed\">\n##INFO=<ID=AN,Number=1,Type=Integer,Description=\"Total number of alleles in called genotypes\">\n##INFO=<ID=DP4,Number=4,Type=Integer,Description=\"Number of high-quality ref-forward , ref-reverse, alt-forward and alt-reverse bases\">\n##INFO=<ID=MQ,Number=1,Type=Integer,Description=\"Average mapping quality\">\n##bcftools_callVersion=1.14+htslib-1.14\n##bcftools_callCommand=call --output-type v --ploidy 1 --multiallelic-caller; Date=Fri Nov  4 10:38:43 2022\n##bcftools_viewVersion=1.14+htslib-1.14\n##bcftools_viewCommand=view --output-file G26832.vcf.gz --output-type z; Date=Fri Nov  4 10:38:43 2022\n#CHROM  POS     ID      REF     ALT     QUAL    FILTER  INFO    FORMAT  G26832_sorted.bam\nMTB_anc 1       .       T       .       284.59  .       DP=64;ADF=49;ADR=11;AD=60;MQSBZ=0;FS=0;MQ0F=0;AN=1;DP4=49,11,0,0;MQ=59  GT:DP:SP:ADF:ADR:AD       0:60:0:49:11:60\nMTB_anc 2       .       T       .       284.59  .       DP=68;ADF=50;ADR=11;AD=61;MQSBZ=0;FS=0;MQ0F=0;AN=1;DP4=50,11,0,0;MQ=59  GT:DP:SP:ADF:ADR:AD       0:61:0:50:11:61\nMTB_anc 3       .       G       .       284.59  .       DP=68;ADF=49;ADR=11;AD=60;MQSBZ=0;FS=0;MQ0F=0;AN=1;DP4=49,11,0,0;MQ=59  GT:DP:SP:ADF:ADR:AD       0:60:0:49:11:60\n...\n\n\nNB. You may have come across a BCF file. Binary Call Format (BCF) is a binary representation of VCF, containing the same information in binary format for improved performance."
  },
  {
    "objectID": "materials/03-file_formats/3.1-file_formats.html#bed",
    "href": "materials/03-file_formats/3.1-file_formats.html#bed",
    "title": "3.1 Common File Formats",
    "section": "3.1.8 BED",
    "text": "3.1.8 BED\nBED stands for Browser Extensible Data. It’s a text file that stores coordinates of genomic regions and has the file extension .bed. ìt has a tabs-delimited file format that allows users to define how data lines of an annotation track are displayed. One of the advantages of this format is the manipulation of coordinates instead of nucleotide sequences, which optimizes the power and computation time when comparing all or part of genomes.\nBED files can have up to 12 columns, but only three are required for the UCSC browser, Galaxy browser and bedtools: chrom - Name of chromosome - chr5, chrX, chr2_random. or scaffold - scaffold10671 chromStart - Starting position of chrom. - First base starts at 0. chromEnd - Ending position. - This value does not get displayed. For example, the first 20 bases would have chromStart value of 0 to and chromEnd value of 20.\n\n\n\n\n\n\nA typical content of a BED3 file\n\n\n\nNC_002945.4 2751    2755\nNC_002945.4 10447   10467\nNC_002945.4 13457   13470\nNC_002945.4 31969   31970\nNC_002945.4 86804   86805\nNC_002945.4 86808   86809\nNC_002945.4 86818   86819\nNC_002945.4 89105   89107\nNC_002945.4 94892   94896"
  },
  {
    "objectID": "materials/03-file_formats/3.1-file_formats.html#gff",
    "href": "materials/03-file_formats/3.1-file_formats.html#gff",
    "title": "3.1 Common File Formats",
    "section": "3.1.9 GFF",
    "text": "3.1.9 GFF\nGFF stands for General Feature Format (also called gene-finding format, generic feature format). It’s a text file that stores gene coordinates and other features and has the file extension .gff. It is used to describe genes and other features of DNA, RNA and protein sequences.\nGFF is an extension of a basic file with the name, start and end parameters (NSE). For example, an NSE (Chromosome2,2000,4000) specifies two kilobases found on chromosome 2. GFF allows the annotation of these segments and it consists of one line per feature, each containing 9 columns of data. Each column is separated by a tab, making it a tabs-delimited file. It uses a header region with a ## string to include metadata.\n\nGeneral GFF3 structure\n\n\n\n\n\n\n\n\nPosition index\nPosition name\nDescription\n\n\n\n\n1\nseqid\nThe name of the sequence where the feature is located.\n\n\n2\nsource\nKeyword identifying the source of the feature, like a program (e.g. Augustus or RepeatMasker) or an organization (like TAIR).\n\n\n3\ntype\nThe feature type name, like “gene” or “exon”. In a well structured GFF file, all the children features always follow their parents in a single block (so all exons of a transcript are put after their parent “transcript” feature line and before any other parent transcript line). In GFF3, all features and their relationships should be compatible with the standards released by the Sequence Ontology Project.\n\n\n4\nstart\nGenomic start of the feature, with a 1-base offset. This is in contrast with other 0-offset half-open sequence formats, like BED.\n\n\n5\nend\nGenomic end of the feature, with a 1-base offset. This is the same end coordinate as it is in 0-offset half-open sequence formats, like BED.[citation needed]\n\n\n6\nscore\nNumeric value that generally indicates the confidence of the source in the annotated feature. A value of “.” (a dot) is used to define a null value.\n\n\n7\nstrand\nSingle character that indicates the strand of the feature; it can assume the values of “+” (positive, or 5’->3’), “-”, (negative, or 3’->5’), “.” (undetermined).\n\n\n8\nphase\nphase of CDS features; it can be either one of 0, 1, 2 (for CDS features) or “.” (for everything else). See the section below for a detailed explanation.\n\n\n9\nattributes\nAll the other information pertaining to this feature. The format, structure and content of this field is the one which varies the most between the three competing file formats.\n\n\n\n\n\n\n\n\n\nA typical content of a GFF3 file\n\n\n\n##gff-version 3\n##feature-ontology https://github.com/The-Sequence-Ontology/SO-Ontologies/blob/v3.1/so.obo\n# Annotated with Bakta\n# Software: v1.5.0\n# Database: v4.0\n# DOI: 10.1099/mgen.0.000685\n# URL: github.com/oschwengers/bakta\n##sequence-region contig_1 1 321788\ncontig_1    Bakta   region  1   321788  .   +   .   ID=contig_1;Name=contig_1\ncontig_1    Prodigal    CDS 15  272 .   +   0   ID=BDKKDL_00005;Name=hypothetical protein;locus_tag=BDKKDL_00005;product=hypothetical protein\ncontig_1    Prodigal    CDS 224 613 .   +   0   ID=BDKKDL_00010;Name=PE-PGRS family protein;locus_tag=BDKKDL_00010;product=PE-PGRS family protein;Dbxref=RefSeq:WP_003914742.1,SO:0001217,UniParc:UPI00019015A7,UniRef:UniRef100_A0A7U9K5X7,UniRef:UniRef50_A0A7U9K5X7,UniRef:UniRef90_A0A7U9K5X7\ncontig_1    Prodigal    CDS 1164    1568    .   +   0   ID=BDKKDL_00015;Name=Uncharacterized protein Rv2742A;locus_tag=BDKKDL_00015;product=Uncharacterized protein Rv2742A;Dbxref=GO:0005886,GO:0016021,RefSeq:WP_003912838.1,SO:0001217,UniParc:UPI00005EE738,UniRef:UniRef100_P0DMQ8,UniRef:UniRef50_P0DMQ8,UniRef:UniRef90_P0DMQ8\ncontig_1    Prodigal    CDS 1565    2377    .   -   0   ID=BDKKDL_00020;Name=Putative envelope-preserving system protein Rv2743c;locus_tag=BDKKDL_00020;product=Putative envelope-preserving system protein Rv2743c;Dbxref=GO:0016021,RefSeq:WP_003414026.1,SO:0001217,UniParc:UPI00000D0F36,UniRef:UniRef100_I6YA50,UniRef:UniRef50_I6YA50,UniRef:UniRef90_I6YA50\n...\n\n\nYou may have come across a GTF file. The GTF (Gene Transfer Format) file type shares the same format as GFF files, though it is used to define gene and transcript-related features exclusively."
  },
  {
    "objectID": "materials/03-file_formats/3.1-file_formats.html#newick",
    "href": "materials/03-file_formats/3.1-file_formats.html#newick",
    "title": "3.1 Common File Formats",
    "section": "3.1.10 NEWICK",
    "text": "3.1.10 NEWICK\nThe NEWICK format is a text file usually with the file extensions: .tree or .treefile. It is a way of representing graph-theoretical trees with edge lengths using parentheses and commas. It stores phylogenetic trees including nodes names and edge lengths.\nThe following tree:\n\n\n\nNewick Format Example tree\n\n\ncould be represented in Newick format in several ways\n(,,(,));                               no nodes are named\n(A,B,(C,D));                           leaf nodes are named\n(A,B,(C,D)E)F;                         all nodes are named\n(:0.1,:0.2,(:0.3,:0.4):0.5);           all but root node have a distance to parent\n(:0.1,:0.2,(:0.3,:0.4):0.5):0.0;       all have a distance to parent\n(A:0.1,B:0.2,(C:0.3,D:0.4):0.5);       distances and leaf names (popular)\n(A:0.1,B:0.2,(C:0.3,D:0.4)E:0.5)F;     distances and all names\n((B:0.2,(C:0.3,D:0.4)E:0.5)F:0.1)A;    a tree rooted on a leaf node (rare)\nBelow is the newick tree data of the first tree you will ever build in this course\nMyfirsttree.tree \n(TBNmA399:0.008270245,TBNmA464:0.007078265,((TBNmA217:0.009068723,((TBNmA361:0.008676792,TBNmA250:0.007952614)0.882:0.000644627,((ERX467883:0.019205883,(((TBNmA286:0.008210194,(TBNmA162:0.000672847,(TBNmA432:0.000681896,TBNmA437:0.000906203)0.520:0.000000005)1.000:0.007987255)0.963:0.009745124,ERX4639416:0.031087096)0.760:0.002127560,(ERX4639417:0.014406021,TBNmA120:0.017392319)0.983:0.003844014)0.561:0.000000005)0.733:0.000446004,(((((ERX2675537:0.037328220,(ERX2822389:0.035236770,((NC_002945.4:0.085788912,(TBNmA403:0.003585076,TBNmA265:0.005594965)1.000:0.077815900)1.000:0.045871008,(ERX4639418:0.021865797,(((TBNmA389:0.011858464,(TBNmA343:0.010603912,TBNmA187:0.010539288)0.887:0.001097047)1.000:0.025977064,((TBNmA041:0.023722379,((TBNmA271:0.001127595,((TBNmA452:0.008680633,TBNmA077:0.009375017)1.000:0.005680396,(TBNmA123:0.010406082,(TBNmA085:0.011381506,TBNmA064:0.009800991)0.946:0.001327826)0.996:0.003589739)0.988:0.009195987)0.881:0.002561916,((((TBNmA327:0.000000005,TBNmA345:0.000224666)1.000:0.012329485,(TBNmA321:0.000975869,TBNmA322:0.001858213)1.000:0.009131999)1.000:0.008206124,((TBNmA382:0.007331542,(TBNmA365:0.005435424,(TBNmA306:0.006093690,TBNmA117:0.007030120)0.543:0.000000005)0.771:0.000000005)0.998:0.002534799,(ERX1275306:0.007758780,(TBNmA050:0.004680429,(TBNmA191:0.006984201,(TBNmA412:0.007273051,TBNmA258:0.009516111)0.823:0.000000005)0.909:0.000672574)0.880:0.000492079)0.976:0.001602252)1.000:0.005254102)1.000:0.008769376,(TBNmA121:0.013273022,(TBNmA280:0.005420607,(TBNmA190:0.005355244,TBNmA428:0.005210953)0.870:0.000000005)0.972:0.002255197)1.000:0.017861767)0.707:0.000136299)0.884:0.000659873)1.000:0.004134566,(TBNmA257:0.002984847,(TBNmA377:0.000000005,(TBNmA362:0.000000005,TBNmA357:0.000000005)0.881:0.000000005)0.944:0.001325432)1.000:0.025946471)0.976:0.000000005)1.000:0.017127738,(ERX512130:0.009550754,(TBNmA394:0.001180640,TBNmA331:0.001151751)1.000:0.006179602)1.000:0.034250718)0.163:0.000000005)0.189:0.000110233)0.987:0.002076210)0.823:0.000755449)1.000:0.009140266,ERX467856:0.036035736)0.667:0.000000005,((TBNmA110:0.017454586,((ERX1749324:0.012028354,TBNmA262:0.011318248)0.985:0.004246403,(TBNmA348:0.012534351,TBNmA067:0.011426344)1.000:0.005264503)0.816:0.000000005)1.000:0.016810353,(TBNmA393:0.032119007,(TBNmA074:0.012340989,(ERX467886:0.000000005,ERX467895:0.000000005)1.000:0.012723309)1.000:0.018036487)0.707:0.001395736)0.999:0.004644168)0.872:0.000000005,(TBNmA395:0.025760081,((TBNmA351:0.011266183,TBNmA266:0.008431616)1.000:0.011565390,(TBNmA383:0.014217025,(TBNmA447:0.003756605,(TBNmA305:0.003438150,TBNmA435:0.004132593)0.993:0.002834680)1.000:0.006652517)1.000:0.009294668)0.858:0.000767446)1.000:0.008991216)0.708:0.000000005,TBNmA384:0.029259232)1.000:0.014945411)1.000:0.014745950)0.681:0.000000005)0.009:0.000000005,((TBNmA124:0.004508430,(TBNmA460:0.008309784,(TBNmA125:0.003377088,TBNmA195:0.005663590)0.953:0.001124832)0.751:0.000219064)0.897:0.000448976,TBNmA290:0.006384894)0.388:0.000000005)0.624:0.000000005);"
  },
  {
    "objectID": "materials/03-file_formats/3.1-file_formats.html#credit",
    "href": "materials/03-file_formats/3.1-file_formats.html#credit",
    "title": "3.1 Common File Formats",
    "section": "3.1.11 Credit",
    "text": "3.1.11 Credit\nInformation on this page has been adapted and modified from the following source(s):\n\nCredit: https://github.com/cambiotraining/sars-cov-2-genomics\nhttps://snipcademy.com/sequence-file-formats\nhttps://en.wikipedia.org/wiki\nWellcome Genome Campus Advanced Courses and Scientific Conferences 2017 - WORKING WITH PATHOGEN GENOMES Course Manual http://www.wellcome.ac.uk/advancedcourses"
  },
  {
    "objectID": "materials/04-sequence_qc/4.1-sequence_qc.html",
    "href": "materials/04-sequence_qc/4.1-sequence_qc.html",
    "title": "4.1 Sequencing Quality Control",
    "section": "",
    "text": "Teaching: 90 min || Exercises: 20 min"
  },
  {
    "objectID": "materials/04-sequence_qc/4.1-sequence_qc.html#overview",
    "href": "materials/04-sequence_qc/4.1-sequence_qc.html#overview",
    "title": "4.1 Sequencing Quality Control",
    "section": "Overview",
    "text": "Overview\n\n\n\n\n\n\n\n\n\n\n\n\nQuestions:\n\n\n\n\nHow do you perform basic statistics to check the quality of next generation sequence reads?\nHow do you identify contaminants in your sequences?\nWhat are the various tools used for carrying out quality control?\n\n\n\n\n\n\n\n\n\nLearning Objectives:\n\n\n\n\nTo perform a QC assessment of high throughput next generation sequence data\nInterpret and critically evaluate data quality reports\nTo identify possible contamination in high throughput sequence data\nRun simple scripts to achieve quality control of your genomic data\n\n\n\n\n\n\n\n\n\nKey Points:\n\n\n\n\nBefore analysing your next generation sequencing data, it is key to quality control the data and only carry on with genomes that pass quality control checks\nBest practice quality control assessments include:\n\nBase quality check\nIdentifying mismatches\nGC content and bias checks\nFalse insertions and deletions\nIdentifying contamination\n\nBest practice quality control tools for performing the quality checks include\n\nfastq-scan\nFastQC\nfastp\nKraken 2\nBracken\nMultiQC\n\nIt is a good practice to clean up your directory as you proceed from analysis to analysis to avoid getting the error message that tells you you are running out of space/memory."
  },
  {
    "objectID": "materials/04-sequence_qc/4.1-sequence_qc.html#background",
    "href": "materials/04-sequence_qc/4.1-sequence_qc.html#background",
    "title": "4.1 Sequencing Quality Control",
    "section": "4.1.1 Background",
    "text": "4.1.1 Background\nBefore we delve into having a look at our own genomic data. Lets take some minutes to explore what to look out for when performing Quality Control (QC) checks on our sequences. For this course, we will largely focus on next generation sequences obtained from Illumina sequencers. As you may already know from the previous lesson, the main output files expected from our Illumina sequencer are .fastq files.\n\nQC assessment of NGS data\nAs you may already know, QC is an important part of any analysis. In this section we are going to look at some of the metrics and graphs that can be used to assess the QC of NGS data.\n\nBase quality\nIllumina sequencing technology relies on sequencing by synthesis. One of the most common problems with this is dephasing. For each sequencing cycle, there is a possibility that the replication machinery slips and either incorporates more than one nucleotide or perhaps misses to incorporate one at all. The more cycles that are run (i.e. the longer the read length gets), the greater the accumulation of these types of errors gets. This leads to a heterogeneous population in the cluster, and a decreased signal purity, which in turn reduces the precision of the base calling. The figure below shows an example of this.\n\n\n\nBase Quality\n\n\nBecause of dephasing, it is possible to have high-quality data at the beginning of the read but really low-quality data towards the end of the read. In those cases you can decide to trim off the low-quality reads, for example using a tool called Trimmomatic. In this workshop, we will do this using the tool fastp. In addition to trimming low quality reads, fastpwill also be used to trim off Illumina adapter/primer sequences.\nThe figures below shows an example of a high-quality read data (left) and a poor quality read data (right).\n\n\n\n\n\n\nHigh-quality read data\n\n\n\n\n\n\n\nPoor quality read data\n\n\n\n\n\n\nBase Quality Comparison\n\n\n\nIn addition to Phasing noise and signal decay resulting from dephasing issues described above, there are several different reasons for a base to be called incorrectly. You can lookup these later by clicking here.\n\n\nMismatches per cycle\nAligning reads to a high-quality reference genome can provide insight to the quality of a sequencing run by showing you the mismatches to the reference sequence. This can help you detect cycle-specific errors. Mismatches can occur due to two main causes, sequencing errors and differences between your sample and the reference genome, which is important to bear in mind when interpreting mismatch graphs. The figures below show an example of a good run (top) and a bad one (bottom). In the first figure, the distribution of the number of mismatches is even between the cycles, which is what we would expect from a good run. However, in the second figure, two cycles stand out with a lot of mismatches compared to the other cycles.\n\n\n\nGood run\n\n\n\n\n\nPoor run\n\n\n\n\nGC bias\nIt is a good idea to compare the GC content of the reads against the expected distribution in a reference sequence. The GC content varies between species, so a shift in GC content like the one seen below (right image) could be an indication of sample contamination. In the left image below, we can see that the GC content of the sample is about the same as for the theoretical reference, at ~65%. However, in the right figure, the GC content of the sample shows two distribution, one is closer to 40% and the other closer to 65%, indicating that there is an issue with this sample — a possible missed sample. Note that, suspecting contamination is perfectly fine for the species we are dealing with (MTBC). For other bacteria where there may be possibility of gene transfer, one can imagine that, such a situation may be from inheriting some plasmids that have a totally different GC content to the bacteria chromosome (This is arguable though).\n\n\n\n\n\n\nSingle GC distribution\n\n\n\n\n\n\n\nDouble GC distribution\n\n\n\n\n\n\nFigure: Base Quality Comparison\n\n\n\n\n\nGC content by cycle\nLooking at the GC content per cycle can help detect if the adapter sequence was trimmed. For a random library, it is expected to be little to no difference between the different bases of a sequence run, so the lines in this plot should be parallel with each other like in the first of the two figures below. In the second of the figures, the initial spikes are likely due to adapter sequences that have not been removed.\n\n\n\nGood run\n\n\n\n\n\nPoor run\n\n\n\n\nInsert size\nFor paired-end sequencing the size of DNA fragments also matters. In the first of the examples below, the insert size peaks around 440 bp. In the second however, there is also a peak at around 200 bp. This indicates that there was an issue with the fragment size selection during library prep.\n\n\n\nGood run\n\n\n\n\n\nPoor run\n\n\n\n\nInsertions/Deletions per cycle\nSometimes, air bubbles occur in the flow cell, which can manifest as false indels. The spike in the second image provides an example of how this can look.\n\n\n\nGood run\n\n\n\n\n\nPoor run\n\n\nIn addition to the QC plots you’ve encountered so far, there are other metrics that are generated with very powerful tools. For this workshop, we will explore these quality metrics with the help of fastq-scan and FastQC tools. It is often not a good practice to carry on analysis on samples that are contaminated with sequences from other species. We will identify contamination using either one of two ways. As earlier mentioned, the GC content varies between species, so a shift in GC content could be an indication of sample contamination. One other way of identifying sample contamination is by using specialized tools to determine/predict the species composition of your sample. For this course, we will determine species composition using the Kraken 2 database."
  },
  {
    "objectID": "materials/04-sequence_qc/4.1-sequence_qc.html#change-to-the-qc-directory-and-activate-the-qc-environment",
    "href": "materials/04-sequence_qc/4.1-sequence_qc.html#change-to-the-qc-directory-and-activate-the-qc-environment",
    "title": "4.1 Sequencing Quality Control",
    "section": "4.1.2 Change to the QC directory and activate the qc environment",
    "text": "4.1.2 Change to the QC directory and activate the qc environment\nBefore we start, change into the QC/ directory and activate the qc environment:\ncd ~/Desktop/workshop_files_Bact_Genomics_2023/04_QC/\nmamba activate qc"
  },
  {
    "objectID": "materials/04-sequence_qc/4.1-sequence_qc.html#generating-qc-stats-and-metrics",
    "href": "materials/04-sequence_qc/4.1-sequence_qc.html#generating-qc-stats-and-metrics",
    "title": "4.1 Sequencing Quality Control",
    "section": "4.1.3 Generating QC stats and metrics",
    "text": "4.1.3 Generating QC stats and metrics\nWe are now ready to explore some quality metrics on our sequence data.\n\n\n\n\n\n\nTools used and how to get help\n\n\n\nThis tutorial uses fastq-scan, FastQC, fastp, Kraken 2, Bracken and MultiQC, which have been preinstalled for you in a virtual environment called qc. This is to help speed up the pace of the workshop. You can lookup the setup page for how to install these tools later. In a latter chapter of the course, you will also be introduced to how to set up these virtual environments and explore its usefulness. For each tool, to get help messages that describe how they are used, you can simply type the name of the tool and hit enter. This only works if you activate the environment in which the tools have already been installed. Alternatively, you can use the help flag -help or -h as appropriate.\n\n\n\nDisk Usage I — Before analysis\nBefore we start investigating our genomic sequences, let’s pause and check the space of our current working directory.\nYou can do this with the disk usage du command\ndu -h\n\n\nCurrent Disk Space In QC Directory\n\n~1.3G\n\nNow, keep this value in mind, we will come back to it at the end of the chapter.\n\n\nfastq-scan\nfastq-scan reads a FASTQ from STDIN and outputs summary statistics (read lengths, per-read qualities, per-base qualities) in JSON format.\n\n\n\n\n\n\nHelp\n\n\n\nDo this to get the help information for fastq-scan\nfastq-scan -h\nUsage: cat FASTQ | fastq-scan [options]\nVersion: 1.0.0\n\nOptional arguments:\n -g INT   Genome size for calculating estimated sequencing coverage. (Default 1)\n -p INT   ASCII offset for input quality scores, can be 33 or 64. (Default 33)  \n -q       Print only the QC stats, do not print read lengths or per-base quality scores\n -v       Print version information and exit\n -h       Show this message and exit\n\n\n\n\n\n\n\n\nUsage\n\n\n\nfastq-scan reads from STDIN, so pretty much any FASTQ output can be piped into fastq-scan. There are a few things to be aware of. It assumes that all FASTQ entries are the four line variant. Also, it has a PHRED offset (33 vs 64) guesser function. By default it is set to PHRED33, it could produce errors if there are not any PHRED33 or PHRED64 specific characters in the quality scores.\n\n\nYou can now go ahead and perform the fastq-scan with the below command.\n\n\n\n\n\n\nNote\n\n\n\nTry this command out first. You may have to add < after the zcat command depending on your OS. zcat works differently with different OS.\nDo this to run the forward reads:\nzcat G26832_R1.fastq.gz | fastq-scan -g 4300000 > G26832_R1_fastq-scan.json\nAnd for the reverse reads:\nzcat G26832_R2.fastq.gz | fastq-scan -g 4300000 > G26832_R2_fastq-scan.json\n\n\nYou should now see two new files generated in your current directory.\nTo see if the files have been created, use this command:\n\n\n\n\n\n\nls | grep \"fastq-scan.json\"\nG26832_R1_fastq-scan.json\nG26832_R2_fastq-scan.json\n\n\n\nTo view the statistics for the forward reads use this command:\ncat G26832_R1_fastq-scan.json\n\n\n\n\n\n\nNote\n\n\n\nThe file created is rather small, so you can afford to use cat to view the entire content. You may want to use head or less for huge files.\n\n\nAs you may have realized, the content of the output file doesn’t look friendly.\nLet’s convert the .json files into a more friendly format, say tsv. You should know what tsv files are by now. If not, you can go back to the previous chapter on file-formats to equip yourself.\n\nParse json files into tsv format\nWe will run a simple python script to achieve this purpose. In your working directory, you will find a file named fastq-scan_parser.py. This is a simple python script that will do the job for us.\nWe can go ahead and execute that python script by running the command below:\npython fastq-scan_parser.py\nYou should now see another file generated in your directory called fastq-scan_summary.tsv. What the python script did was to extract the relevant information from all .json files in the working directory and convert them to a tsv file.\nNow let’s go ahead and have a look at the tsv file with the command\ncat fastq-scan_summary.tsv\nsample  total_bp    coverage    read_total  read_min    read_mean   read_std    read_median read_max    read_25th   read_75th   qual_min    qual_mean   qual_std    qual_max    qual_median qual_25th   qual_75th\nG26832_R1   305515607   71.0501 3024907 101 101.0   0.0 101 101 101 101 14  35.2243 3.49844 37  37  35  37\nAlternatively, you can open the tsv file with any appropriate GUI software (excel or libreoffice)\n\n\n\nFastQC\nFastQC is a program designed to spot potential problems in high throughput sequencing datasets. It runs a set of analyses on one or more raw sequence files in fastq or bam format and produces a report which summarises the results. It provides a modular set of analyses which you can use to give a quick impression of whether your data has any problems of which you should be aware before doing any further analysis.\nFastQC will highlight any areas where the library looks unusual and where you should take a closer look. The program is not tied to any specific type of sequencing technique and can be used to look at libraries coming from a large number of different experiment types (Genomic Sequencing, ChIP-Seq, RNA-Seq, BS-Seq etc etc).\nThe main functions of FastQC are:\n\nImport of data from BAM, SAM or FastQ files (any variant)\nProviding a quick overview to tell you in which areas there may be problems\nSummary graphs and tables to quickly assess your data\nExport of results to an HTML based permanent report\nOffline operation to allow automated generation of reports without running the interactive application\n\n\n\n\n\n\n\nHelp\n\n\n\nDo this to get the help information for FastQC\nfastqc -h\n\n            FastQC - A high throughput sequence QC analysis tool\n\nSYNOPSIS\n\n        fastqc seqfile1 seqfile2 .. seqfileN\n\n    fastqc [-o output dir] [--(no)extract] [-f fastq|bam|sam] \n           [-c contaminant file] seqfile1 .. seqfileN\n\nDESCRIPTION\n\n    FastQC reads a set of sequence files and produces from each one a quality\n    control report consisting of a number of different modules, each one of \n    which will help to identify a different potential type of problem in your\n    data.\n    \n    If no files to process are specified on the command line then the program\n    will start as an interactive graphical application.  If files are provided\n    on the command line then the program will run with no user interaction\n    required.  In this mode it is suitable for inclusion into a standardised\n    analysis pipeline.\n    \n    The options for the program as as follows:\n    \n    -h --help       Print this help file and exit\n    \n    -v --version    Print the version of the program and exit\n    \n    -o --outdir     Create all output files in the specified output directory.\n                    Please note that this directory must exist as the program\n                    will not create it.  If this option is not set then the \n                    output file for each sequence file is created in the same\n                    directory as the sequence file which was processed.\n                    \n    --casava        Files come from raw casava output. Files in the same sample\n                    group (differing only by the group number) will be analysed\n                    as a set rather than individually. Sequences with the filter\n                    flag set in the header will be excluded from the analysis.\n                    Files must have the same names given to them by casava\n                    (including being gzipped and ending with .gz) otherwise they\n                    won't be grouped together correctly.\n...\n\n\n\n\n\n\n\n\nUsage\n\n\n\nFastQC reads a set of sequence files and produces from each one a quality control report consisting of a number of different modules, each one of which will help to identify a different potential type of problem in your data. If no files to process are specified on the command line then the program will start as an interactive graphical application. If files are provided on the command line then the program will run with no user interaction required. In this mode it is suitable for inclusion into a standardised analysis pipeline.\nThe general format of the command is:\nfastqc [-o output dir] [--(no)extract] [-f fastq|bam|sam] [-c contaminant file] seqfile1 seqfile2 .. seqfileN\n\n\nYou can now go ahead and perform the fastQC with the below command.\n\n\n\n\n\n\nNote\n\n\n\nDo this to run the forward reads:\nfastqc --threads 4 G26832_R1.fastq.gz\nStarted analysis of G26832_R1.fastq.gz\nApprox 5% complete for G26832_R1.fastq.gz\nApprox 10% complete for G26832_R1.fastq.gz\nApprox 15% complete for G26832_R1.fastq.gz\nApprox 20% complete for G26832_R1.fastq.gz\nApprox 25% complete for G26832_R1.fastq.gz\n...\nAnd for the reverse reads:\nfastqc --threads 4 G26832_R2.fastq.gz\n\n\n\n\n\n\n\n\nOutput\n\n\n\nIf you specified an output -o directory then you should look out for that file being created in that directory. For our situation, we didn’t specify any output directory so the result will just be in the current directory. You should now see two new files generated in your current directory.\n\n\nHow do you tell which file has been recently produced?\n\n\nHint\n\nPerform a simple ls command with the arguments -lhrt and the last file in the output should be the most recent.\nls -lhrt\n\nWe are interested in the one that ends with .html. Go ahead and open it. Being an .html file, it will prefer to open in a browser, and that’s just how we want to make sense out of it.\nWe have already seen some of the content of the output file from the background to this chapter. However, this time, let’s look at a few more and also with some more details.\n\nBasic Statistics\nThe first information you encounter is the basic statistics. The Basic Statistics module generates some simple composition statistics for the file analysed.\n\nFilename: The original filename of the file which was analysed\nFile type: Says whether the file appeared to contain actual base calls or colorspace data which had to be converted to base calls\nEncoding: Says which ASCII encoding of quality values was found in this file.\nTotal Sequences: A count of the total number of sequences processed. There are two values reported, actual and estimated. At the moment these will always be the same.\nFiltered Sequences: If running in Casava mode sequences flagged to be filtered will be removed from all analyses. The number of such sequences removed will be reported here. The total sequences count above will not include these filtered sequences and will the number of sequences actually used for the rest of the analysis.\nSequence Length: Provides the length of the shortest and longest sequence in the set. If all sequences are the same length only one value is reported.\n%GC: The overall %GC of all bases in all sequences\n\n\n\n\nFastQC Basic Statistics\n\n\n\n\nPer Base Sequence Quality\nThe plot shows an overview of the range of quality values across all bases at each position in the FastQ file.\n\n\n\nPer Base Sequence Quality\n\n\nFor each position a BoxWhisker type plot is drawn. The elements of the plot are as follows: - The central red line is the median value - The yellow box represents the inter-quartile range (25-75%) - The upper and lower whiskers represent the 10% and 90% points - The blue line represents the mean quality\nThe y-axis on the graph shows the quality scores. The higher the score the better the base call. The background of the graph divides the y-axis into very good quality calls (green), calls of reasonable quality (orange), and calls of poor quality (red). The quality of calls on most platforms will degrade as the run progresses, so it is common to see base calls falling into the orange area towards the end of a read.\nIt should be mentioned that there are number of different ways to encode a quality score in a FastQ file. FastQC attempts to automatically determine which encoding method was used, but in some very limited datasets it is possible that it will guess this incorrectly (ironically only when your data is universally very good!). The title of the graph will describe the encoding FastQC thinks your file used.\nNB. Results from this module will not be displayed if your input is a BAM/SAM file in which quality scores have not been recorded.\nWarning A warning will be issued if the lower quartile for any base is less than 10, or if the median for any base is less than 25.\nFailure This module will raise a failure if the lower quartile for any base is less than 5 or if the median for any base is less than 20.\n\n\nCommon reasons for warnings\n\nThe most common reason for warnings and failures in this module is a general degradation of quality over the duration of long runs. In general sequencing chemistry degrades with increasing read length and for long runs you may find that the general quality of the run falls to a level where a warning or error is triggered.\nIf the quality of the library falls to a low level then the most common remedy is to perform quality trimming where reads are truncated based on their average quality. For most libraries where this type of degradation has occurred you will often be simultaneously running into the issue of adapter read-through so a combined adapter and quality trimming step is often employed.\nAnother possibility is that a warn / error is triggered because of a short loss of quality earlier in the run, which then recovers to produce later good quality sequence. This can happen if there is a transient problem with the run (bubbles passing through a flow cell for example). You can normally see this type of error by looking at the per-tile quality plot (if available for your platform). In these cases trimming is not advisable as it will remove later good sequence, but you might want to consider masking bases during subsequent mapping or assembly.\nIf your library has reads of varying length then you can find a warning or error is triggered from this module because of very low coverage for a given base range. Before committing to any action, check how many sequences were responsible for triggering an error by looking at the sequence length distribution module results.\n\n\n\nPer Sequence Quality Scores\nThe per sequence quality score report allows you to see if a subset of your sequences have universally low quality values. It is often the case that a subset of sequences will have universally poor quality, often because they are poorly imaged (on the edge of the field of view etc), however these should represent only a small percentage of the total sequences.\n\n\n\nPer Sequence Quality Scores\n\n\nIf a significant proportion of the sequences in a run have overall low quality then this could indicate some kind of systematic problem - possibly with just part of the run (for example one end of a flow cell).\nNB. Results from this module will not be displayed if your input is a BAM/SAM file in which quality scores have not been recorded.\nWarning A warning is raised if the most frequently observed mean quality is below 27 - this equates to a 0.2% error rate.\nFailure An error is raised if the most frequently observed mean quality is below 20 - this equates to a 1% error rate.\n\n\nCommon reasons for warnings\n\nThis module is generally fairly robust and errors here usually indicate a general loss of quality within a run. For long runs this may be alleviated through quality trimming. If a bi-modal, or complex distribution is seen then the results should be evaluated in concert with the per-tile qualities (if available) since this might indicate the reason for the loss in quality of a subset of sequences.\n\n\n\nPer Base Sequence Content\nPer Base Sequence Content plots out the proportion of each base position in a file for which each of the four normal DNA bases has been called.\n\n\n\nPer Base Sequence Content\n\n\nIn a random library you would expect that there would be little to no difference between the different bases of a sequence run, so the lines in this plot should run parallel with each other. The relative amount of each base should reflect the overall amount of these bases in your genome, but in any case they should not be hugely imbalanced from each other.\nIt’s worth noting that some types of library will always produce biased sequence composition, normally at the start of the read. Libraries produced by priming using random hexamers (including nearly all RNA-Seq libraries) and those which were fragmented using transposases inherit an intrinsic bias in the positions at which reads start. This bias does not concern an absolute sequence, but instead provides enrichment of a number of different K-mers at the 5’ end of the reads. Whilst this is a true technical bias, it isn’t something which can be corrected by trimming and in most cases doesn’t seem to adversely affect the downstream analysis. It will however produce a warning or error in this module.\nWarning This module issues a warning if the difference between A and T, or G and C is greater than 10% in any position.\nFailure This module will fail if the difference between A and T, or G and C is greater than 20% in any position.\n\n\nCommon reasons for warnings\n\nThere are a number of common scenarios which would elicit a warning or error from this module.\nOverrepresented sequences: If there is any evidence of overrepresented sequences such as adapter dimers or rRNA in a sample then these sequences may bias the overall composition and their sequence will emerge from this plot. Biased fragmentation: Any library which is generated based on the ligation of random hexamers or through tagmentation should theoretically have good diversity through the sequence, but experience has shown that these libraries always have a selection bias in around the first 12bp of each run. This is due to a biased selection of random primers, but doesn’t represent any individually biased sequences. Nearly all RNA-Seq libraries will fail this module because of this bias, but this is not a problem which can be fixed by processing, and it doesn’t seem to adversely affect the ability to measure expression. Biased composition libraries: Some libraries are inherently biased in their sequence composition. The most obvious example would be a library which has been treated with sodium bisulphite which will then have converted most of the cytosines to thymines, meaning that the base composition will be almost devoid of cytosines and will thus trigger an error, despite this being entirely normal for that type of library If you are analysing a library which has been aggressively adapter trimmed then you will naturally introduce a composition bias at the end of the reads as sequences which happen to match short stretches of adapter are removed, leaving only sequences which do not match. Sudden deviations in composition at the end of libraries which have undergone aggressive trimming are therefore likely to be spurious.\n\n\n\nPer Base N Content\nIf a sequencer is unable to make a base call with sufficient confidence then it will normally substitute an N rather than a conventional base] call\nThis module plots out the percentage of base calls at each position for which an N was called.\n\n\n\nPer Base N Content\n\n\nIt’s not unusual to see a very low proportion of Ns appearing in a sequence, especially nearer the end of a sequence. However, if this proportion rises above a few percent it suggests that the analysis pipeline was unable to interpret the data well enough to make valid base calls.\nWarning This module raises a warning if any position shows an N content of >5%.\nFailure This module will raise an error if any position shows an N content of >20%.\n\n\nCommon reasons for warnings\n\nThe most common reason for the inclusion of significant proportions of Ns is a general loss of quality, so the results of this module should be evaluated in concert with those of the various quality modules. You should check the coverage of a specific bin, since it’s possible that the last bin in this analysis could contain very few sequences, and an error could be prematurely triggered in this case.\nAnother common scenario is the incidence of a high proportions of N at a small number of positions early in the library, against a background of generally good quality. Such deviations can occur when you have very biased sequence composition in the library to the point that base callers can become confused and make poor calls. This type of problem will be apparent when looking at the per-base sequence content results.\n\n\n\n\nfastp\nclick here to view the publication on fastp fastp is a tool designed to provide fast all-in-one pre-processing for FastQ files.\n\n\n\n\n\n\nHelp\n\n\n\nDo this to get the help information for fastp\nfastp --help\nfastp: an ultra-fast all-in-one FASTQ pre-processor\nversion 0.23.2\nusage: fastp [options] ... \noptions:\n  -i, --in1                            read1 input file name (string [=])\n  -o, --out1                           read1 output file name (string [=])\n  -I, --in2                            read2 input file name (string [=])\n  -O, --out2                           read2 output file name (string [=])\n      --unpaired1                      for PE input, if read1 passed QC but read2 not, it will be written to unpaired1. Default is to discard it. (string [=])\n      --unpaired2                      for PE input, if read2 passed QC but read1 not, it will be written to unpaired2. If --unpaired2 is same as --unpaired1 (default mode), both unpaired reads will be written to this same file. (string [=])\n...\n\n\n\nFeatures\n\ncomprehensive quality profiling for both before and after filtering data (quality - curves, base contents, KMER, Q20/Q30, GC Ratio, duplication, adapter contents…)\nfilter out bad reads (too low quality, too short, or too many N…)\ncut low quality bases for per read in its 5’ and 3’ by evaluating the mean quality - from a sliding window (like Trimmomatic but faster).\ntrim all reads in front and tail\ncut adapters. Adapter sequences can be automatically detected, which means you don’t have to input the adapter sequences to trim them.\ncorrect mismatched base pairs in overlapped regions of paired end reads, if one base is with high quality while the other is with ultra low quality\ntrim polyG in 3’ ends, which is commonly seen in NovaSeq/NextSeq data. Trim polyX in 3’ ends to remove unwanted polyX tailing (i.e. polyA tailing for mRNA-Seq data)\npreprocess unique molecular identifier (UMI) enabled data, shift UMI to sequence name.\nreport JSON format result for further interpreting.\nvisualize quality control and filtering results on a single HTML page (like FASTQC - but faster and more informative).\nsplit the output to multiple files (0001.R1.gz, 0002.R1.gz…) to support parallel - processing. Two modes can be used, limiting the total split file number, or limiting the lines of each split file.\nsupport long reads (data from PacBio / Nanopore devices).\nsupport reading from STDIN and writing to STDOUT\nsupport interleaved input\nsupport ultra-fast FASTQ-level deduplication\n\n\n\n\n\n\n\nUsage\n\n\n\nfor single end data (not compressed)\nfastp -i in.fq -o out.fq\nfor paired end data (gzip compressed)\nfastp -i in_R1.fq.gz -I in_R2.fq.gz -o out_R1.fq.gz -O out_R2.fq.gz\nBy default, the HTML report is saved to fastp.html (can be specified with -h option), and the JSON report is saved to fastp.json (can be specified with -j option).\n\n\nYou can now go ahead and perform the fastp with the below command. We will perform the run on both reads.\n\n\n\n\n\n\nNote\n\n\n\nfastp --cut_front --cut_tail --trim_poly_x --cut_mean_quality 30 --qualified_quality_phred 30 --unqualified_percent_limit 10 --length_required 50 --in1 G26832_R1.fastq.gz --in2 G26832_R2.fastq.gz --out1 G26832_R1.trim.fastq.gz --out2 G26832_R2.trim.fastq.gz --json G26832_fastp.json --html G26832_fastp.html --thread 4 --detect_adapter_for_pe\nDetecting adapter sequence for read1...\n>Illumina TruSeq Adapter Read 1\nAGATCGGAAGAGCACACGTCTGAACTCCAGTCA\n\nDetecting adapter sequence for read2...\n>Illumina TruSeq Adapter Read 2\nAGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGT\n\nRead1 before filtering:\ntotal reads: 3024907\ntotal bases: 305515607\nQ20 bases: 291946114(95.5585%)\nQ30 bases: 278924986(91.2965%)\n\nRead2 before filtering:\ntotal reads: 3024907\ntotal bases: 305515607\nQ20 bases: 283844973(92.9069%)\nQ30 bases: 265510454(86.9057%)\n\nRead1 after filtering:\ntotal reads: 2042550\ntotal bases: 204034659\nQ20 bases: 203578565(99.7765%)\nQ30 bases: 202244612(99.1227%)\n...\nYou can also read out what is printed out on the terminal as a log file by passing the output to a textfile named fastp.log. This way, you don’t see all the long print out on the terminal.\nfastp --cut_front --cut_tail --trim_poly_x --cut_mean_quality 30 --qualified_quality_phred 30 --unqualified_percent_limit 10 --length_required 50 --in1 G26832_R1.fastq.gz --in2 G26832_R2.fastq.gz --out1 G26832_R1.trim.fastq.gz --out2 G26832_R2.trim.fastq.gz --json G26832_fastp.json --html G26832_fastp.html --thread 4 --detect_adapter_for_pe > fastp.log\n\n\nYou should now see four new files generated in your current directory.\nG26832_R2.trim.fastq.gz\nG26832_R1.trim.fastq.gz\nG26832_fastp.json\nG26832_fastp.html\nNow let’s go ahead and see what our output looks like by opening the G26832_fastp.html file. You should be able to interpret the output by now.\n\n\n\nSummary of fastp report\n\n\nLet’s have a look at the quality of both reads before and after trimming. What obvious differences do you notice.\n\n\n\n\n\n\nRead Quality before filtering\n\n\n\n\n\n\n\nRead Quality after filtering\n\n\n\n\n\n\nRead Quality before and after fastp processing\n\n\n\n\n\n\n\n\n\nBase content quality before filtering\n\n\n\n\n\n\n\nBase content quality after filtering\n\n\n\n\n\n\nBase Content Quality before and after fastp processing"
  },
  {
    "objectID": "materials/04-sequence_qc/4.1-sequence_qc.html#identifying-contamination",
    "href": "materials/04-sequence_qc/4.1-sequence_qc.html#identifying-contamination",
    "title": "4.1 Sequencing Quality Control",
    "section": "4.1.4 Identifying contamination",
    "text": "4.1.4 Identifying contamination\nIt is always a good idea to check that your data is from the species you expect it to be. A very useful tool for this is Kraken. In this workshop we will go through how you can use Kraken to check your samples for contamination.\n\nKraken 2\nclick here to view the publication on Kraken 2 kraken is a taxonomic sequence classifier that assigns taxonomic labels to DNA sequences. Kraken examines the k-mers within a query sequence and uses the information within those k-mers to query a database. That database maps k-mers to the lowest common ancestor (LCA) of all genomes known to contain a given k-mer.\nThe first version of Kraken used a large indexed and sorted list of -mer/LCA pairs as its database. While fast, the large memory requirements posed some problems for users, and so Kraken 2 was created to provide a solution to those problems.\n\n\n\n\n\n\nHelp\n\n\n\nDo this to get the help information for Kraken 2\nKraken2 --help\nUsage: kraken2 [options] <filename(s)>\n\nOptions:\n  --db NAME               Name for Kraken 2 DB\n                          (default: none)\n  --threads NUM           Number of threads (default: 1)\n  --quick                 Quick operation (use first hit or hits)\n  --unclassified-out FILENAME\n                          Print unclassified sequences to filename\n  --classified-out FILENAME\n                          Print classified sequences to filename\n  --output FILENAME       Print output to filename (default: stdout); \"-\" will\n                          suppress normal output\n  --confidence FLOAT      Confidence score threshold (default: 0.0); must be\n                          in [0, 1].\n...\n\n\n\n\n\n\n\n\nUsage\n\n\n\nKraken 2 is run on a database. You don’t have to worry about the database for now. All has been setup for you. To set up the database yourself, visit the setup page\nThe general synthax for running Kraken 2 is:\nkraken2 --db $DBNAME [other_options] <filename(s)>\n\n\nTo run Kraken 2, you need to provide the path to the database. By default, the input files are assumed to be in FASTA format, so in this case we also need to tell Kraken that our input files are in FASTQ format, gzipped, and that they are paired end reads.\nYou can now go ahead and run Kraken 2 with the below command.\n\n\n\n\n\n\nNote\n\n\n\nTake note of these: 1. You have to specify the directory to the database 2. Our input files will be the trimmed fastq sequences from the fastp analysis.\nDo this to run Kraken 2: \nkraken2 --db ../database/minikraken2_v1_8GB --threads 4 --unclassified-out G26832.unclassified#.fastq --classified-out G26832.classified#.fastq --report G26832.kraken2.report.txt --output G26832.kraken2.out --gzip-compressed --report-zero-counts --paired G26832_R1.trim.fastq.gz G26832_R2.trim.fastq.gz\nLoading database information... done.\n2042550 sequences (407.73 Mbp) processed in 18.212s (6729.3 Kseq/m, 1343.29 Mbp/m).\n  2038335 sequences classified (99.79%)\n  4215 sequences unclassified (0.21%)\n\n\nYou should now see some new files generated in your current directory. How many new files have been generated? Do you see that the fastq files are unzipped?\n\n\n\n\n\n\nLet’s save some disk space by zipping the four new fastq files created\n\n\n\ngzip *.fastq\n\n\nYou can try to read the two kraken report output text files created, but I’m sure that won’t make real sense to you.\n\n\nQuick look at Kraken 2 report\n\nRun the command to have a quick look at the Kraken 2 report.\nhead -n 20 G26832.kraken2.report.txt\nThe six columns in this file are: - Percentage of reads covered by the clade rooted at this taxon - Number of reads covered by the clade rooted at this taxon - Number of reads assigned directly to this taxon - A rank code, indicating (U)nclassified, (D)omain, (K)ingdom, (P)hylum, (C)lass, (O)rder, (F)amily, (G)enus, or (S)pecies. All other ranks are simply ‘-’. - NCBI taxonomy ID - Scientific name\n\nJust like we did for the fastq-scan output, we will have to convert the Kraken 2 output to a more readable format. Before we do this, we will have to perform one more analysis on our sample. The Kraken 2 often does not provide exhaustive results. To re-estimate taxonomic abundance of the samples we have to do this with another tool called Bracken using the output of Kraken 2.\n\n\nBracken\nclick here to view the publication on Bracken Bracken is a companion program to Kraken 1, KrakenUniq, or Kraken 2 While Kraken classifies reads to multiple levels in the taxonomic tree, Bracken allows estimation of abundance at a single level using those classifications (e.g. Bracken can estimate abundance of species within a sample)\n\n\n\n\n\n\nHelp\n\n\n\nDo this to get the help information for fastq-scan \nbracken -h\nUsage: bracken -d MY_DB -i INPUT -o OUTPUT -w OUTREPORT -r READ_LEN -l LEVEL -t THRESHOLD\n  MY_DB          location of Kraken database\n  INPUT          Kraken REPORT file to use for abundance estimation\n  OUTPUT         file name for Bracken default output\n  OUTREPORT      New Kraken REPORT output file with Bracken read estimates\n  READ_LEN       read length to get all classifications for (default: 100)\n  LEVEL          level to estimate abundance at [options: D,P,C,O,F,G,S,S1,etc] (default: S)\n  THRESHOLD      number of reads required PRIOR to abundance estimation to perform reestimation (default: 0)\n\n\n\n\n\n\n\n\nUsage\n\n\n\nThe general synthax is:\nbracken -d ${KRAKEN_DB} -t ${THREADS} -k ${KMER_LEN} -l ${READ_LEN} -i {INPUT} -o {OUTPUT}\nNB. Bracken relies on the Kraken 2 database, so we will specify the same directory as before and also use the output report of Kraken 2 as our input.\n\n\nYou can now go ahead and perform Bracken with the below command.\n\n\n\n\n\n\nNote\n\n\n\nRun Bracken with the following command:\nbracken -l S -t 10 -d ../database/minikraken2_v1_8GB -i G26832.kraken2.report.txt -o G26832_bracken_S.tsv\n >> Checking for Valid Options...\n >> Running Bracken \n      >> python src/est_abundance.py -i G26832.kraken2.report.txt -o G26832_bracken_S.tsv -k ../../Analysis/database/minikraken2_v1_8GB/database100mers.kmer_distrib -l S -t 10\nPROGRAM START TIME: 11-09-2022 18:28:06\n>> Checking report file: G26832.kraken2.report.txt\nBRACKEN SUMMARY (Kraken report: G26832.kraken2.report.txt)\n    >>> Threshold: 10 \n    >>> Number of species in sample: 11780 \n          >> Number of species with reads > threshold: 17 \n          >> Number of species with reads < threshold: 11763 \n    >>> Total reads in sample: 2042550\n          >> Total reads kept at species level (reads > threshold): 75467\n          >> Total reads discarded (species reads < threshold): 60\n          >> Reads distributed: 1962804\n          >> Reads not distributed (eg. no species above threshold): 4\n          >> Unclassified reads: 4215\nBRACKEN OUTPUT PRODUCED: G26832_bracken_S.tsv\nPROGRAM END TIME: 11-09-2022 18:28:06\n  Bracken complete.\n\n\nYou should now see two new files generated in your current directory. You can take a look at them as they are small files. You can already guess what specie we are working with from the G26832_bracken_S.tsv file.\n\n\nProducing a more friendly species composition file\nImagine you have over 100 samples and have produced Bracken output files for each file using a simple bash script. Ho do you check each individual file to see what species abundance there is? This will be tedious right? How can we make things easier for us? For instance have them all in one .tsv file to examine at a go. Your guess is as good as mine. …Let’s write a script for that.\nTo do this, we can simply parse all the Kraken 2 and Bracken results into a more friendly .tsv file that summarises all the output into one file. We will achieve this with a simple python script.\n\nParse Kraken 2 and Bracken results files into tsv format\nWe will run a simple python script to achieve this purpose. In your working directory, you will find a file named kraken_parser.py. This is a simple python script that will do the job for us.\nWe can go ahead and execute that python script by running the command below:\npython kraken_parser.py\nYou should now see another file generated in your directory called Bracken_species_composition.tsv.\nNow let’s go ahead and have a look at the tsv file with the command below. Remember, we have run analysis on only one sample and so will expect only one line of results.\n\n\n\n\n\n\ncat Bracken_species_composition.tsv\nname    Mycobacterium tuberculosis  other\nG26832_bracken  99.58682581959188   0.41317418040812015\n\n\n\nAlternatively, you can open the tsv file with any appropriate GUI software (excel or libreoffice)\nNow, what if we want to also see all the QC statistics in one go provided we performed the analysis for multiple samples (or in our case even for just one sample).\nThankfully, bioinformaticians are not sleeping, there is one final tool (MultiQC) we will discuss here which puts all that analysis we have performed into a single report that is viewable on a web browser, yes an .html file.\n\n\n\nMultiQC\nclick here to view the publication on MultiQC or check out their website\nMultiQC is a tool to create a single report with interactive plots for multiple bioinformatics analyses across many samples.\nReports are generated by scanning given directories for recognised log files. These are parsed and a single HTML report is generated summarising the statistics for all logs found. MultiQC reports can describe multiple analysis steps and large numbers of samples within a single plot, and multiple analysis tools making it ideal for routine fast quality control.\n\n\n\n\n\n\nHelp\n\n\n\nDo this to get the help information for fastq-scan\nmultiqc -h\n\n\n\n\n\n\n\n\n\nUsage\n\n\n\nOnce installed, you can use MultiQC by navigating to your analysis directory (or a parent directory) and running the tool.\nThe general synthax is:\nmultiqc .\nYes, that’s it! MultiQC will scan the specified directory (. is the current dir) and produce a report detailing whatever it finds.\n\n\nYou can now go ahead and perform the MultiQC with the below command. We will only add the -f flag which overwrites any existing MultiQC reports.\n\n\n\n\n\n\nNote\n\n\n\nPerform the analysis with this command:\nmultiqc -f .\n\n\n\nThe report is created in multiqc_report.html by default. Tab-delimited data files are also created in a directory called multiqc_data/, containing extra information. These can be easily inspected using Excel.\nYou should now see two new files generated in your current directory.\nLet’s have a quick look the report by simply opening it in a web browser.\n\nWe should now know what each plot represent. If you don’t at this point, then you may want to start from the beginning of this document."
  },
  {
    "objectID": "materials/04-sequence_qc/4.1-sequence_qc.html#qc-bash-script-putting-it-all-together",
    "href": "materials/04-sequence_qc/4.1-sequence_qc.html#qc-bash-script-putting-it-all-together",
    "title": "4.1 Sequencing Quality Control",
    "section": "4.1.5 QC bash script: Putting it all together",
    "text": "4.1.5 QC bash script: Putting it all together\nNow let’s try this out! We will generate QC stats for three pairs of reads from Illumina paired-end sequencing. We will perform all the analysis above in one run while generating all the various files at each stage within the pipeline in tandem.\n\nRunning multiple samples with a simple bash script\nAll the fastq files needed for this analysis are located in the current working QC directory. You can have a look by simply running the below command to check out all the forward reads present in the current directory.\n\n\n\n\n\n\nls *R1.fastq.gz \nG26832_R1.fastq.gz  G33041_R1.fastq.gz ERX1027829_ERR948844_R1.fastq.gz\n\n\n\nLet’s have a look at the QC bash script we are going to run:\ncat qcloop.sh\nThe script contains several commands, some are combined together using pipes. (UNIX pipes is a very powerful and elegant concept which allows us to feed the output of one command into the next command and avoid writing intermediate files. If you are not comfortable with UNIX or how scripts look like and how they work, consider having a go at the UNIX tutorial or more specifically bonus shell script.\nNow run the script to create the QC stats (this may take a while):\n\n\n\n\n\n\nbash qcloop.sh\n###########################################\n#### loop bash script for running QC ######\n###########################################\n#               #\n#  ___   ____   #\n# / _ \\ / ___|  #\n# | | | | |     #\n# | |_| | |___  #\n# \\__\\_\\ \\____| #\n#               #\n#               #\n############################################\nfastq-scan_parser.py exists and will run in a second.\n <<<<<<<running fastq-scan_parser>>>>>>>\nTraceback (most recent call last):\n...\n\n\n\nThe script will produce all the intermediate and final files we have encountered in this session.\nPerform a simple ls command with the arguments -lhrt and view the most recent files created.\nls -lhrt\nWhich files do you consider most useful?\nNow, let’s have a look at the two most important files here — The multiqc_report.html and the Bracken_species_composition.tsv.\n\n\n\n\n\n\nExercise 4.1.5.1: MultiQC and Species Composition Report Interpretation\n\n\n\nHave a quick look at both MultiQC report and Bracken species composition files and attempt these questions\n\nHow many genomes were analysed?\nWhat is the average read length of each read?\nWhat is the average GC content of each genome?\nWill you pass all the reads as being good reads? Given a phred quality score threshold of 30.\nIdentify the reads that failed QC and report what could be wrong with the sequences.\nWhat are the most abundant species identified in each of the genomes?\nAre there any suspected contaminants in any of the genomes?\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n\nSorry, no solution.\nIf you are having any challenges just hold on, we will solve this together in class.\n\n\n\n\n\nIf you have successfully gone through the exercise above, congratulations!!! You are a bioinformatics QC expert now.\nNow, let’s copy the trimmed genome that we are most confident in to another directory called short_read_mapping. We will carry out our mapping and pseudogenome assembly of short reads from this genome in our next lesson.\n\n\n\n\n\n\nExercise 4.1.5.2: Copy to and view content from the short_read_mapping directory\n\n\n\nThe next lesson’s directory is called short_read_mapping. Go ahead and copy the two reads, G26832_R1.trim.fastq.gz and G26832_R2.trim.fastq.gz into the short_read_mapping directory. This directory is one level above your current directory. After copying the files to the short_read_mapping directory, verify if they were successfully copied.\nNB. Don’t leave your current working QC directory\n\n\n\n\n\n\nSolution:\n\n\n\n\n\nDo this to copy the files to the short_read_mapping directory\ncp G26832_R?.trim.fastq.gz ../short_read_mapping\nDo this to view the files from the short_read_mapping directory\nls ../short_read_mapping/G26832*\n\n\n\n\n\n\n\nDisk Usage II — Cleaning up after analysis\nNow that we are done investigating our genomic sequences, let’s pause again and check the space of our current working directory.\nYou can do this with the disk usage du command\ndu -h\nHow much disk space have you used since the start of the analysis? I’m sure it’s more than 4G. That’s fine. Let’s do some cleaning up. We will remove rm all files that we may not need for further analysis. Most of these files are intermediate files and can always be reproduced if we need to.\nYou will find this cleaning process very useful in the next chapter where we will generate tonnes and tonnes of data.\n\nremove all trimmed fastq files if not needed\nrm *trim*\n\n\nremove all classified and unclassified fastq files if not needed\nrm *classified*\n\n\nremove all Kraken 2 output if not needed\nrm *kraken2.out\n\n\n\n\n\n\n\nExercise 4.1.5.3: Advance Exercise\n\n\n\nIf you have gone through this material in good time and still want to try your hands on some more advanced stuff, you are free to do this exercise.\nTry modifying the qcloop script to be able to carry out analysis on a specific pair of fastq reads given that you have more than one pair of fastq in your working directory (assuming you want to perform QC again on only the one that passed our quality checks).\nYou can call the new script `qcloop_modified``\nYou should be able to call out the script to carry out the analysis on the specified fastq files with the below command\nbash qcloop_modified G26832_R1.fastq.gz G26832_R2.fastq.gz\n\n\n\n\n\n\nSolution:"
  },
  {
    "objectID": "materials/04-sequence_qc/4.1-sequence_qc.html#deactivate-qc-environment",
    "href": "materials/04-sequence_qc/4.1-sequence_qc.html#deactivate-qc-environment",
    "title": "4.1 Sequencing Quality Control",
    "section": "4.1.6 deactivate qc environment",
    "text": "4.1.6 deactivate qc environment\nNow that we are done with all our analysis, let’s deactivate the qc environment:\nmamba deactivate"
  },
  {
    "objectID": "materials/04-sequence_qc/4.1-sequence_qc.html#credit",
    "href": "materials/04-sequence_qc/4.1-sequence_qc.html#credit",
    "title": "4.1 Sequencing Quality Control",
    "section": "4.1.7 Credit",
    "text": "4.1.7 Credit\nInformation on this page has been adapted and modified from the following source(s):\n\nhttps://github.com/sanger-pathogens/QC-training\nhttps://github.com/rpetit3/fastq-scan\nhttps://www.bioinformatics.babraham.ac.uk/projects/fastqc/\nhttps://github.com/OpenGene/fastp\nhttps://github.com/DerrickWood/kraken2\nhttps://github.com/jenniferlu717/Bracken\nhttps://github.com/ewels/MultiQC"
  },
  {
    "objectID": "materials/05-short_read_mapping/5.1-short_read_mapping.html",
    "href": "materials/05-short_read_mapping/5.1-short_read_mapping.html",
    "title": "5.1 Short Read Mapping",
    "section": "",
    "text": "Teaching: 90 min || Exercises: 20 min"
  },
  {
    "objectID": "materials/05-short_read_mapping/5.1-short_read_mapping.html#overview",
    "href": "materials/05-short_read_mapping/5.1-short_read_mapping.html#overview",
    "title": "5.1 Short Read Mapping",
    "section": "Overview",
    "text": "Overview\n\n\n\n\n\n\n\n\n\n\n\n\nQuestions:\n\n\n\n\nWhat is mapping?\nCan I visualize how my paired end reads map to a reference?\nHow do I generate a vcf file and what information is contained within such a file?\nWhat are the steps involved in assembling Mycobacterium tuberculosis genome from next generation sequencing data?\nHow can I do reference-based assembly of Mycobacterium tuberculosis genomes?\n\n\n\n\n\n\n\n\n\nLearning Objectives:\n\n\n\n\nUnderstand what mapping is and why the need to map to a reference genome.\nTo introduce mapping and variant calling software, BWA, SAMtools, bcftools.\nTo show how sequence variation data such as SNPs and INDELs can be viewed in single and multiple BAM files, and VCF variant filtering using simple genome browsers.\nTo recognise what the main steps are in processing raw sequencing short read data to generate consensus genome sequences.\nTo show how short-read mapping can be executed with a simple script.\nTo understand the relevance of file organization and cleaning up after analysis.\n\n\n\n\n\n\n\n\n\nKey Points:\n\n\n\n\nMapping your sequences to a reference assembled genome helps you compare your genome to the reference and be able to tease out genetic variants that may have phenotypic consequences.\nThe main steps to generate MTB consensus sequences from a clean sequence date are:\n\nmap and sort reads to reference genome with tools like bwa and samtools\ncall variant/mutation with tools like bcftools\nproduce a consensus sequence\n\nArtemis and IGV are very useful tools to visualize SNPs and INDELs in mapped sequence reads and VCF files."
  },
  {
    "objectID": "materials/05-short_read_mapping/5.1-short_read_mapping.html#background",
    "href": "materials/05-short_read_mapping/5.1-short_read_mapping.html#background",
    "title": "5.1 Short Read Mapping",
    "section": "5.1.1 Background",
    "text": "5.1.1 Background\nOne of the main reasons why we will sequence the DNA of a particular organism is to be able to obtain a consensus genome from which further downstream analysis can be performed. Can you think of any analysis that you can use a consensus genome for? Okay, wait a minute, if you don’t know what a consensus genome is, we will explain it pretty soon. Basically, we will need to combine all the short reads we have generated from the Illumina sequencer into an assembled genome which we refer to as a consensus genome.\nAlthough different software/tools are used depending on which kind of sequencing platform was used to generate the sequences, the main goal is the same: to align the sequencing reads to a reference genome, and identify any DNA changes (SNPs or Indels) relative to the reference genome from which we can build a consensus genome based on the reference genomic position. This is called consensus assembly, since we are assembling the genome of our sample from the reads and generating a consensus sequence based on changes present in several reads covering a particular position of the genome.\n\n\n\nMapping: Consensus Assembly\n\n\nTo achieve a consensus genome, the general data processing steps are:\n\nMap the reads to a reference genome.\nPerform variant calling (SNPs and indels) to identify changes relative to the reference sequence.\nGenerate a consensus sequence for the sample based on those variants."
  },
  {
    "objectID": "materials/05-short_read_mapping/5.1-short_read_mapping.html#activate-the-mapping-environment",
    "href": "materials/05-short_read_mapping/5.1-short_read_mapping.html#activate-the-mapping-environment",
    "title": "5.1 Short Read Mapping",
    "section": "5.1.2 Activate the mapping environment",
    "text": "5.1.2 Activate the mapping environment\nNow navigate into the short_read_mapping_MTB/ directory and activate the mapping environment:\ncd ~/Desktop/workshop_files_Bact_Genomics_2023/05_mapping/short_read_mapping_MTB/\nmamba activate mapping\nHave a quick look at the directory\nls -al\nWe are now ready to perform some mapping and consensus assembly."
  },
  {
    "objectID": "materials/05-short_read_mapping/5.1-short_read_mapping.html#mapping-to-a-reference",
    "href": "materials/05-short_read_mapping/5.1-short_read_mapping.html#mapping-to-a-reference",
    "title": "5.1 Short Read Mapping",
    "section": "5.1.3 Mapping to a reference",
    "text": "5.1.3 Mapping to a reference\nA common task in processing sequencing reads is to align them to a reference genome, which is typically referred to as read mapping or read alignment. We will continue exemplifying how this works for Illumina data, however the principle is similar for Nanopore data (although the software used is often different, due to the higher error rates and longer reads typical of the platform).\nGenerally, these are the steps involved in read mapping:\n\nGenome Indexing | Because reference genomes can be quite long, most mapping algorithms require that the genome is pre-processed, which is called genome indexing. You can think of a genome index in a similar way to an index at the end of a textbook, which tells you in which pages of the book you can find certain keywords. Similarly, a genome index is used by mapping algorithms to quickly search through its sequence and find a good match with the reads it is trying to align against it. Each mapping software requires its own index, but we only have to generate the genome index once.\nRead mapping | This is the actual step of aligning the reads to a reference genome. There are different popular read mapping programs such as bowtie2 or bwa (for this workshop, we will use the bwa). The input to these programs includes the genome index (from the previous step) and the FASTQ file(s) with reads. The output is an alignment in a file format called SAM (text-based format - takes a lot of space) or BAM (compressed binary format - much smaller file size). You have already encountered these file formats in our file format session.\nBAM Sorting | The mapping programs output the sequencing reads in a random order (the order in which they were processed). But, for downstream analysis, it is good to sort the reads by their position in the genome, which makes it faster to process the file.\nBAM Indexing | This is similar to the genome indexing we mentioned above, but this time creating an index for the alignment file. This index is often required for downstream analysis and for visualising the alignment with programs such as the integrated genome viewer (IGV).\n\n\nDisk Usage I — Before analysis\nBefore we start performing any mapping analysis, let’s pause and check the space of our current working directory as we did for our previous lesson.\nYou can do this with the disk usage du command\ndu -h\n\n\nCurrent Disk Space In QC Directory\n\n~895MB\n\nNow, keep this value in mind, and this time, don’t forget it. We will come back to it at the end of this chapter.\n\n\nGenome Indexing with bwa\nBurrows-Wheeler Aligner (BWA) is a software package for mapping low-divergent sequences against a large reference genome, such as the human genome. It consists of three algorithms: BWA-backtrack, BWA-SW and BWA-MEM. The first algorithm is designed for Illumina sequence reads up to 100bp, while the rest two for longer sequences ranged from 70bp to 1Mbp. BWA-MEM and BWA-SW share similar features such as long-read support and split alignment, but BWA-MEM, which is the latest, is generally recommended for high-quality queries as it is faster and more accurate. BWA-MEM also has better performance than BWA-backtrack for 70-100bp Illumina reads.\n\n\nThere are three algorithms, which one should I choose?\n\nFor 70bp or longer Illumina, 454, Ion Torrent and Sanger reads, assembly contigs and BAC sequences, BWA-MEM is usually the preferred algorithm. For short sequences, BWA-backtrack may be better. BWA-SW may have better sensitivity when alignment gaps are frequent.\n\n\n\n\n\n\n\nHelp\n\n\n\nDo this to get the help information for bwa\nbwa\nProgram: bwa (alignment via Burrows-Wheeler transformation)\nVersion: 0.7.17-r1188\nContact: Heng Li <lh3@sanger.ac.uk>\n\nUsage:   bwa <command> [options]\n\nCommand: index         index sequences in the FASTA format\n         mem           BWA-MEM algorithm\n         fastmap       identify super-maximal exact matches\n         pemerge       merge overlapping paired ends (EXPERIMENTAL)\n         aln           gapped/ungapped alignment\n         samse         generate alignment (single ended)\n         sampe         generate alignment (paired ended)\n         bwasw         BWA-SW for long queries\n\n         shm           manage indices in shared memory\n         fa2pac        convert FASTA to PAC format\n         pac2bwt       generate BWT from PAC\n         pac2bwtgen    alternative algorithm for generating BWT\n         bwtupdate     update .bwt to the new format\n         bwt2sa        generate SA from BWT and Occ\n\nNote: To use BWA, you need to first index the genome with `bwa index'.\n      There are three alignment algorithms in BWA: `mem', `bwasw', and\n      `aln/samse/sampe'. If you are not sure which to use, try `bwa mem'\n      first. Please `man ./bwa.1' for the manual.\n\n\nAs we stated in our previous lesson, the starting material we will be using for our mapping is the fastq sequence that passed our QC checks. You may recall that this was identified as a Mycobacterium tuberculosis species. This species belongs to the Mycobacterium tuberculosis complex (MTBC). Consequently, we would have to align it to an appropriate reference within the same species. Fortunately for the world of TB, as with many other pathogens, there is a reference genome (MTB H37Rv) which was sequenced as far back in 1998 by Cole et. al 1998.\n\n\n\n\n\n\nUsage\n\n\n\nThe general format of the command is:\nbwa <command> [options]\n\n\nYou can now go ahead and perform reference indexing of the H37Rv using bwa with the below command.\n\n\n\n\n\n\nDo this to make the index file:\nbwa index MTB_H37Rv.fasta\n[bwa_index] Pack FASTA... 0.13 sec\n[bwa_index] Construct BWT for the packed sequence...\n[bwa_index] 1.14 seconds elapse.\n[bwa_index] Update BWT... 0.03 sec\n[bwa_index] Pack forward-only FASTA... 0.02 sec\n[bwa_index] Construct SA from BWT and Occ... 0.40 sec\n[main] Version: 0.7.17-r1188\n[main] CMD: bwa index MTB_H37Rv.fasta\n[main] Real time: 1.843 sec; CPU: 1.742 sec\n\n\n\nYou should now see five new files generated in your current directory.\nTo see if the files have been created, run this command:\n\n\n\n\n\n\nls -l MTB_H37Rv.fasta.*\n...\n--- 1.1M Jan 24 14:48 MTB_H37Rv.fasta.pac\n--- 4.3M Jan 24 14:48 MTB_H37Rv.fasta.bwt\n---   89 Jan 24 14:48 MTB_H37Rv.fasta.ann\n---   12 Jan 24 14:48 MTB_H37Rv.fasta.amb\n--- 2.2M Jan 24 14:48 MTB_H37Rv.fasta.sa\n\n\n\nTo view the statistics for the forward reads use this command:\ncat \nThat’s it – our reference index is now created\n\n\nRead mapping bwa\n\nUsage\nYou have already encountered the general format of the command from above. Here, we will use the mem algorithm.\nThe general command should look like this\nbwa mem [options]] index_file_name read_1.fastq.gz read_2.fastq.gz > aln.sam\nLet’s now go ahead and perform the mapping/alignment with bwa with the below command.\n\n\n\n\n\n\nDo this to perform the mapping/alignment:\n\n\n\nbwa mem -t 4 MTB_H37Rv.fasta G26832_R1.trim.fastq.gz G26832_R2.trim.fastq.gz > G26832.aln.sam\nM::bwa_idx_load_from_disk] read 0 ALT contigs\n[M::process] read 400806 sequences (40000169 bp)...\n[M::process] read 400548 sequences (40000024 bp)...\n[M::mem_pestat] # candidate unique pairs for (FF, FR, RF, RR): (146, 184906, 36, 147)\n[M::mem_pestat] analyzing insert size distribution for orientation FF...\n[M::mem_pestat] (25, 50, 75) percentile: (111, 222, 474)\n[M::mem_pestat] low and high boundaries for computing mean and std.dev: (1, 1200)\n[M::mem_pestat] mean and std.dev: (276.34, 242.38)\n[M::mem_pestat] low and high boundaries for proper pairs: (1, 1563)\n[M::mem_pestat] analyzing insert size distribution for orientation FR...\n[M::mem_pestat] (25, 50, 75) percentile: (179, 239, 304)\n\n\nYou should now see one new file generated in your current directory.\nG26832.aln.sam\nThis is a huge file and you don’t want to open it completely.\nIf you are eager to see what’s in it perform the command below:\n\n\n\n\n\n\npeep into .sam file\n\n\n\nhead -n 4 G26832.aln.sam\n@SQ     SN:NC_000962.3  LN:4411532\n@PG     ID:bwa  PN:bwa  VN:0.7.17-r1188 CL:bwa mem -t 4 MTB_H37Rv.fasta G26832_R1.trim.fastq.gz G26832_R2.trim.fastq.gz\nD00535:103:CD2W4ANXX:8:2209:1263:2022   99      NC_000962.3     1225671 60      101M    =       1225828 258     CGGGTTCTGGTCGTCGGAGAACGCCGACCACCCGGGTGCCGCGCTGGTCGGGAACGACAGCTTACCCGCACTGATCGAATCGCCGATGGGCTGCACACCGC       BCABBGGGGGGGGGGGGGGFGGGGGGDGGGEGDGFGGGGGGGGGGDGGGGGDGGGGDGGGGEGGGGGGGGGGE=EGGGGGGGGGGGGGGGGGGGGGGECC@       NM:i:0  MD:Z:101        MC:Z:101M       AS:i:101    XS:i:0\nD00535:103:CD2W4ANXX:8:2209:1263:2022   147     NC_000962.3     1225828 60      101M    =       1225671 -258    GGAGCACTCGTCGCCGGAGAGGTTGCCGTGGTCGACTTGTTGTCGCCGCGGAGGCCGATCACCAGGATCACCACCAGTAGGATGACACCCAGCACCGCGAG       EGGGD6GCGDDADGGGGGGGGGDGDGGGGGGGGGGEEGGGGDGGDGGGGGG<G>/=GF@FFGG@CGG@BFBGGGGGGEGCGGGGGGGGGGGFGGGGB@BBB       NM:i:0  MD:Z:101        MC:Z:101M       AS:i:101    XS:i:0\n\n\n\n\n\nBAM Sorting with samtools sort\nSamtools is a set of utilities that manipulate alignments in the SAM (Sequence Alignment/Map), BAM, and CRAM formats. It converts between the formats, does sorting, merging and indexing, and can retrieve reads in any regions swiftly.\nEach command has its own man page which can be viewed using e.g. man samtools-view or with a recent GNU man using man samtools view. You can also just use samtools view --help to call out the specific help function for a samtools command. For the view command, options exist to change the output format from SAM to BAM or CRAM, so this command also acts as a file format conversion utility.\n\n\n\n\n\n\nHelp\n\n\n\nDo this to get the help information for samtools\nsamtools --help\nProgram: samtools (Tools for alignments in the SAM format)\nVersion: 1.15 (using htslib 1.14)\n\nUsage:   samtools <command> [options]\n\nCommands:\n  -- Indexing\n     dict           create a sequence dictionary file\n     faidx          index/extract FASTA\n     fqidx          index/extract FASTQ\n     index          index alignment\n\n  -- Editing\n     calmd          recalculate MD/NM tags and '=' bases\n     fixmate        fix mate information\n     reheader       replace BAM header\n     targetcut      cut fosmid regions (for fosmid pool only)\n     addreplacerg   adds or replaces RG tags\n     markdup        mark duplicates\n     ampliconclip   clip oligos from the end of reads\n...\n\n\n\n\n\n\n\n\nUsage\n\n\n\nThe general format of the command is:\nsamtools <command> [options]\n\n\nBefore we proceed to sorting our .sam file, let’s convert it to a .bam file using the command view. This creates a much more smaller binary file which is easier to read by downstream analysis tools.\n\n\n\n\n\n\nWe do this by running the following command:\n\n\n\nsamtools view -F 4 --threads 4 -bhS G26832.aln.sam > G26832.aln.bam\nDon’t expect a print out – the .bam file just get created in the background Perform a simple ls to confirm the new .bam file created.\nThe optional arguments we have used in the above command have the following meaning:\n\n\n\n\n\n\n\noption\ninterpretation\n\n\n\n\n-F\nUsed for filtering out (excluding) flagged reads in the alignment. Here. We are excluding unmapped reads\n\n\n--threads\nSpecifies the number of additional threads to use. 4 used in this case\n\n\n-b\nSpecifies the output format in .bam\n\n\n-h\nInclude header in SAM output\n\n\n-S\nSpecifies that the input file is in .sam format\n\n\n>\nDirect the standard output to a relevant file name. You may also use -o in place of > to specify the output file name\n\n\n\n\n\n\n\n\n\n\n\nCombining the last two commands\n\n\n\nThe .sam file we created with bwa is often not really needed and most pipelines instantly convert it to a .bam file. This helps save a lot of disk space.\nWe can combine the two commands by simply using the pipe |. Do you remember its use from our previous lesson? We will see a lot of this in our scripts later.\nYou don’t need to run the below command, but if you wanted to perform both analysis at a go you would run this command:\nbwa mem -t 4 MTB_H37Rv.fasta G26832_R1.trim.fastq.gz G26832_R2.trim.fastq.gz | samtools view -F 4 --threads 4 -bhS -o G26832.aln.bam\n\n\nThe general synthax for the samtools sort command is:\nsamtools sort [-l level] [-u] [-m maxMem] [-o out.bam] [-O format] [-M] [-K kmerLen]\n [-n] [-t tag] [-T tmpprefix] [-@ threads] [in.sam|in.bam|in.cram]\nYou can now go ahead and perform the BAM sorting with samtools using the below command.\n\n\n\n\n\n\n\n\n\n\nDo this to run the forward reads:\nsamtools sort -@ 4 -o G26832.sorted.aln.bam -T G26832 G26832.aln.bam\n[bam_sort_core] merging from 0 files and 4 in-memory blocks...\n\n\nOur sorted aligned file is now ready.\n\n\nBAM Indexing with samtools index\nWe will use another command from samtools called the samtools index to index our .sorted.aln.bam file.\nThis index coordinate-sorted BAM files for fast random access.\n\n\n\n\n\n\nUsage\n\n\n\nThe general format of the command follows the same as listed in previous commands\n\n\nYou can now go ahead and perform the indexing of the .sorted.aln.bam file with the below command.\n\n\n\n\n\n\nNB.\n\n-b create a BAI index. This is currently the default when no format options are used.\nThe BAI index format can handle individual chromosomes up to 512 Mbp (2^29 bases) in length. If your input file might contain reads mapped to positions greater than that, you will need to use a CSI index\nWhen no output filename is specified, for a BAM file sorted.aln.bam, either sorted.aln.bam.bai or sorted.aln.bam.csi will be created, depending on the index format selected.\n\nDo this to perform the indexing:\nsamtools index -@ 4 G26832.sorted.aln.bam\nDon’t expect a print out in the terminal. Our indexed file should be created in the current directory\nG26832.sorted.aln.bam.bai\n\n\n\n\n\n\n\n\n\nExercise 5.1.3.1: Obtain some mapping statistics with samtools\n\n\n\nSo far, we have explored three commands from the samtools – samtools view, samtools sort and samtools index. Check out the various commands of samtools using help or man and identify any command that can allow you to generate some mapping statistics.\nGo ahead and perform the analysis and output your results to G26832.mapstats.txt.\nThis file can also be used to produce a quality control report.\nHave a look at the first 47 lines. This initial section contains a summary of the alignment and includes some general statistics. In particular, you can see how many bases mapped, and how much of the genome that was covered.\nNow look at the output and try to answer the questions below.\n\nWhat is the total number of reads?\nWhat proportion of the reads were mapped?\nHow many pairs were mapped to a different chromosome?\nWhat is the insert size mean and standard deviation?\nHow many reads were paired properly?\n\nFinally, look out for the samtools command to create some QC plots from the output of the stats command.\nNB. for all the statistics commands, the BAM index file needs to be present in the same directory.\n\n\n\n\n\n\nSolution:\n\n\n\n\n\nRun the below command to perform the analysis:\nsamtools stats --threads 4 G26832.sorted.aln.bam > G26832.mapstats.txt\nView the first 47 lines:\nhead -n 47 G26832.mapstats.txt\nThis file contains a number of useful stats that we can use to get a better picture of our data, and it can even be plotted with plot-bamstats.\nFirst let’s have a closer look at some of the different stats. Each part of the file starts with a # followed by a description of the section and how to extract it from the file. Let’s have a look at all the sections in the file:\ngrep ^'#' G26832.mapstats.txt | grep 'Use'\nPlot the bamstats:\nplot-bamstats -p data/lane1-plots/ data/lane1.sorted.bam.bchk\n\n\n\n\n\n\n\nOther samtools commands to explore\nIn addition to the stats command, you may want to explore the flagstat and ìdxstats` commands below at you free time.\n\nsamtools flagstat\nsamtools flagstat – counts the number of alignments in a BAN file for each FLAG type. The commands does a full pass through the input file to calculate and print statistics to stdout. It provides counts for each of 13 categories based primarily on bit flags in the FLAG field. Information on the meaning of the flags is given in the SAM specification document.\nEach category in the output is broken down into QC pass and QC fail. In the default output format, these are presented as “#PASS + #FAIL” followed by a description of the category.\nThe first row of output gives the total number of reads that are QC pass and fail (according to flag bit 0x200). For example:\n4131522 + 2 in total (QC-passed reads + QC-failed reads)\nWhich would indicate that there are a total of 4131524 reads in the input file, 4131522 of which are marked as QC pass and 2 of which are marked as “not passing quality controls”\nClick here for more description on understanding the output of the command.\nYou can go ahead and try out the command and have a look at its output.\n\n\n\n\n\n\n\n\n\n\nsamtools flagstat --threads 4 G26832.sorted.aln.bam > G26832.flagstat.txt\n\n\n\n\nsamtools idxstats\nsamtools idxstats – reports alignment summary statistics The command retrieves and print stats in the index file corresponding to the input file.\nThe output is TAB-delimited with each line consisting of reference sequence name, sequence length, # mapped read-segments and # unmapped read-segments. It is written to stdout. Note this may count reads multiple times if they are mapped more than once or in multiple fragments.\n\n\n\n\n\n\nsamtools idxstats G26832.sorted.aln.bam > G26832.idxstats.tsv"
  },
  {
    "objectID": "materials/05-short_read_mapping/5.1-short_read_mapping.html#genome-visualization",
    "href": "materials/05-short_read_mapping/5.1-short_read_mapping.html#genome-visualization",
    "title": "5.1 Short Read Mapping",
    "section": "5.1.4 Genome Visualization",
    "text": "5.1.4 Genome Visualization\nYou can think of some benefits derived from manually visualizing genomes – Compared to cost of writing, debugging and running computational scripts, visual inspections may be low cost and may also have high performance. For instance what you physically see really makes sense (seeing patterns for instance) and you have the chance of also identifying issues and outliers even though laborious (debugging).\nThere are over 40 genome browsers – some works only online, but there are others you can get a local copy. Which one you choose depends on a number of factors: - task at hand - kind of data - size of data - confidentiality of data (data privacy)\nWe will not go deep into the tools used for browsing genomes, however, we will attempt to give in-depth introduction into a couple of these tools.\n\nIntegrative Genomics Viewer (IGV)\nThe Integrative Genomics Viewer (IGV) is a high-performance, easy-to-use, interactive tool for the visual exploration of genomic data. It supports flexible integration of all the common types of genomic data and metadata, investigator-generated or publicly available, loaded from local or cloud sources.\nDatasets that can be analysed with IGV range from epigenomics, microarrays, NGS alignments, RNA-Seq to mRNA-, CNV- Seq.\nYou can click here to Download the IGV software and also check out their Publication.\nAligned reads from sequencing can be loaded into IGV in the BAM format, SAM format, or CRAM format. IGV requires that BAM and CRAM files have an associated index file.\nThe main data file must include the .bam or .cram extension. The index file should have the same filename but with the .bai or .crai extension. For example, the index file for G26832.sorted.aln.bam would be named G26832.sorted.aln.bam.bai. When loading by URL, the URL to both the data file and the index file should be specified. When loading by file, IGV automatically searches for the index file within the same directory as the data file.\n\n\n\n\n\n\nUsing IGV: The basics\n\n\n\nAfter installation of the software, the basic things to do to get you going are: - Launch the software - Select a reference genome - Load your data - Load any extra annotation data relative to your reference genome that you may need - Navigate through the data\n\n\n\nPrimary files to load\n\n\n\n\n\nZoom in to about 30kb to view the aligned reads\n\n\nNB. For this workshop, we will focus on analysing our genomic data to look out for variants - SNPs and INDELs.\n\n\n\n\n\n\n\n\nExercise 5.1.4.1: Open IGV and load data\n\n\n\nFollow the steps outlined above and load your data into IGV. The data to load can be found in the genome_browser directory /Desktop/workshop_files_Bact_Genomics_2023/05_mapping/genome_browser/.\nYou should have IGV already installed on your machines.\n\n\n\n\n\n\nSolution:\n\n\n\n\n\nThe solution to this exercise is your success in loading your data into IGV.\n\nOpen IGV and go to Genome → Load Genome from File….\nIn the file browser that opens go to the folder /Desktop/workshop_files_Bact_Genomics_2023/05_mapping/genome_browser/ and select the file MTB_H37Rv.fasta to open it. This is the reference file with no annotation.\nNext, go to File → Load from File….\nIn the file browser that opens go to the folder /Desktop/workshop_files_Bact_Genomics_2023/05_mapping/genome_browser/ and select the file G26832.sorted.aln.bam to open it.\nGo back to File → Load from file… and this time load the gff file MTB_H37Rv_annotation.gb containing the annotations.\n\n\n\n\n\n\nThere are a huge variety of features you can explore in IGV. \nClick to watch the YouTube video below for a brief introduction to IGV – Sequencing Data Basics.\n\n\n\n\nClick to watch the YouTube video below for a brief introduction on SNP analysis in IGV.\n\n\n\n\n\nIdentifying SNPs | Homozygous, Heterozygous, Strand Bias\nImportant metrics for evaluating the validity of SNPs are: - Coverage - Amount of support - Strand bias / PCR artifacts - Mapping quality scores - Base quality scores\n\n\n\nIdentifying Single Nucleotide Polymorphisms (SNPs)\n\n\nIdentifying Homozygous SNPs At position 2,259,657 the nucleotide A from the reference sequence has mutated to a C within our sample genome. This represents a single nucleotide polymorphism (SNP). If you scroll through, you see that all the 109 reads covering that position called the nucleotide C implying this is a true positive SNP and also called an homozygous SNP.\n\n\n\nCloser look into SNPs\n\n\nIdentifying Heterozygous SNPs In other SNP situations, you will find that only about half the number of reads covering that SNP site called a different nucleotide. Such SNPs are called heterozygous SNPs. This arise because there may be more than one population (sub-population) of bacteria within the sample. You can see a lot of this within the region 2,266,478 to 2,266,628. At position 2,266,598 you that out of the 205 reads, exactly half mapped to the nucleotide C and the remaining half mapped to the nucleotide G.\nDetecting Strand Bias In some situations, due to sequencing errors, a false heterozygous SNP can be called. One way to detect false heterozygous SNPs is by looking at the number of forward and reverse reads that called that SNP. If you find that all the SNPs are found on only the forward reads and not reverse reads (or vice versa), then it probably was due to sequencing error. This type of error is called strand bias.\n\n\nIdentifying Insertions and Deletions (InDels)\n\n\n\nIdentifying INDELs\n\n\nIdentifying insertions At position 1894300 the nucleotide GG from the reference sequence has mutated to GGTCTTGCCGCG within our sample genome. This represents an insertion of 10 nucleotides. If you scroll through your genomic data, you see that more than 80% of the aligned reads have this mutation. Can you identify other insertions?\nIdentifying deletions At position 330711 the nucleotide GCACCATGAACCATCTGGCGT from the reference sequence has mutated to G within our sample genome. This represents a deletion of 20 nucleotides at that site. If you scroll through, you see that all the 68 aligned reads at that position have this mutation. Can you identify other deletions?\nNote that, these INDELS we have detected, may or may not have phenotypic consequences depending on the location.\n\n\n\n\n\n\nVCF files can also be loaded into IGV.\n\n\n\nWe have not yet examined a VCF file. But you should already know how the file format looks like and what it likely contains. We will look into it more detailed in a matter of minutes. For now, click to watch the YouTube video below for a brief introduction on what VCF files are and how to browse VCF files in IGV.\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 5.1.4.2: Informative INDELS\n\n\n\nNow you know what INDELS look like. Follow the steps above and load your sample .vcf and perform the following task. 1. Identify 2 informative insertions 2. Identify 2 informative deletions\n\n\nHint\n\nLook for INDELS located within annotated genes\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n\n\n\n\n\n\n\nIf you have the time, watch the YouTube video below for some more detailed explanation into the use of IGV.\n\n\n\n\nFollow this link to explore more on IGV.\n\n\n\nArtemis\nThe Artemis Software is a set of software tools for genome browsing and annotation. Several types/sets of information can be viewed simultaneously within different contexts. For example, Artemis gives you the two views of the same genome region, so you can zoom in to inspect detailed DNA sequence motifs, and also zoom out to view local gene architecture (e.g. operons), or even an entire chromosome or genome, all within one screen. It is also possible to perform analyses within Artemis and save the output for future reference.\nThe Artemis Software includes: - Artemis - ACT - BamView - DNAPlotter\nArtemis is a free genome browser and annotation tool that allows visualisation of sequence features, next generation data and the results of analyses within the context of the sequence, and also its six-frame translation. Artemis is written in Java, and is available for UNIX, Macintosh and Windows systems. It can read EMBL and GENBANK database entries or sequence in FASTA, indexed FASTA or raw format. Other sequence features can be in EMBL, GENBANK or GFF format.\nACT is a free tool for displaying pairwise comparisons between two or more DNA sequences. It can be used to identify and analyse regions of similarity and difference between genomes and to explore conservation of synteny, in the context of the entire sequences and their annotation.\nDNAPlotter generates images of circular and linear DNA maps.\nBamView is a standalone BAM/CRAM file viewer.\n\nStarting up the Artemis software\nDouble click the Artemis icon on the desktop. Alternatively, you can type art in the terminal and hit Enter to open artemis. A small start-up window will appear (see below). The directory /Desktop/workshop_files_Bact_Genomics_2023/05_mapping/genome_browser/ all files you will need for this module. Now follow the sequence of numbers to load up the MTB_H37Rv.fasta reference sequence.\n\n\n\nStarting up the Artemis software\n\n\n\n\nLoading an annotation file (entry) into Artemis\nFollow the numbers to load the annotation file MTB_H37Rv_annotation.gb.\n\n\n\nLoading an annotation file (entry) into Artemis\n\n\n\n\nThe basics of Artemis\nLet’s take a closer look at what Artemis offers us:\n\nDrop-down menus: There are lots in there so don`t worry about all the details right now.\nEntry (top line): shows which entries are currently loaded with the default entry highlighted in yellow (this is the entry into which newly created features are created). Selected feature: the details of a selected feature are shown here; in this case gene STY0004 (yellow box surrounded by thick black line).\nThis is the main sequence view panel. The central 2 grey lines represent the forward (top) and reverse (bottom) DNA strands. Above and below those are the 3 forward and 3 reverse reading frames. Stop codons are marked on the reading frames as black vertical bars. Genes and other annotated features (eg. Pfam and Prosite matches) are displayed as coloured boxes. We often refer to predicted genes as coding sequences or CDSs.\nThis panel has a similar layout to the main panel but is zoomed in to show nucleotides and amino acids. Double click on a CDS in the main view to see the zoomed view of the start of that CDS. Note that both this and the main panel can be scrolled left and right (7, below) zoomed in and out (6, below).\nFeature panel: This panel contains details of the various features, listed in the order that they occur on the DNA. Any selected features are highlighted. The list can be scrolled (8, below).\nSliders for zooming view panels.\nSliders for scrolling along the DNA.\nSlider for scrolling feature list.\n\n\n\n\nThe basics of Artemis\n\n\n\n\nGetting around in Artemis\nThere are three main ways of getting to a particular DNA region in Artemis:\n-the Goto drop-down menu\n-the Navigator and\n-the Feature Selector\nThe best method depends on what you are trying to do. Knowing which one to use comes with practice.\nThe functions on the ‘Goto’ menu (below the Navigator option) are shortcuts for getting to locations within a selected feature or for jumping to the start or end of the DNA sequence. This is really intuitive. You can give it a try. The Navigator panel is also within the Goto menu and fairly intuitive so open it up and give it a try.\n\n\n\nGetting around in Artemis\n\n\nIt may seem that Goto Start of Selection and Goto Feature Start do the same thing. Well they do if you have a feature selected but Goto Start of Selection will also work for a region which you have selected by click-dragging in the main window. So yes, give it a try!\n\n\n\n\n\n\nSuggested tasks I:\n\n\n\n\nZoom out, select / highlight a large region of sequence by clicking the left hand button and dragging the cursor then go to the start and end of this selected region.\nSelect a CDS then go to the start and end.\nGo to the start and end of the genome sequence.\nSelect a CDS. Within it, go to a base (nucleotide) and/or amino acid of your choice.\nHighlight a region then, from the right click menu, select Zoom to Selection.\n\n\n\n\n\n\n\n\n\nSuggested tasks II:\n\n\n\nBelow are a few more suggested places to try your hands on.\n\nThink of a number between 1 and 4411532 and go to that base (notice how the cursors on the horizontal sliders move with you).\nYour favourite gene name (it may not be there so you could try rpob).\nUse Goto Feature With This Qualifier value to search the contents of all qualifiers for a particular term. For example using the word pseudogene will take you to the next feature with the word pseudogene in any of its qualifiers. Note how repeated clicking of the Goto button takes you to the following pseudogene in the order that they occur on the chromosome.\ntRNA genes. Type tRNA in the Goto Feature With This Key.\nRegulator-binding DNA consensus sequence (real or made up!). Note that degenerate base values can be used.\nAmino acid consensus sequences (real or made up!). You can use Xs Note that it searches all six reading frames regardless of whether the amino acids are encoded or not. Try searching for the protein SSTYI\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nFeature Keys and Qualifiers – a brief explanation of what they are and a sample of the ones we use.\n\n\n\n\n\n\nFeature Keys:\n\n\n\nThey describe features with DNA coordinates and once marked, they all appear in the Artemis main window. The ones we use are:\n\nCDS: Marks the extent of the coding sequence.\nRBS: Ribosomal binding site\nmisc_feature: Miscellaneous feature in the DNA\nrRNA: Ribosomal RNA\nrepeat_region\nrepeat_unit\nstem_loop\ntRNA: Transfer RNA\n\n\n\n\n\n\n\n\n\nQualifiers:\n\n\n\nThey describe features in relation to their coordinates.\nOnce marked they appear in the lower part of the Artemis window. They describe the feature whose coordinates appear in the location part of the editing window. The ones commonly use for annotation at the Sanger Institute are:\n-class: Classification scheme used in-house -colour: Also used in-house in order to differentiate between different types of genes and other features. -gene: Descriptive gene a name, eg. ilvE, argA etc. -label: Allows you to label a gene/feature in the main view panel. -note: This qualifier allows for the inclusion of free text. This could be a description of the evidence supporting the functional prediction or other notable features/information which cannot be described using other qualifiers. -product: The assigned possible function for the protein goes here. -pseudo: Matches in different frames to consecutive segments of the same protein in the databases can be linked or joined as one and edited in one window. They are marked as pseudogenes. They are normally not functional and are considered to have been mutated. -locus_tag : Systematic gene number, eg SAS1670, Sty2412 etc.\n\n\nThe list of keys and qualifiers accepted by EMBL in sequence/annotation submission files are listed at the following web page: (http://www3.ebi.ac.uk/Services/WebFeat/)\n\n\n\n\nViewing Plots and Graphs\nFeature plots can be displayed by selecting a CDS feature then clicking View and Feature Plots. The window which appears shows plots predicting hydrophobicity, hydrophilicity and coiled-coil regions for the protein product of the selected CDS.\nIn addition to looking at the fine detail of the annotated features it is also possible to look at the characteristics of the DNA covering the region displayed. This can be done by adding various plots to the display, showing different characteristics of the DNA. Some of the plots can be used to look at the protein coding potential of translation frames within the DNA, and others can be used to search for horizontally acquired DNA (such as GC frame plot).\nTo view the graphs: Click on the Graph menu to see all those available and then tick the box for GC Content (%). To adjust the smoothing of the graph you change the window size over which the points on the graph are calculated, using the slider shown below.\n\n\n\nViewing Plots and Graphs\n\n\n\n\nViewing and exploring bam files in Artemis\nLike we did with IGV, it is also very possible and maybe much intuitive to view bam files in Artemis. Let’s go ahead and load in our G26832.sorted.aln.bam file by following the instructions below.\n\n\n\n\n\n\nMay require more time to load\n\n\n\nPlease make sure you do not go to a zoomed-out view of Artemis, but stay at this level, as display of BAM files does take time to load!\n\n\n\n\n\nLoading bam files in Artemis\n\n\nYou should see the BAM window appear as in the screen shot below. In the top panel of the window each little horizontal line represents a sequencing read. Notice that all the reads are blue which indicates that these are unique reads, whereas we would have seen alternating green reads representing “duplicated” reads that have been mapped to exactly the same position on the reference sequence. Remember we have removed duplicates during our trimming hence don’t expect to see it them here. (To save space, if there are duplicated reads only one is shown, which means that there could be a large number of duplicated reads at a given position but the software only depicts one).\nIf you click a read its mate pair will also be selected. Also note that if the cursor hovers over a read for long enough details of that read will appear in a small box. You can right-click in the bam view (the panel showing the coloured sequence reads) to see more options and explore them. Note that, if selected reads fall in a region of interest, being able to access some information easily can be really helpful.\n\n\n\nViewing and exploring bam files in Artemis\n\n\nThere are a lot of quality checks you can perform with a loaded bam file especially if it has not been cleaned. You can explore these quality features with a raw (unfiltered/untrimmed) bam file. You can actually use several details relating to the mapping of a read to filter the reads from the BAM file that are shown in the window. To do this, right-click again over the stack plot window showing the reads and select Filter Reads…. A window will appear with many options for filtering.\n\n\nViewing and calling out SNPs\nTo view SNPs use your right mouse button to click in the BAM view window. Then in the popup menu click on Show and check the SNP marks box. SNPs in your data in comparison to the reference sequence are shown as red marks on the individual reads. In other words, the red marks appear on the stacked reads highlighting every base in a read that does not match the reference. When you zoom in you can see some SNPs that are present in all reads and appear as vertical red lines, whereas other SNPs are more sporadically distributed. The former are more likely to be true SNPs whereas the latter may be sequencing errors, although this would not always be true.\n\n\n\nViewing SNPs in Artemis\n\n\nClearly there are many more features of Artemis which we will not have time to explain in detail. If you still have time, before getting on with the next section it might be worth browsing the menus. Hopefully you will find most of them easy to understand. You can always go back to this after the when you have enough time to explore. All the lessons we learnt in exploring the BAM file in IGV can also be explored in Artemis.\n\n\n\n\n\n\nExercise 5.1.4.3: Viewing multiple BAM files in Artemis\n\n\n\nYou are not limited to viewing only one BAM file in Artemis, you can also view multiple BAM files at the same time. Remember that a BAM file is a processed set of aligned reads from (in this case) one bacterium aligned against a reference sequence. So in principle we can view multiple different bacterial isolates mapped against the same reference concurrently.\nYour task is to read in another BAM file generated from our mapping script and compare the two files.\nDo you see any striking differences? Make a short comparative report on what you see and present it to the class.\n\n\n\n\n\n\nSolution:\n\n\n\n\n\n\nRight click in the BAM view window, select Add BAM/CRAM ... and load in the second bam file.\n\n\n\n\n\n\n\nNote\n\n\n\nIn the first instance Artemis reads all the new reads into the same window. This is useful if you have multiple sequencing runs for the same sample. But in this instance you may want to split the reads into separate windows so that we can view them independently. This is done in the next step.\n\n\n\nFirst, clone the BAM view window. Right-click over the BAM window and select ‘Clone window’.\n\n\n\n\n\n\n\nNote\n\n\n\nIf you right-click over the top BAM window and select BAM files you can individually select the files as desired. This means you can display each BAM file in its own window by de-selecting one or the other file.\n\n\n\nNow compare both sequences and produce your report.\n\nAll participants will be given 3 minutes each and expected to present their findings in class.\n\n\n\n\n\n\n\n\n\n\n\nExercise 5.1.4.4: Repeat IGV and Artemis exploration on Staphyloccocus aureus genome\n\n\n\nYou should be familiar with navigating your way through the IGV and Artemis genome browsers. You can use any preferred genome browser for this exercise.\nTry your hands on this using the Staphylococcus aureus genome present in the same genome_browser directory.\n\nWhat is the full length of the Staphylococcus aureus genome?\nYou saw earlier on the rpob gene in the MTB H37Rv geneome. Do you think this gene is also present in the Staphylococcus aureus genome? Go ahead and look for it and write down its exact position in the genome. How does it compare with that in the MTB H37Rv genome?\nBlast the rpob gene in NCBI and explore the first 1000 hits. List the number of species that share closely related rpob sequences with that of Staphylococcus aureus rpob?\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n\nWe will go through the solution in class."
  },
  {
    "objectID": "materials/05-short_read_mapping/5.1-short_read_mapping.html#variant-calling",
    "href": "materials/05-short_read_mapping/5.1-short_read_mapping.html#variant-calling",
    "title": "5.1 Short Read Mapping",
    "section": "5.1.5 Variant calling",
    "text": "5.1.5 Variant calling\nAs you saw from the genome visualization tools, it may be tedious to generate a list of variants if you are working with several BAM files. Thankfully, there are command line tools that helps us achieve this purpose and even perform more advanced analysis on our BAM files, which have been developed by the same samtools development team. It isn’t called samtools this time, but rather bcftools, I guess the name originated from the output file format – .bcf.\nbcftools is a set of utilities for variant calling and that also manipulate variant calls in the Variant Call Format (VCF) and its binary counterpart BCF. All commands work transparently with both VCFs and BCFs, both uncompressed and BGZF-compressed.\nBCFtools is designed to work on a stream. It regards an input file “-” as the standard input (stdin) and outputs to the standard output (stdout). Several commands can thus be combined with Unix pipes\nYou can check out their publications and their manual page to get more information.\n\n\n\n\n\n\nHelp\n\n\n\nDo this to get the help information for bcftools\nbcftools --help\nProgram: bcftools (Tools for variant calling and manipulating VCFs and BCFs)\nLicense: GNU GPLv3+, due to use of the GNU Scientific Library\nVersion: 1.14 (using htslib 1.14)\n\nUsage:   bcftools [--version|--version-only] [--help] <command> <argument>\n\nCommands:\n\n -- Indexing\n    index        index VCF/BCF files\n\n -- VCF/BCF manipulation\n    annotate     annotate and edit VCF/BCF files\n    concat       concatenate VCF/BCF files from the same set of samples\n    convert      convert VCF/BCF files to different formats and back\n    isec         intersections of VCF/BCF files\n    merge        merge VCF/BCF files files from non-overlapping sample sets\n    norm         left-align and normalize indels\n    plugin       user-defined plugins\n    query        transform VCF/BCF into user-defined formats\n    reheader     modify VCF/BCF header, change sample names\n    sort         sort VCF/BCF file\n    view         VCF/BCF conversion, view, subset and filter VCF/BCF files\n\n -- VCF/BCF analysis\n    call         SNP/indel calling\n    consensus    create consensus sequence by applying VCF variants\n    cnv          HMM CNV calling\n    csq          call variation consequences\n    filter       filter VCF/BCF files using fixed thresholds\n    gtcheck      check sample concordance, detect sample swaps and contamination\n    mpileup      multi-way pileup producing genotype likelihoods\n    polysomy     detect number of chromosomal copies\n    roh          identify runs of autozygosity (HMM)\n    stats        produce VCF/BCF stats\n\n Most commands accept VCF, bgzipped VCF, and BCF with the file type detected\n automatically even when streaming from a pipe. Indexed VCF and BCF will work\n in all situations. Un-indexed VCF and BCF and streams will work in most but\n not all situations.\n\n\n\n\n\n\n\n\nUsage\n\n\n\nThe general format of running commands with the bcftool is:\nbcftools [COMMAND] [OPTIONS]\n\n\nFor this workshop, we will explore only four of the commands. - bcftools mpileup - bcftools call - bcftools view - bcftools filter\n\nbcftools: mpileup, call and view\nbcftools mpileup – Generates VCF or BCF containing genotype likelihoods at each genomic position with coverage for one or multiple alignment (BAM or CRAM) files.\nbcftools call – SNP/indel variant calling from VCF/BCF. To be used in conjunction with bcftools mpileup. This command replaces the former bcftools view caller.\nbcftools view – subset and filter VCF or BCF files by position and filtering expression. Convert between VCF and BCF.\n\n\n\n\n\n\nHelp\n\n\n\nFor specific help on any of the above tools use these commands:\nDo this to get the help information for bcftools mpileup\nbcftools mpileup\nDo this to get the help information for bcftools call\nbcftools call\nDo this to get the help information for bcftools view\nbcftools view\n\n\n\n\n\n\n\n\nUsage\n\n\n\nThe general format of the commands are:\nfor bcftools mpileup:\nbcftools mpileup [OPTIONS] -f ref.fa in.bam [in2.bam […​]]\nfor bcftools call:\nbcftools call [OPTIONS] FILE\nfor bcftools view:\nbcftools view [OPTIONS] file.vcf.gz [REGION […​]]\n\n\nI am sure we are now very much aware and happy with the use of the pipe |. We will combine all three commands with | in order to speed up things.\n\n\n\n\n\n\nWe do this by running the following command:\n\n\n\nbcftools mpileup --fasta-ref MTB_H37Rv.fasta --min-BQ 20 --annotate FORMAT/AD,FORMAT/ADF,FORMAT/ADR,FORMAT/DP,FORMAT/SP,INFO/AD,INFO/ADF,INFO/ADR G26832.sorted.aln.bam | bcftools call --output-type v --ploidy 1 --multiallelic-caller | bcftools view --output-file G26832.vcf.gz --output-type z\n[mpileup] 1 samples in 1 input files\n[mpileup] maximum number of reads per input file set to -d 250\n\n\nLet’s briefly define each of the options we have used in the above command\n\n\n\n\n\n\n\n\n\n\nInput option\nInput required\nDescription\n\n\n\n\n–fasta-ref\nFILE\nThe faidx-indexed reference file in the FASTA format. The file can be optionally compressed by bgzip. Reference is required by default unless the –no-reference option is set [null]\n\n\n–min-BQ\nINT\nCaps the base quality to a maximum value [60]. This can be particularly useful on technologies that produce overly optimistic high qualities, leading to too many false positives or incorrect genotype assignments.\n\n\n–annotate\nLIST\nComma-separated list of FORMAT and INFO tags to output. (case-insensitive, the “FORMAT/” prefix is optional, and use “?” to list available annotations on the command line) [null]: (see table below for the list of “FORMAT/” and “ÏNFO/” tags)\n\n\n–output-type\nb|u|z|v[0-9]\nOutput compressed BCF (b), uncompressed BCF (u), compressed VCF (z), uncompressed VCF (v). Use the -Ou option when piping between bcftools subcommands to speed up performance by removing unnecessary compression/decompression and VCF←→BCF conversion. The compression level of the compressed formats (b and z) can be set by by appending a number between 0-9.\n\n\n–ploidy\nASSEMBLY[?]\npredefined ploidy, use list (or any other unused word) to print a list of all predefined assemblies. Append a question mark to print the actual definition.\n\n\n–multiallelic-caller\n\nalternative model for multiallelic and rare-variant calling designed to overcome known limitations in -c calling model (conflicts with -c)\n\n\n\n“FORMAT/” and “INFO/” tags\n\n\n\n\n\n\n\nTag\nDescription\n\n\n\n\nFORMAT/AD\n.. Allelic depth (Number=R,Type=Integer)\n\n\nFORMAT/ADF\n.. Allelic depths on the forward strand (Number=R,Type=Integer)\n\n\nFORMAT/ADR\n.. Allelic depths on the reverse strand (Number=R,Type=Integer)\n\n\nFORMAT/DP\n.. Number of high-quality bases (Number=1,Type=Integer)\n\n\nFORMAT/SP\n.. Phred-scaled strand bias P-value (Number=1,Type=Integer)\n\n\nFORMAT/SCR\n.. Number of soft-clipped reads (Number=1,Type=Integer)\n\n\nINFO/AD\n.. Total allelic depth (Number=R,Type=Integer)\n\n\nINFO/ADF\n.. Total allelic depths on the forward strand (Number=R,Type=Integer)\n\n\nINFO/ADR\n.. Total allelic depths on the reverse strand (Number=R,Type=Integer)\n\n\nINFO/SCR\n.. Number of soft-clipped reads (Number=1,Type=Integer)\n\n\nFORMAT/DV\n.. Deprecated in favor of FORMAT/AD; Number of high-quality non-reference bases, (Number=1,Type=Integer)\n\n\nFORMAT/DP4\n.. Deprecated in favor of FORMAT/ADF and FORMAT/ADR; Number of high-quality ref-forward, ref-reverse, alt-forward and alt-reverse bases (Number=4,Type=Integer)\n\n\nFORMAT/DPR\n.. Deprecated in favor of FORMAT/AD; Number of high-quality bases for each observed allele (Number=R,Type=Integer)\n\n\nINFO/DPR\n.. Deprecated in favor of INFO/AD; Number of high-quality bases for each observed allele (Number=R,Type=Integer)\n\n\n\nYou can have a quick look at the output file G26832.vcf.gz Note that, the output file is a compressed file. To view a compressed file we use zcat, and since we do not want to view the entire content of the file, we pipe it to head. Pipe | should be your friend by now. :)\n\n\n\n\n\n\nquick look at first 42 lines\n\n\n\nzcat G26832.vcf.gz | head -n 42\n##fileformat=VCFv4.2\n##FILTER=<ID=PASS,Description=\"All filters passed\">\n##bcftoolsVersion=1.14+htslib-1.14\n##bcftoolsCommand=mpileup --fasta-ref MTB_H37Rv.fasta --min-BQ 20 --annotate FORMAT/AD,FORMAT/ADF,FORMAT/ADR,FORMAT/DP,FORMAT/SP,INFO/AD,INFO/ADF,INFO/ADR G26832.sorted.aln.bam\n##reference=file://MTB_H37Rv.fasta\n##contig=<ID=NC_000962.3,length=4411532>\n##ALT=<ID=*,Description=\"Represents allele(s) other than observed.\">\n##INFO=<ID=INDEL,Number=0,Type=Flag,Description=\"Indicates that the variant is an INDEL.\">\n##INFO=<ID=IDV,Number=1,Type=Integer,Description=\"Maximum number of raw reads supporting an indel\">\n##INFO=<ID=IMF,Number=1,Type=Float,Description=\"Maximum fraction of raw reads supporting an indel\">\n##INFO=<ID=DP,Number=1,Type=Integer,Description=\"Raw read depth\">\n##INFO=<ID=VDB,Number=1,Type=Float,Description=\"Variant Distance Bias for filtering splice-site artefacts in RNA-seq data (bigger is better)\",Version=\"3\">\n##INFO=<ID=RPBZ,Number=1,Type=Float,Description=\"Mann-Whitney U-z test of Read Position Bias (closer to 0 is better)\">\n##INFO=<ID=MQBZ,Number=1,Type=Float,Description=\"Mann-Whitney U-z test of Mapping Quality Bias (closer to 0 is better)\">\n##INFO=<ID=BQBZ,Number=1,Type=Float,Description=\"Mann-Whitney U-z test of Base Quality Bias (closer to 0 is better)\">\n##INFO=<ID=MQSBZ,Number=1,Type=Float,Description=\"Mann-Whitney U-z test of Mapping Quality vs Strand Bias (closer to 0 is better)\">\n##INFO=<ID=SCBZ,Number=1,Type=Float,Description=\"Mann-Whitney U-z test of Soft-Clip Length Bias (closer to 0 is better)\">\n##INFO=<ID=FS,Number=1,Type=Float,Description=\"Phred-scaled p-value using Fisher's exact test to detect strand bias\">\n##INFO=<ID=SGB,Number=1,Type=Float,Description=\"Segregation based metric.\">\n##INFO=<ID=MQ0F,Number=1,Type=Float,Description=\"Fraction of MQ0 reads (smaller is better)\">\n##FORMAT=<ID=PL,Number=G,Type=Integer,Description=\"List of Phred-scaled genotype likelihoods\">\n##FORMAT=<ID=DP,Number=1,Type=Integer,Description=\"Number of high-quality bases\">\n##FORMAT=<ID=SP,Number=1,Type=Integer,Description=\"Phred-scaled strand bias P-value\">\n##FORMAT=<ID=AD,Number=R,Type=Integer,Description=\"Allelic depths (high-quality bases)\">\n##FORMAT=<ID=ADF,Number=R,Type=Integer,Description=\"Allelic depths on the forward strand (high-quality bases)\">\n##FORMAT=<ID=ADR,Number=R,Type=Integer,Description=\"Allelic depths on the reverse strand (high-quality bases)\">\n##INFO=<ID=AD,Number=R,Type=Integer,Description=\"Total allelic depths (high-quality bases)\">\n##INFO=<ID=ADF,Number=R,Type=Integer,Description=\"Total allelic depths on the forward strand (high-quality bases)\">\n##INFO=<ID=ADR,Number=R,Type=Integer,Description=\"Total allelic depths on the reverse strand (high-quality bases)\">\n##FORMAT=<ID=GT,Number=1,Type=String,Description=\"Genotype\">\n##INFO=<ID=AC,Number=A,Type=Integer,Description=\"Allele count in genotypes for each ALT allele, in the same order as listed\">\n##INFO=<ID=AN,Number=1,Type=Integer,Description=\"Total number of alleles in called genotypes\">\n##INFO=<ID=DP4,Number=4,Type=Integer,Description=\"Number of high-quality ref-forward , ref-reverse, alt-forward and alt-reverse bases\">\n##INFO=<ID=MQ,Number=1,Type=Integer,Description=\"Average mapping quality\">\n##bcftools_callVersion=1.14+htslib-1.14\n##bcftools_callCommand=call --output-type v --ploidy 1 --multiallelic-caller; Date=Mon Nov 14 15:29:39 2022\n##bcftools_viewVersion=1.14+htslib-1.14\n##bcftools_viewCommand=view --output-file G26832.vcf.gz --output-type z; Date=Mon Nov 14 15:29:39 2022\n#CHROM  POS     ID      REF     ALT     QUAL    FILTER  INFO    FORMAT  G26832.sorted.aln.bam\nNC_000962.3     1       .       T       .       284.59  .       DP=41;ADF=33;ADR=7;AD=40;MQSBZ=0;FS=0;MQ0F=0;AN=1;DP4=33,7,0,0;MQ=59        GT:DP:SP:ADF:ADR:AD     0:40:0:33:7:40\nNC_000962.3     2       .       T       .       284.59  .       DP=45;ADF=37;ADR=7;AD=44;MQSBZ=0;FS=0;MQ0F=0;AN=1;DP4=37,7,0,0;MQ=59        GT:DP:SP:ADF:ADR:AD     0:44:0:37:7:44\nNC_000962.3     3       .       G       .       284.59  .       DP=45;ADF=37;ADR=7;AD=44;MQSBZ=0;FS=0;MQ0F=0;AN=1;DP4=37,7,0,0;MQ=59        GT:DP:SP:ADF:ADR:AD     0:44:0:37:7:44 \n\n\n\n\n\n\n\n\nExercise 5.1.5.1: View only informative lines.\n\n\n\nWrite down a quick command that you will use to view only the informative lines starting from the #chrom line.\n\n\n\n\n\n\nSolution:\n\n\n\n\n\nThe command below performs the requested task. NB. The -v, –invert-match option tells grep to select only non-matching lines.\nzcat G26832.vcf.gz | grep -v \"##\" | head -n 10\n#CHROM  POS     ID      REF     ALT     QUAL    FILTER  INFO    FORMAT  G26832.sorted.aln.bam\nNC_000962.3     1       .       T       .       284.59  .       DP=41;ADF=33;ADR=7;AD=40;MQSBZ=0;FS=0;MQ0F=0;AN=1;DP4=33,7,0,0;MQ=59GT:DP:SP:ADF:ADR:AD      0:40:0:33:7:40\nNC_000962.3     2       .       T       .       284.59  .       DP=45;ADF=37;ADR=7;AD=44;MQSBZ=0;FS=0;MQ0F=0;AN=1;DP4=37,7,0,0;MQ=59GT:DP:SP:ADF:ADR:AD      0:44:0:37:7:44\nNC_000962.3     3       .       G       .       284.59  .       DP=45;ADF=37;ADR=7;AD=44;MQSBZ=0;FS=0;MQ0F=0;AN=1;DP4=37,7,0,0;MQ=59GT:DP:SP:ADF:ADR:AD      0:44:0:37:7:44\nNC_000962.3     4       .       A       .       284.59  .       DP=46;ADF=38;ADR=7;AD=45;MQSBZ=0;FS=0;MQ0F=0;AN=1;DP4=38,7,0,0;MQ=59GT:DP:SP:ADF:ADR:AD      0:45:0:38:7:45\nNC_000962.3     5       .       C       .       284.59  .       DP=50;ADF=40;ADR=8;AD=48;MQSBZ=0;FS=0;MQ0F=0;AN=1;DP4=40,8,0,0;MQ=59GT:DP:SP:ADF:ADR:AD      0:48:0:40:8:48\nNC_000962.3     6       .       C       .       284.59  .       DP=51;ADF=41;ADR=8;AD=49;MQSBZ=0;FS=0;MQ0F=0;AN=1;DP4=41,8,0,0;MQ=59GT:DP:SP:ADF:ADR:AD      0:49:0:41:8:49\nNC_000962.3     7       .       G       .       284.59  .       DP=52;ADF=42;ADR=8;AD=50;MQSBZ=0;FS=0;MQ0F=0;AN=1;DP4=42,8,0,0;MQ=59GT:DP:SP:ADF:ADR:AD      0:50:0:42:8:50\nNC_000962.3     8       .       A       .       284.59  .       DP=52;ADF=42;ADR=8;AD=50;MQSBZ=0;FS=0;MQ0F=0;AN=1;DP4=42,8,0,0;MQ=59GT:DP:SP:ADF:ADR:AD      0:50:0:42:8:50\nNC_000962.3     9       .       T       .       284.59  .       DP=52;ADF=42;ADR=8;AD=50;MQSBZ=0;FS=0;MQ0F=0;AN=1;DP4=42,8,0,0;MQ=59GT:DP:SP:ADF:ADR:AD      0:50:0:42:8:50\n\n\n\n\n\n\n\n\n\n\n\nExercise 5.1.5.2: Count number of INDELS\n\n\n\nWrite down a quick command that you will use to count only the insertions and deletions (INDELS) present in our .vcf file.\nHow many INDELS do you find?\nGo ahead and list the first 9 INDELS.\n\n\nHint\n\nThere is a flag called ‘INDEL’\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n\nThe command below counts the number of INDELS.\n\n\n\n\n\n\nzcat G26832.vcf.gz | grep \"INDEL\" | wc -l\n95\n\n\n\nThe command below lists the first 9 lines containing INDELS.\n\n\n\n\n\n\n\n\n\n\nzcat G26832.vcf.gz | grep \"INDEL\" | head -n 10\n##INFO=<ID=INDEL,Number=0,Type=Flag,Description=\"Indicates that the variant is an INDEL.\">\nNC_000962.3     27518   .       TAAAAA  TAAAA   228.359 .       INDEL;IDV=88;IMF=0.926316;DP=95;ADF=0,47;ADR=4,36;AD=4,83;VDB=0.7912;SGB=-0.693147;RPBZ=-4.01107;MQBZ=0;MQSBZ=0;SCBZ=-2.34859;FS=0;MQ0F=0;AC=1;AN=1;DP4=0,4,47,36;MQ=60 GT:PL:DP:SP:ADF:ADR:AD  1:255,0:87:14:0,47:4,36:4,83\nNC_000962.3     47138   .       CTTT    CTT     225.417 .       INDEL;IDV=92;IMF=0.948454;DP=97;ADF=0,49;ADR=0,37;AD=0,86;VDB=0.657263;SGB=-0.693147;RPBZ=-0.661043;MQBZ=0;MQSBZ=0;SCBZ=-2.85007;FS=0;MQ0F=0;AC=1;AN=1;DP4=0,0,49,37;MQ=60     GT:PL:DP:SP:ADF:ADR:AD   1:255,0:86:0:0,49:0,37:0,86\nNC_000962.3     49233   .       AGGG    AGG     225.417 .       INDEL;IDV=81;IMF=0.94186;DP=86;ADF=0,44;ADR=0,36;AD=0,80;VDB=0.00122889;SGB=-0.693147;RPBZ=-3.73886;MQBZ=0;MQSBZ=0;SCBZ=-2.06091;FS=0;MQ0F=0;AC=1;AN=1;DP4=0,0,44,36;MQ=60     GT:PL:DP:SP:ADF:ADR:AD   1:255,0:80:0:0,44:0,36:0,80\nNC_000962.3     55553   .       CCG     CCGTCG  228.31  .       INDEL;IDV=46;IMF=0.647887;DP=71;ADF=5,21;ADR=2,25;AD=7,46;VDB=0.0134748;SGB=-0.693147;RPBZ=1.91017;MQBZ=0;MQSBZ=0;BQBZ=-4.33586;SCBZ=-3.44435;FS=0;MQ0F=0;AC=1;AN=1;DP4=5,2,21,25;MQ=60 GT:PL:DP:SP:ADF:ADR:AD  1:255,0:53:6:5,21:2,25:7,46\nNC_000962.3     125830  .       GAAAA   GAAAAA  228.416 .       INDEL;IDV=67;IMF=0.8375;DP=80;ADF=1,28;ADR=2,36;AD=3,64;VDB=0.730354;SGB=-0.693147;RPBZ=-3.82272;MQBZ=0;MQSBZ=0;SCBZ=-4.61458;FS=0;MQ0F=0;AC=1;AN=1;DP4=1,2,28,36;MQ=60 GT:PL:DP:SP:ADF:ADR:AD  1:255,0:67:0:1,28:2,36:3,64\nNC_000962.3     131174  .       TGG     TGGG    228.424 .       INDEL;IDV=87;IMF=0.977528;DP=89;ADF=0,46;ADR=1,39;AD=1,85;VDB=0.0036349;SGB=-0.693147;RPBZ=-0.152335;MQBZ=0;MQSBZ=0;SCBZ=0.265603;FS=0;MQ0F=0;AC=1;AN=1;DP4=0,1,46,39;MQ=60    GT:PL:DP:SP:ADF:ADR:AD   1:255,0:86:0:0,46:1,39:1,85\nNC_000962.3     175277  .       GC      GCC     228.38  .       INDEL;IDV=67;IMF=0.943662;DP=71;ADF=2,24;ADR=1,39;AD=3,63;VDB=0.0525057;SGB=-0.693147;RPBZ=-1.65917;MQBZ=0;MQSBZ=0;SCBZ=0;FS=0;MQ0F=0;AC=1;AN=1;DP4=2,1,24,39;MQ=60     GT:PL:DP:SP:ADF:ADR:AD  1:255,0:66:3:2,24:1,39:3,63\nNC_000962.3     234496  .       C       CGT     228.391 .       INDEL;IDV=78;IMF=0.896552;DP=87;ADF=2,46;ADR=2,31;AD=4,77;VDB=0.617617;SGB=-0.693147;RPBZ=-1.63117;MQBZ=0;MQSBZ=0;SCBZ=-4.37836;FS=0;MQ0F=0;AC=1;AN=1;DP4=2,2,46,31;MQ=60      GT:PL:DP:SP:ADF:ADR:AD   1:255,0:81:0:2,46:2,31:4,77\nNC_000962.3     330711  .       GCACCATGAACCATCTGGCGT   G       228.424 .       INDEL;IDV=66;IMF=0.825;DP=80;ADF=1,34;ADR=0,32;AD=1,66;VDB=3.86599e-10;SGB=-0.693147;RPBZ=-5.86754;MQBZ=0;MQSBZ=0;SCBZ=-8.45738;FS=0;MQ0F=0;AC=1;AN=1;DP4=1,0,34,32;MQ=60       GT:PL:DP:SP:ADF:ADR:AD  1:255,0:67:0:1,34:0,32:1,66\n\n\n\n\n\n\n\n\n\nbcftools filter\nbcftools filter – basically filters VCF files by applying fixed-threshold filters\n\n\n\n\n\n\nHelp\n\n\n\nDo this to get the help information for bcftools filter\nbcftools filter\nAbout:   Apply fixed-threshold filters.\nUsage:   bcftools filter [options] <in.vcf.gz>\n\nOptions:\n    -e, --exclude EXPR             Exclude sites for which the expression is true (see man page for details)\n    -g, --SnpGap INT[:TYPE]        Filter SNPs within <int> base pairs of an indel (the default) or any combination of indel,mnp,bnd,other,overlap\n    -G, --IndelGap INT             Filter clusters of indels separated by <int> or fewer base pairs allowing only one to pass\n    -i, --include EXPR             Include only sites for which the expression is true (see man page for details\n    -m, --mode [+x]                \"+\": do not replace but add to existing FILTER; \"x\": reset filters at sites which pass\n        --no-version               Do not append version and command line to the header\n    -o, --output FILE              Write output to a file [standard output]\n    -O, --output-type u|b|v|z[0-9] u/b: un/compressed BCF, v/z: un/compressed VCF, 0-9: compression level [v]\n    -r, --regions REGION           Restrict to comma-separated list of regions\n    -R, --regions-file FILE        Restrict to regions listed in a file\n        --regions-overlap 0|1|2    Include if POS in the region (0), record overlaps (1), variant overlaps (2) [1]\n    -s, --soft-filter STRING       Annotate FILTER column with <string> or unique filter name (\"Filter%d\") made up by the program (\"+\")\n    -S, --set-GTs .|0              Set genotypes of failed samples to missing (.) or ref (0)\n    -t, --targets REGION           Similar to -r but streams rather than index-jumps\n    -T, --targets-file FILE        Similar to -R but streams rather than index-jumps\n        --targets-overlap 0|1|2    Include if POS in the region (0), record overlaps (1), variant overlaps (2) [0]\n        --threads INT              Use multithreading with <int> worker threads [0]\n\n\n\n\n\n\n\n\nUsage\n\n\n\nThe general format of the command is:\nbcftools filter [OPTIONS] FILE\n\n\nBefore we run bcftools filter we will need to create an index of our .vcf.gz file. To do this, we will use the command tabix.\n\n\n\n\n\n\ntabix\n\n\n\nYou can check out it’s help function\n\n\n\n\n\n\nHelp\n\n\n\nDo this to get the help information for tabix\ntabix --help\nVersion: 1.14\nUsage:   tabix [OPTIONS] [FILE] [REGION [...]]\n\nIndexing Options:\n   -0, --zero-based           coordinates are zero-based\n   -b, --begin INT            column number for region start [4]\n   -c, --comment CHAR         skip comment lines starting with CHAR [null]\n   -C, --csi                  generate CSI index for VCF (default is TBI)\n   -e, --end INT              column number for region end (if no end, set INT to -b) [5]\n...\n\n\n\n\n\n\n\n\nUsage\n\n\n\nThe general format of the command is:\ntabix [OPTIONS] [FILE] [REGION [...]]\n\n\nWe will index our file by running the following command:\ntabix -p vcf -f G26832.vcf.gz\nYou shouldn’t expect a message printed out on the screen.\nJust look out for the new file generated\nG26832.vcf.gz.tbi\n\n\nNow that we have our index file let’s go back to perform our filtering.\nWe perform the filtering by running the following command:\n\n\n\n\n\n\nbcftools filter\n\n\n\nbcftools filter --output G26832.filtered.vcf.gz --soft-filter LowQual --exclude \"%QUAL<25 || FORMAT/DP<10 || MAX(FORMAT/ADF)<2 || MAX(FORMAT/ADR)<2 || MAX(FORMAT/AD)/SUM(FORMAT/DP)<0.9 || MQ<30 || MQ0F>0.1\" --output-type z G26832.vcf.gz\nbusy\nDon’t expect any print out on your terminal. The expected file G26832.filtered.vcf.gz should be created by now. Look out for it in your directory.\nThe optional arguments we have used in the above command have the following meaning:\n\n\n\noption\ninterpretation\n\n\n\n\n--soft-filter\nAnnotate FILTER column with the unique filter name LowQual\n\n\n--exclude\nExclude sites for which the expression is true see this link for all the possible expressions\n\n\n--output-type z\nSpecifies a compressed VCF output\n\n\n\n\n\nCan you think of any analysis we can use our G26832.filtered.vcf.gz file for?\nThe last thing we will do is to try to generate a consensus sequence or an assembled genome from our G26832.filtered.vcf.gz file. We call this assembled genome a pseudogenome. Remember that the assembled genome will be coming from our mapped sequences and so will have the same number of genomic positions as our reference genome. Later in this course, we will see how to assemble a genome directly from the raw reads. We call this kind of assembly a denovo assembly.\nFor now, let’s focus on getting our pseudogenome.\nThere are various tools to do this but we will use a simple python script to archive this."
  },
  {
    "objectID": "materials/05-short_read_mapping/5.1-short_read_mapping.html#generating-a-consensus-sequence",
    "href": "materials/05-short_read_mapping/5.1-short_read_mapping.html#generating-a-consensus-sequence",
    "title": "5.1 Short Read Mapping",
    "section": "5.1.6 Generating a consensus sequence",
    "text": "5.1.6 Generating a consensus sequence\nOur consensus sequence (pseudogenome) will be generated in a fasta file format.\n\nParse vcf files into fasta format\nWe will run a simple python script to achieve this purpose. In your working directory, you will find a file named vcf2pseudogenome.py. This is a simple python script that will do the job for us.\nWe can go ahead and execute that python script by running the command below:\n\n\n\n\n\n\nexecute python script\n\n\n\npython vcf2pseudogenome.py  -r MTB_H37Rv.fasta -b G26832.filtered.vcf.gz -o G26832.fas\n10000\n20000\n30000\n40000\n50000\n60000\n70000\n...\n\n\n\nYou should now see another file generated in your directory called G26832.fas. What the python script did was to pick the variants called from the vcf file and replace them in the reference genome to call out the new file.\nNow let’s go ahead and have a look at the top five lines of the G26832.fas file with the command:\n\n\n\n\n\n\nhead -n 5 G26832.fas\n>G26832\nttgaccgatgaccccggttcaggcttcaccacagtgtggaacgcggtcgtctccgaactt\naacggcgaccctaaggttgacgacggacccagcagtgatgctaatctcagcgctccgctg\nacccctcagcaaagggcttggctcaatctcgtccagccattgaccatcgtcgaggggttt\ngctctgttatccgtgccgagcagctttgtccaaaacgaaatcgagcgccatctgcgggcc\n\n\n\n\n\n\nDisk Usage II — Cleaning up after analysis\nNow that we are done investigating our genomic sequences, let’s pause again and check the space of our current working directory.\nYou can do this with the disk usage du command\ndu -h\nHow much disk space have you used since the start of the analysis? I’m sure it’s about 2.2G. Just from one set of reads, we have generated about 2G of data we may not need. Imaging if we had hundreds of samples. What files do you consider not necessary to keep? Let’s do some cleaning up. We will remove rm all files that we may not need for further analysis. Most of these files are intermediate files and can always be reproduced if we need to.\n\nremove all .sam files if not needed\nrm *.sam\n\n\nremove all bam files if not needed\nrm *bam*"
  },
  {
    "objectID": "materials/05-short_read_mapping/5.1-short_read_mapping.html#mapping-to-pseudogenome-bash-script-putting-it-all-together",
    "href": "materials/05-short_read_mapping/5.1-short_read_mapping.html#mapping-to-pseudogenome-bash-script-putting-it-all-together",
    "title": "5.1 Short Read Mapping",
    "section": "5.1.7 Mapping to pseudogenome bash script: Putting it all together",
    "text": "5.1.7 Mapping to pseudogenome bash script: Putting it all together\nNow let’s try this out! We will generate pseudogenomes and some quality statistics for three pairs of reads from Illumina paired-end sequencing. We will perform all the analysis above in one run while generating all the various files at each stage within the pipeline in tandem.\n\nRunning multiple samples with a simple bash script\nThis is a pipeline to map short reads to a reference assembly. It outputs some quality statistics, does variant calling and produces a pseudogenome of each input sample.\nAll the fastq files needed for this analysis are located in the current working short_read_mapping_MTB directory. You can have a look by simply running the below command to check out all the forward reads present in the current directory.\n\n\n\n\n\n\nls *R1.trim.fastq.gz\nG26831_R1.trim.fastq.gz  G26832_R1.trim.fastq.gz  G26854_R1.trim.fastq.gz\n\n\n\nLet’s have a look at the QC bash script we are going to run:\ncat mappingTofasta.sh\nJust like the script used in our previous lesson, this script also contains several commands, some are combined together using pipes. (UNIX pipes is a very powerful and elegant concept which allows us to feed the output of one command into the next command and avoid writing intermediate files. If you are not comfortable with UNIX or how scripts look like and how they work, consider revisiting the UNIX tutorial or more specifically bonus shell script.\nNow run the script to create the quality statistics and pseudogenomes (this may take a while):\n\n\n\n\n\n\nmapping to fasta\n\n\n\nbash mappingTofasta.sh\n\n#################################################################################\n#### loop bash script for running Mapping to generating pseudogenome fasta files ######\n#######################################################################################\n#               #\n#  ___   ____   #\n# / _ \\ / ___|  #\n# | | | | |     #\n# | |_| | |___  #\n# \\__\\_\\ \\____| #\n#               #\n#               #\n#######################################################################################\nMTB_H37Rv.fasta exists and will be used in the next run in a second.\n <<<<<<<running bwa index>>>>>>>\n[bwa_index] Pack FASTA... 0.04 sec\n[bwa_index] Construct BWT for the packed sequence...\n[bwa_index] 0.98 seconds elapse.\n...\n\n\nThe script will produce all the intermediate and final files we have encountered in this session.\nPerform a simple ls command with the arguments -lhrt and view the most recent files created.\nls -lhrt\nCongratulations!!! You just created your first set of pseudogenomes using a bash script.\n\n\n\n\n\n\n\nExercise 5.1.7.1: Advance Exercise: Reproducible Research Assessment\n\n\n\nLet’s try to apply what we’ve learnt so far to see if we can reproduce the output figure below retrieved from a publication.\n\n\n\nST88 phylogeny and accessory genome analysis\n\n\nYou can click here to read the abstract of the publication Kpeli et. a., 2017 PeerJ\nIn the publication, they state that they used comparative genomics to assess relatedness among 17 ST88 CA-MRSA isolates recovered from patients attending Buruli ulcer treatment centres in Ghana, three non-African ST88s and 15 other MRSA lineages from the current study and Amissah et. al., 2015 PNTD.\nYour task is simple. Each person will be provided with two pairs of sequencing reads – one from the above paper and the other from elsewhere.\n\nNavigate to the short_read_mapping_Staph directory and activate the mapping environment. You have all you need for this exercise in this directory.\nPerforms QC on the pair of genome and select the best genome\nPerform mapping analysis and generate a pseudogenome\nUpload your pseudogenome to the dropbox folder \n\nNB. All the pseudogenomes would be concatenated to draw a tree at the end of the phylogenetic lesson.\n\n\n\n\n\n\nSolution:\n\n\n\n\n\nChange to the required working directory\ncd ~/Desktop/workshop_files_Bact_Genomics_2023/05_mapping/short_read_mapping_Staph/\nrun the qc analysis\n\nrun the mappingTofasta analysis"
  },
  {
    "objectID": "materials/05-short_read_mapping/5.1-short_read_mapping.html#deactivate-mapping-environment",
    "href": "materials/05-short_read_mapping/5.1-short_read_mapping.html#deactivate-mapping-environment",
    "title": "5.1 Short Read Mapping",
    "section": "5.1.8 Deactivate mapping environment",
    "text": "5.1.8 Deactivate mapping environment\nNow that we are done with all our analysis, let’s deactivate the mapping environment:\nmamba deactivate"
  },
  {
    "objectID": "materials/05-short_read_mapping/5.1-short_read_mapping.html#credit",
    "href": "materials/05-short_read_mapping/5.1-short_read_mapping.html#credit",
    "title": "5.1 Short Read Mapping",
    "section": "5.1.9 Credit",
    "text": "5.1.9 Credit\nInformation on this page has been adapted and modified from the following source(s):\n\nhttps://github.com/cambiotraining/sars-cov-2-genomics\nhttps://bio-bwa.sourceforge.net/\nhttp://www.htslib.org/doc/samtools.html\nhttps://software.broadinstitute.org/software/igv/home\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC3346182/\nhttps://github.com/sanger-pathogens/Artemis\nWellcome Genome Campus Advanced Courses and Scientific Conferences 2017 - WORKING WITH PATHOGEN GENOMES Course Manual http://www.wellcome.ac.uk/advancedcourses"
  },
  {
    "objectID": "materials/06-assembly_annotation/6.1-assembly_annotation.html",
    "href": "materials/06-assembly_annotation/6.1-assembly_annotation.html",
    "title": "6.1 Assembly and Annotation",
    "section": "",
    "text": "Teaching: 50 min || Exercises: 20 min"
  },
  {
    "objectID": "materials/06-assembly_annotation/6.1-assembly_annotation.html#overview",
    "href": "materials/06-assembly_annotation/6.1-assembly_annotation.html#overview",
    "title": "6.1 Assembly and Annotation",
    "section": "Overview",
    "text": "Overview\n\n\n\n\n\n\n\n\n\n\n\n\nQuestions:\n\n\n\n\nWhat is de novo genome assembly?\nHow do I assemble my genome?\nHow do I assess the quality of my genome?\nWhat is genome annotation?\nHow do I annotate my genome?\n\n\n\n\n\n\n\n\n\nLearning Objectives:\n\n\n\n\nUnderstand what de novo genome assembly is and how it differs from reference-based assembly.\nAssemble sequence data with shovill\nGenerate metrics for your assembly with QUAST.\nAnnotate your genome with prokka\n\n\n\n\n\n\n\n\n\nKeypoints:\n\n\n\n\nUnderstand what de novo genome assembly is and how we can use shovill to create one.\nExtract and interprete the assembly metrics using QUAST to decide how good your assembly is.\nUnderstand what genome annotation is and how these can be added using prokka."
  },
  {
    "objectID": "materials/06-assembly_annotation/6.1-assembly_annotation.html#background",
    "href": "materials/06-assembly_annotation/6.1-assembly_annotation.html#background",
    "title": "6.1 Assembly and Annotation",
    "section": "6.1 Background",
    "text": "6.1 Background\nThere are two approaches for genome assembly: reference-based (or comparative) or de novo. In a reference-based assembly, we use a reference genome as a guide to map our sequence data to and thus reassemble our sequence this way (this is what we did in the previous module). Alternatively, we can create a ‘new’ (de novo) assembly that does not rely on a map or reference and more closely reflects the actual genome structure of the isolate that was sequenced.\n\n\n\nGenome assembly\n\n\n\nGenome assemblers\nSeveral tools are available for de novo genome assembly depending on whether you’re trying to assemble short-read sequence data, long reads or else a combination of both. Two of the most commonly used assemblers for short-read Illumina data are Velvet and SPAdes. SPAdes has become the de facto standard de novo genome assembler for Illumina whole genome sequencing data of bacteria and is a major improvement over previous assemblers like Velvet. However, some of its components can be slow and it traditionally did not handle overlapping paired-end reads well. Shovill is a pipeline which uses SPAdes at its core, but alters the steps before and after the primary assembly step to get similar results in less time. Shovill also supports other assemblers like SKESA, Velvet and Megahit.\n\n\nDisk Usage I — Before analysis\n\nBefore we start performing any assemblies, let’s pause and check the space of our current working directory as we did for our previous lesson.\nYou can do this with the disk usage du command\ndu -h\n\n\nCurrent Disk Space In assembly_annotation_MTB Directory\n\n~247MB\n\nNow, keep this value in mind, and this time, don’t forget it. We will come back to it at the end of this chapter."
  },
  {
    "objectID": "materials/06-assembly_annotation/6.1-assembly_annotation.html#de-novo-genome-assembly",
    "href": "materials/06-assembly_annotation/6.1-assembly_annotation.html#de-novo-genome-assembly",
    "title": "6.1 Assembly and Annotation",
    "section": "6.2 de novo genome assembly",
    "text": "6.2 de novo genome assembly\n\nde novo genome assembly with Shovill\nWe’re going to use Shovill to create our de novo genome assemblies. Let’s navigate the module directory and activate the shovill environment:\n\ncd workshop_files_Bact_Genomics_2023/06_assembly_annotation \n\nmamba activate shovill\n\n\n\n\n\n\nHelp\n\n\n\nDo this to get the help information for Shovill:\nshovill -h\nSYNOPSIS\n  De novo assembly pipeline for Illumina paired reads\nUSAGE\n  shovill [options] --outdir DIR --R1 R1.fq.gz --R2 R2.fq.gz\nGENERAL\n  --help          This help\n  --version       Print version and exit\n  --check         Check dependencies are installed\nINPUT\n  --R1 XXX        Read 1 FASTQ (default: '')\n  --R2 XXX        Read 2 FASTQ (default: '')\n  --depth N       Sub-sample --R1/--R2 to this depth. Disable with --depth 0 (default: 150)\n  --gsize XXX     Estimated genome size eg. 3.2M <blank=AUTODETECT> (default: '')\nOUTPUT\n  --outdir XXX    Output folder (default: '')\n  --force         Force overwite of existing output folder (default: OFF)\n  --minlen N      Minimum contig length <0=AUTO> (default: 0)\n  --mincov n.nn   Minimum contig coverage <0=AUTO> (default: 2)\n  --namefmt XXX   Format of contig FASTA IDs in 'printf' style (default: 'contig%05d')\n  --keepfiles     Keep intermediate files (default: OFF)\nRESOURCES\n  --tmpdir XXX    Fast temporary directory (default: '')\n  --cpus N        Number of CPUs to use (0=ALL) (default: 8)\n  --ram n.nn      Try to keep RAM usage below this many GB (default: 16)\nASSEMBLER\n  --assembler XXX Assembler: megahit skesa velvet spades (default: 'spades')\n  --opts XXX      Extra assembler options in quotes eg. spades: '--sc' (default: '')\n  --kmers XXX     K-mers to use <blank=AUTO> (default: '')\nMODULES\n  --trim          Enable adaptor trimming (default: OFF)\n  --noreadcorr    Disable read error correction (default: OFF)\n  --nostitch      Disable read stitching (default: OFF)\n  --nocorr        Disable post-assembly correction (default: OFF)\nHOMEPAGE\n  https://github.com/tseemann/shovill - Torsten Seemann\n\n\nWe’ve chosen one of the TB genomes from our course dataset to assemble with Shovill: ERX1275297_ERR1203055 (TBNm 2359).\n\n\n\n\n\n\nUsage\n\n\n\nThe basic command for running Shovill is as follows:\nshovill --outdir <OUTDIR> --R1 <FWD FASTQ> --R2 <REV FASTQ> --gsize <GENOME SIZE>\nThe meaning of the options used is:\n\n\n\n\n\n\n\n\nInput option\nInput required\nDescription\n\n\n\n\n–outdir\nDIRECTORY\ndirectory to write the output files to\n\n\n–R1\nFILE\nRead 1 FASTQ file\n\n\n–R2\nFILE\nRead 2 FASTQ file\n\n\n–gsize\nTXT\nThe estimated genome size that shovill will use to calculate read coverage\n\n\n\n\n\n\n\n\n\n\n\nRun shovill\n\n\n\nLet’s build our first de novo assembly with shovill:\nshovill --outdir ERX1275297_ERR1203055 --R1 ERX1275297_ERR1203055_1.fastq.gz --R2 ERX1275297_ERR1203055_2.fastq.gz --gsize 4.3M\n[shovill] Hello ajv37\n[shovill] You ran: /home/ajv37/miniconda3/envs/shovill/bin/shovill --outdir ERX1275297_ERR1203055 --R1 ERX1275297_ERR1203055_1.fastq.gz --R2 ERX1275297_ERR1203055_2.fastq.gz --gsize 4.3M\n[shovill] This is shovill 1.1.0\n[shovill] Written by Torsten Seemann\n[shovill] Homepage is https://github.com/tseemann/shovill\n[shovill] Operating system is linux\n[shovill] Perl version is v5.32.1\n[shovill] Machine has 64 CPU cores and 187.36 GB RAM\n[shovill] Using bwa - /home/ajv37/miniconda3/envs/shovill/bin/bwa | Version: 0.7.17-r1188\n[shovill] Using flash - /home/ajv37/miniconda3/envs/shovill/bin/flash | FLASH v1.2.11\n[shovill] Using java - /home/ajv37/miniconda3/envs/shovill/bin/java | openjdk version \"11.0.8-internal\" 2020-07-14\n[shovill] Using kmc - /home/ajv37/miniconda3/envs/shovill/bin/kmc | K-Mer Counter (KMC) ver. 3.2.1 (2022-01-04)\n[shovill] Using lighter - /home/ajv37/miniconda3/envs/shovill/bin/lighter | Lighter v1.1.2\n[shovill] Using megahit - /home/ajv37/miniconda3/envs/shovill/bin/megahit | MEGAHIT v1.2.9\n[shovill] Using megahit_toolkit - /home/ajv37/miniconda3/envs/shovill/bin/megahit_toolkit | v1.2.9\n[shovill] Using pigz - /home/ajv37/miniconda3/envs/shovill/bin/pigz | pigz 2.6\n[shovill] Using pilon - /home/ajv37/miniconda3/envs/shovill/bin/pilon | Pilon version 1.24 Thu Jan 28 13:00:45 2021 -0500\n[shovill] Using samclip - /home/ajv37/miniconda3/envs/shovill/bin/samclip | samclip 0.4.0\n[shovill] Using samtools - /home/ajv37/miniconda3/envs/shovill/bin/samtools | Version: 1.16.1 (using htslib 1.16)\n[shovill] Using seqtk - /home/ajv37/miniconda3/envs/shovill/bin/seqtk | Version: 1.3-r106\n[shovill] Using skesa - /home/ajv37/miniconda3/envs/shovill/bin/skesa | SKESA 2.4.0\n...\nThe following files will be created in the ERX1275297_ERR1203055 directory:\n\n\n\nOutput file\nDescription\n\n\n\n\ncontigs.fa\nThe final assembly you should use\n\n\nshovill.log\nFull log file for bug reporting\n\n\nshovill.corrections\nList of post-assembly corrections\n\n\ncontigs.gfa\nAssembly graph\n\n\nspades.fasta\nRaw assembled contigs\n\n\n\n\n\nBefore we move on to the next step, let’s rename our assembly (contigs.fa) to something more useful:\ncd ERX1275297_ERR1203055\n\nmv contigs.fa ERX1275297_ERR1203055_contigs.fa\nFinally, deactivate the shovill environment:\nmamba deactivate"
  },
  {
    "objectID": "materials/06-assembly_annotation/6.1-assembly_annotation.html#assembly-quality-assessment",
    "href": "materials/06-assembly_annotation/6.1-assembly_annotation.html#assembly-quality-assessment",
    "title": "6.1 Assembly and Annotation",
    "section": "6.3 Assembly quality assessment",
    "text": "6.3 Assembly quality assessment\n\nAssembly QC with QUAST\nBefore we do any further analyses with our assemblies, we need to assess the quality. To do this we use a tool called QUAST which generates a number of useful metrics for assemblies. These include but aren’t limited to:\n\nThe total number of contigs greater than 0 bp. Ideally, we like to see assemblies in the smallest number of contigs (‘pieces’) as this means there is likely less missing data contained in gaps between contigs.\nThe total length of the assembly. We normally know what the expected length of our assembly is (for more diverse organisms like E. coli there may be a range of genome sizes). The length of your assembly should be close to the expected genome size. If it’s too big or too small, either there is some kind of contamination or else you’ve sequenced the wrong species!\nThe N50 of the assembly. This is the final metric often used to assess the quality of an assembly. The N50 is typically calculated by averaging the length of the largest contigs that account for 50% of the genome size. This is a bit more complicated conceptually but the higher the N50 the better. A large N50 implies that you have a small number of larger contigs in your assembly which equals a good assembly.\n\nLet’s activate the quast environment:\nmamba activate quast\n\n\n\n\n\n\nHelp\n\n\n\nDo this to get the help information for QUAST:\nquast -h\nQUAST: Quality Assessment Tool for Genome Assemblies\nVersion: 5.0.2\n\nUsage: python /home/ajv37/.conda/envs/quast/bin/quast [options] <files_with_contigs>\n\nOptions:\n-o  --output-dir  <dirname>       Directory to store all result files [default: quast_results/results_<datetime>]\n-r                <filename>      Reference genome file\n-g  --features [type:]<filename>  File with genomic feature coordinates in the reference (GFF, BED, NCBI or TXT)\n                                  Optional 'type' can be specified for extracting only a specific feature type from GFF\n-m  --min-contig  <int>           Lower threshold for contig length [default: 500]\n-t  --threads     <int>           Maximum number of threads [default: 25% of CPUs]\n\nAdvanced options:\n-s  --split-scaffolds                 Split assemblies by continuous fragments of N's and add such \"contigs\" to the comparison\n-l  --labels \"label, label, ...\"      Names of assemblies to use in reports, comma-separated. If contain spaces, use quotes\n-L                                    Take assembly names from their parent directory names\n-e  --eukaryote                       Genome is eukaryotic (primarily affects gene prediction)\n    --fungus                          Genome is fungal (primarily affects gene prediction)\n    --large                           Use optimal parameters for evaluation of large genomes\n                                      In particular, imposes '-e -m 3000 -i 500 -x 7000' (can be overridden manually)\n-k  --k-mer-stats                     Compute k-mer-based quality metrics (recommended for large genomes)\n                                      This may significantly increase memory and time consumption on large genomes\n    --k-mer-size                      Size of k used in --k-mer-stats [default: 101]\n    --circos                          Draw Circos plot\n-f  --gene-finding                    Predict genes using GeneMarkS (prokaryotes, default) or GeneMark-ES (eukaryotes, use --eukaryote)\n    --mgm                             Use MetaGeneMark for gene prediction (instead of the default finder, see above)\n    --glimmer                         Use GlimmerHMM for gene prediction (instead of the default finder, see above)\n    --gene-thresholds <int,int,...>   Comma-separated list of threshold lengths of genes to search with Gene Finding module\n                                      [default: 0,300,1500,3000]\n    --rna-finding                     Predict ribosomal RNA genes using Barrnap\n-b  --conserved-genes-finding         Count conserved orthologs using BUSCO (only on Linux)\n    --operons  <filename>             File with operon coordinates in the reference (GFF, BED, NCBI or TXT)\n    --est-ref-size <int>              Estimated reference size (for computing NGx metrics without a reference)\n    --contig-thresholds <int,int,...> Comma-separated list of contig length thresholds [default: 0,1000,5000,10000,25000,50000]\n-u  --use-all-alignments              Compute genome fraction, # genes, # operons in QUAST v1.* style.\n                                      By default, QUAST filters Minimap's alignments to keep only best ones\n-i  --min-alignment <int>             The minimum alignment length [default: 65]\n    --min-identity <float>            The minimum alignment identity (80.0, 100.0) [default: 95.0]\n-a  --ambiguity-usage <none|one|all>  Use none, one, or all alignments of a contig when all of them\n                                      are almost equally good (see --ambiguity-score) [default: one]\n    --ambiguity-score <float>         Score S for defining equally good alignments of a single contig. All alignments are sorted \n                                      by decreasing LEN * IDY% value. All alignments with LEN * IDY% < S * best(LEN * IDY%) are \n                                      discarded. S should be between 0.8 and 1.0 [default: 0.99]\n    --strict-NA                       Break contigs in any misassembly event when compute NAx and NGAx.\n                                      By default, QUAST breaks contigs only by extensive misassemblies (not local ones)\n-x  --extensive-mis-size  <int>       Lower threshold for extensive misassembly size. All relocations with inconsistency\n                                      less than extensive-mis-size are counted as local misassemblies [default: 1000]\n    --scaffold-gap-max-size  <int>    Max allowed scaffold gap length difference. All relocations with inconsistency\n                                      less than scaffold-gap-size are counted as scaffold gap misassemblies [default: 10000]\n    --unaligned-part-size  <int>      Lower threshold for detecting partially unaligned contigs. Such contig should have\n                                      at least one unaligned fragment >= the threshold [default: 500]\n    --skip-unaligned-mis-contigs      Do not distinguish contigs with >= 50% unaligned bases as a separate group\n                                      By default, QUAST does not count misassemblies in them\n    --fragmented                      Reference genome may be fragmented into small pieces (e.g. scaffolded reference) \n    --fragmented-max-indent  <int>    Mark translocation as fake if both alignments are located no further than N bases \n                                      from the ends of the reference fragments [default: 85]\n                                      Requires --fragmented option\n    --upper-bound-assembly            Simulate upper bound assembly based on the reference genome and reads\n    --upper-bound-min-con  <int>      Minimal number of 'connecting reads' needed for joining upper bound contigs into a scaffold\n                                      [default: 2 for mate-pairs and 1 for long reads]\n    --est-insert-size  <int>          Use provided insert size in upper bound assembly simulation [default: auto detect from reads or 255]\n    --plots-format  <str>             Save plots in specified format [default: pdf].\n                                      Supported formats: emf, eps, pdf, png, ps, raw, rgba, svg, svgz\n    --memory-efficient                Run everything using one thread, separately per each assembly.\n                                      This may significantly reduce memory consumption on large genomes\n    --space-efficient                 Create only reports and plots files. Aux files including .stdout, .stderr, .coords will not be created.\n                                      This may significantly reduce space consumption on large genomes. Icarus viewers also will not be built\n-1  --pe1     <filename>              File with forward paired-end reads (in FASTQ format, may be gzipped)\n-2  --pe2     <filename>              File with reverse paired-end reads (in FASTQ format, may be gzipped)\n    --pe12    <filename>              File with interlaced forward and reverse paired-end reads. (in FASTQ format, may be gzipped)\n    --mp1     <filename>              File with forward mate-pair reads (in FASTQ format, may be gzipped)\n    --mp2     <filename>              File with reverse mate-pair reads (in FASTQ format, may be gzipped)\n    --mp12    <filename>              File with interlaced forward and reverse mate-pair reads (in FASTQ format, may be gzipped)\n    --single  <filename>              File with unpaired short reads (in FASTQ format, may be gzipped)\n    --pacbio     <filename>           File with PacBio reads (in FASTQ format, may be gzipped)\n    --nanopore   <filename>           File with Oxford Nanopore reads (in FASTQ format, may be gzipped)\n    --ref-sam <filename>              SAM alignment file obtained by aligning reads to reference genome file\n    --ref-bam <filename>              BAM alignment file obtained by aligning reads to reference genome file\n    --sam     <filename,filename,...> Comma-separated list of SAM alignment files obtained by aligning reads to assemblies\n                                      (use the same order as for files with contigs)\n    --bam     <filename,filename,...> Comma-separated list of BAM alignment files obtained by aligning reads to assemblies\n                                      (use the same order as for files with contigs)\n                                      Reads (or SAM/BAM file) are used for structural variation detection and\n                                      coverage histogram building in Icarus\n    --sv-bedpe  <filename>            File with structural variations (in BEDPE format)\n\nSpeedup options:\n    --no-check                        Do not check and correct input fasta files. Use at your own risk (see manual)\n    --no-plots                        Do not draw plots\n    --no-html                         Do not build html reports and Icarus viewers\n    --no-icarus                       Do not build Icarus viewers\n    --no-snps                         Do not report SNPs (may significantly reduce memory consumption on large genomes)\n    --no-gc                           Do not compute GC% and GC-distribution\n    --no-sv                           Do not run structural variation detection (make sense only if reads are specified)\n    --no-gzip                         Do not compress large output files\n    --no-read-stats                   Do not align reads to assemblies\n                                      Reads will be aligned to reference and used for coverage analysis,\n                                      upper bound assembly simulation, and structural variation detection.\n                                      Use this option if you do not need read statistics for assemblies.\n    --fast                            A combination of all speedup options except --no-check\n\nOther:\n    --silent                          Do not print detailed information about each step to stdout (log file is not affected)\n    --test                            Run QUAST on the data from the test_data folder, output to quast_test_output\n    --test-sv                         Run QUAST with structural variants detection on the data from the test_data folder,\n                                      output to quast_test_output\n-h  --help                            Print full usage message\n-v  --version                         Print version\n\nOnline QUAST manual is available at http://quast.sf.net/manual\n\n\n\n\n\n\n\n\nUsage\n\n\n\nThe basic command for running QUAST is as follows:\nquast.py --output-dir <OUTDIR> <ASSEMBLY>  \nThe meaning of the option used is:\n\n\n\nInput option\nInput required\nDescription\n\n\n\n\n–output-dir\nDIRECTORY\nDirectory to write the output files to\n\n\n\n\n\n\n\n\n\n\n\nRun QUAST\n\n\n\nLet’s run QUAST on the genome we’ve just created to assess the quality:\nquast.py --output-dir quast ERX1275297_ERR1203055_contigs.fa\nVersion: 5.0.2\n\nSystem information:\n  OS: Linux-3.10.0-1160.80.1.el7.x86_64-x86_64-with-redhat-7.9-Nitrogen (linux_64)\n  Python version: 3.7.12\n  CPUs number: 64\n\nStarted: 2022-12-15 16:44:12\n\nLogging to /rds/project/rds-PzYD5LltalA/Teaching/Ghana/shovill/ERX1275297_ERR1203055.1/quast/quast.log\nNOTICE: Maximum number of threads is set to 16 (use --threads option to set it manually)\n\nCWD: /rds/project/rds-PzYD5LltalA/Teaching/Ghana/shovill/ERX1275297_ERR1203055.1\nMain parameters: \n  MODE: default, threads: 16, minimum contig length: 500, minimum alignment length: 65, \\\n  ambiguity: one, threshold for extensive misassembly size: 1000\n...\nThe following files will be created in the quast directory:\n\n\n\n\n\n\n\nOutput file\nDescription\n\n\n\n\nreport.txt\nAssessment summary in plain text format\n\n\nreport.tsv\nTab-separated version of the summary, suitable for spreadsheets (Google Docs, Excel, etc)\n\n\nreport.tex\nLaTeX version of the summary\n\n\nicarus.html\nIcarus main menu with links to interactive viewers\n\n\nreport.pdf\nAll other plots combined with all tables (file is created if matplotlib python library is installed)\n\n\nreport.html\nHTML version of the report with interactive plots inside\n\n\ntransposed_report.txt\nTransposed assessment summary in plain text format\n\n\ntransposed_report.tsv\nTransposed tab-separated version of the summary\n\n\ntransposed_report.tex\nTransposed LaTeX version of the summary\n\n\n\n\n\n\n\n\n\n\n\nExercise 6.1.1: Investigate the QUAST output\n\n\n\nNow that we’ve got the results from QUAST, how good is the assembly we created with shovill?\n\nWhat is the total number of contigs?\nWhat is the total length of the assembly?\nWhat is the N50 of the assembly?\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n\nThere are a couple of ways we could answer the questions above as the assembly metrics can be parsed in different ways. Let’s parse the report.tsv:\ncat report.tsv\nThis will provide all the information we need:\nAssembly    ERX1275297_ERR1203055_contigs\n# contigs (>= 0 bp) 227\n# contigs (>= 1000 bp)  86\n# contigs (>= 5000 bp)  61\n# contigs (>= 10000 bp) 55\n# contigs (>= 25000 bp) 48\n# contigs (>= 50000 bp) 35\nTotal length (>= 0 bp)  4397844\nTotal length (>= 1000 bp)   4361978\nTotal length (>= 5000 bp)   4311192\nTotal length (>= 10000 bp)  4270901\nTotal length (>= 25000 bp)  4162429\nTotal length (>= 50000 bp)  3732349\n# contigs   105\nLargest contig  298563\nTotal length    4376331\nGC (%)  65.58\nN50 99158\nN75 64010\nL50 15\nL75 28\n# N's per 100 kbp   0.00\n\nThe total number of contigs > 0bp is 227\nThe total length of the assembly is 4397844\nThe N50 of the assembly is 99158\n\nWhat do you think? Should we move forward with this assembly? The answer is yes!\n\n\n\n\n\nFinally, deactivate the quast environment:\nmamba deactivate"
  },
  {
    "objectID": "materials/06-assembly_annotation/6.1-assembly_annotation.html#genome-annotation",
    "href": "materials/06-assembly_annotation/6.1-assembly_annotation.html#genome-annotation",
    "title": "6.1 Assembly and Annotation",
    "section": "6.4 Genome annotation",
    "text": "6.4 Genome annotation\nGenome annotation is a multi-level process that includes prediction of protein-coding genes (CDSs), as well as other functional genome units such as structural RNAs, tRNAs, small RNAs, pseudogenes, control regions, direct and inverted repeats, insertion sequences, transposons and other mobile elements. The most commonly used tools for annotating bacterial genomes are Prokka and, more recently, Bakta. Both use a tool called prodigal to predict the protein-coding regions along with other tools for predicting other genomic features such as Aragorn for tRNA. Once the genomic regions have been predicted, the tools use a database of existing bacterial genome annotations, normally generated from a large collection of genomes such as UniRef, to add this information to your genome.\n\nAnnotation with Prokka\nProkka is a software tool to annotate bacterial, archaeal and viral genomes quickly and produce standards-compliant output files (GenBank, EMBL and gff) for further analysis or viewing in genome browsers. It is suitable for annotating de novo assemblies of bacteria, but not appropriate for human genomes (or any other eukaryote).\nBefore we run Prokka, let’s activate the environment:\nmamba activate prokka\n\n\n\n\n\n\nHelp\n\n\n\nDo this to get the help information for Prokka:\nprokka --help\nName:\n  Prokka 1.14.6 by Torsten Seemann <torsten.seemann@gmail.com>\nSynopsis:\n  rapid bacterial genome annotation\nUsage:\n  prokka [options] <contigs.fasta>\nGeneral:\n  --help             This help\n  --version          Print version and exit\n  --citation         Print citation for referencing Prokka\n  --quiet            No screen output (default OFF)\n  --debug            Debug mode: keep all temporary files (default OFF)\nSetup:\n  --dbdir [X]        Prokka database root folders (default '/home/ajv37/miniconda3/envs/prokka2/db')\n  --listdb           List all configured databases\n  --setupdb          Index all installed databases\n  --cleandb          Remove all database indices\n  --depends          List all software dependencies\nOutputs:\n  --outdir [X]       Output folder [auto] (default '')\n  --force            Force overwriting existing output folder (default OFF)\n  --prefix [X]       Filename output prefix [auto] (default '')\n  --addgenes         Add 'gene' features for each 'CDS' feature (default OFF)\n  --addmrna          Add 'mRNA' features for each 'CDS' feature (default OFF)\n  --locustag [X]     Locus tag prefix [auto] (default '')\n  --increment [N]    Locus tag counter increment (default '1')\n  --gffver [N]       GFF version (default '3')\n  --compliant        Force Genbank/ENA/DDJB compliance: --addgenes --mincontiglen 200 --centre XXX (default OFF)\n  --centre [X]       Sequencing centre ID. (default '')\n  --accver [N]       Version to put in Genbank file (default '1')\nOrganism details:\n  --genus [X]        Genus name (default 'Genus')\n  --species [X]      Species name (default 'species')\n  --strain [X]       Strain name (default 'strain')\n  --plasmid [X]      Plasmid name or identifier (default '')\nAnnotations:\n  --kingdom [X]      Annotation mode: Archaea|Bacteria|Mitochondria|Viruses (default 'Bacteria')\n  --gcode [N]        Genetic code / Translation table (set if --kingdom is set) (default '0')\n  --prodigaltf [X]   Prodigal training file (default '')\n  --gram [X]         Gram: -/neg +/pos (default '')\n  --usegenus         Use genus-specific BLAST databases (needs --genus) (default OFF)\n  --proteins [X]     FASTA or GBK file to use as 1st priority (default '')\n  --hmms [X]         Trusted HMM to first annotate from (default '')\n  --metagenome       Improve gene predictions for highly fragmented genomes (default OFF)\n  --rawproduct       Do not clean up /product annotation (default OFF)\n  --cdsrnaolap       Allow [tr]RNA to overlap CDS (default OFF)\nMatching:\n  --evalue [n.n]     Similarity e-value cut-off (default '1e-09')\n  --coverage [n.n]   Minimum coverage on query protein (default '80')\nComputation:\n  --cpus [N]         Number of CPUs to use [0=all] (default '8')\n  --fast             Fast mode - only use basic BLASTP databases (default OFF)\n  --noanno           For CDS just set /product=\"unannotated protein\" (default OFF)\n  --mincontiglen [N] Minimum contig size [NCBI needs 200] (default '1')\n  --rfam             Enable searching for ncRNAs with Infernal+Rfam (SLOW!) (default '0')\n  --norrna           Don't run rRNA search (default OFF)\n  --notrna           Don't run tRNA search (default OFF)\n  --rnammer          Prefer RNAmmer over Barrnap for rRNA prediction (default OFF)\n\n\n\n\n\n\n\n\nUsage\n\n\n\nThe basic command for running Prokka is as follows:\nprokka --outdir <OUTPUT DIRECTORY> --prefix <OUTPUT PREFIX> <ASSEMBLY>\nThe meaning of the options used is:\n\n\n\nInput option\nInput required\nDescription\n\n\n\n\n–outdir\nDIRECTORY\nName of output directory\n\n\n–prefix\nPREFIX\nPrefix for output files\n\n\n\n\n\n\n\n\n\n\n\nRun Prokka\n\n\n\nLet’s run Prokka on the genome we created earlier to do produce annotation files for this sample:\nprokka --prefix ERX1275297_ERR1203055 --outdir ERX1275297_ERR1203055 ERX1275297_ERR1203055_contigs.fa\n[18:38:38] This is prokka 1.14.6\n[18:38:38] Written by Torsten Seemann <torsten.seemann@gmail.com>\n[18:38:38] Homepage is https://github.com/tseemann/prokka\n[18:38:38] Local time is Tue Jan 24 18:38:38 2023\n[18:38:38] You are ajv37\n[18:38:38] Operating system is linux\n[18:38:38] You have BioPerl 1.7.8\nArgument \"1.7.8\" isn't numeric in numeric lt (<) at /home/ajv37/miniconda3/envs/prokka2/bin/prokka line 259.\n[18:38:38] System has 64 cores.\n[18:38:38] Will use maximum of 8 cores.\n...\nThe following files will be created:\n\n\n\n\n\n\n\nOutput file\nDescription\n\n\n\n\nERX1275297_ERR1203055.tsv\nTab-separated file of all features: locus_tag,ftype,len_bp,gene,EC_number,COG,product\n\n\nERX1275297_ERR1203055.gff\nThis is the master annotation in GFF3 format, containing both sequences and annotations. It can be viewed directly in Artemis or IGV.\n\n\nERX1275297_ERR1203055.gbk\nThis is a standard Genbank file derived from the master .gff. If the input to prokka was a multi-FASTA, then this will be a multi-Genbank, with one record for each sequence.\n\n\nERX1275297_ERR1203055.sqn\nAn ASN1 format “Sequin” file for submission to Genbank. It needs to be edited to set the correct taxonomy, authors, related publication etc.\n\n\nERX1275297_ERR1203055.fna\nNucleotide FASTA file of the input contig sequences.\n\n\nERX1275297_ERR1203055.ffn\nNucleotide FASTA file of all the prediction transcripts (CDS, rRNA, tRNA, tmRNA, misc_RNA)\n\n\nERX1275297_ERR1203055.faa\nProtein FASTA file of the translated CDS sequences.\n\n\nERX1275297_ERR1203055.fsa\nNucleotide FASTA file of the input contig sequences, used by “tbl2asn” to create the .sqn file. It is mostly the same as the .fna file, but with extra Sequin tags in the sequence description lines.\n\n\nERX1275297_ERR1203055.tbl\nFeature Table file, used by “tbl2asn” to create the .sqn file.\n\n\nERX1275297_ERR1203055.txt\nStatistics relating to the annotated features found.\n\n\nERX1275297_ERR1203055.log\nContains all the output that Prokka produced during its run. This is a record of what settings you used, even if the –quiet option was enabled.\n\n\nERX1275297_ERR1203055.err\nUnacceptable annotations - the NCBI discrepancy report.\n\n\n\n\n\nThe file format most commonly used for downstream analyses such as manual curation or pan-genome analysis is the gff file. We covered gff files back in Common File Formats so you may want to remind yourself what’s inside them.\n\n\n\n\n\n\nExercise 6.1.2: Investigate the Prokka output\n\n\n\nWe can extract summary statistics for Prokka by examining the output files.\n\nHow many coding regions (CDS) did Prokka predict?\nHow many rRNA genes were identified?\nHow many repeat regions were identified?\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n\nThe easiest way to pull out the summary statistics is to examine the ERX1275297_ERR1203055.txt file:\ncat ERX1275297_ERR1203055.txt\norganism: Genus species strain \ncontigs: 227\nbases: 4397844\nCDS: 4076\nrRNA: 3\nrepeat_region: 3\ntRNA: 52\ntmRNA: 1\n\nProkka predicted 4076 CDSs in ERX1275297_ERR1203055\n3 rRNA genes were identified\n3 repeat_regions were identified\n\n\n\n\n\n\nFinally, deactivate the prokka environment:\nmamba deactivate\n\n\nDisk Usage II — Cleaning up after analysis\n\nNow that we are done investigating our assembling and annotating our genome, let’s pause again and check the space of our current working directory.\nYou can do this with the disk usage du command\ndu -h\nHow much disk space have you used since the start of the analysis?"
  },
  {
    "objectID": "materials/06-assembly_annotation/6.1-assembly_annotation.html#credit",
    "href": "materials/06-assembly_annotation/6.1-assembly_annotation.html#credit",
    "title": "6.1 Assembly and Annotation",
    "section": "Credit",
    "text": "Credit\nInformation on this page has been adapted and modified from the following source(s): https://github.com/Joseph7e/MDIBL-T3-WGS-Tutorial#genome-assembly"
  },
  {
    "objectID": "materials/07-software_management/7.1-software_management.html",
    "href": "materials/07-software_management/7.1-software_management.html",
    "title": "7.1 Managing Software",
    "section": "",
    "text": "Teaching: 60 min || Exercises: 20 min"
  },
  {
    "objectID": "materials/07-software_management/7.1-software_management.html#overview",
    "href": "materials/07-software_management/7.1-software_management.html#overview",
    "title": "7.1 Managing Software",
    "section": "Overview",
    "text": "Overview\n\n\n\n\n\n\n\n\n\n\n\n\nQuestions:\n\n\n\n\nHow do I install and manage software on my computer or a HPC?\n\n\n\n\n\n\n\n\n\nLearning Objectives:\n\n\n\n\nUnderstand what a package manager is, and how it can be used to manage software installation on a on a personal computer or HPC environment.\nInstall the Conda package manager.\nCreate a software environment and install software using Conda.\nUnderstand that mamba is a faster, more efficient version of Conda\n\n\n\n\n\n\n\n\n\nKeypoints:\n\n\n\n\nTo install your own software, you can use the Conda package manager.\nConda allows you to have separate “software environments”, where multiple package versions can co-exist on your system.\nUse conda env create <ENV> to create a new software environment and conda install -n <ENV> <PROGRAM> to install a program in that environment.\nUse conda activate <ENV> to “activate” the software environment and make all the programs installed there available.\nConda is a very useful tool but can be slow, so increasingly mamba is being used instead."
  },
  {
    "objectID": "materials/07-software_management/7.1-software_management.html#the-conda-package-manager",
    "href": "materials/07-software_management/7.1-software_management.html#the-conda-package-manager",
    "title": "7.1 Managing Software",
    "section": "7.1 The conda Package Manager",
    "text": "7.1 The conda Package Manager\nOften you may want to use software packages that are not be installed by default on your computer or the compute cluster you may have access to. There are several ways you could manage your own software installation, but in this module we will be using Conda, which gives you access to a large number of scientific packages.\nThere are two main software distributions that you can download and install, called Anaconda and Miniconda.\nMiniconda is a lighter version, which includes only base Python, while Anaconda is a much larger bundle of software that includes many other packages (see the Documentation Page for more information).\nOne of the strengths of using Conda to manage your software is that you can have different versions of your software installed alongside each other, organised in environments. Organising software packages into environments is extremely useful, as it allows to have a reproducible set of software versions that you can use and resuse in your projects.\n\n\n\nIllustration of Conda environments.\n\n\n\nInstalling Conda (Optional)\nTo start with, let’s install Conda on your machine. In this course we will install the Miniconda bundle, as it’s lighter and faster to install:\n\nMake sure you’re logged in to the HPC and in the home directory (cd ~).\nDownload the Miniconda installer by running:\n\nwget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n\nRun the installation script just downloaded:\n\nbash Miniconda3-latest-Linux-x86_64.sh\n\nFollow the installation instructions accepting default options (answering ‘yes’ to any questions)\nRun the following command:\n\nconda config --add channels defaults; conda config --add channels bioconda; conda config --add channels conda-forge; conda config --set channel_priority strict\nThis adds two channels (sources of software) useful for bioinformatics and data science applications.\n\n\n\n\n\n\nTip\n\n\n\nAnaconda and Miniconda are also available for Windows and Mac OS. See the Conda Installation Documents for instructions.\n\n\n\n\nInstalling Software Using Conda\nThe command used to install and manage software is called conda. Although we will only cover the basics in this course, it has an excellent documentation and a useful cheatsheet.\nThe first thing to do is to create a software environment for our project. Although this is optional (you could instead install everything in the “base” default environment), it is a good practice as it means the software versions remain stable within each project.\nTo create an environment we use:\nconda create --name ENV\nWhere “ENV” is the name we want to give to that environment. Once the environment is created, we can install packages using:\nconda install --name ENV PROGRAM\nWhere “PROGRAM” is the name of the software we want to install.\n\n\n\n\n\n\nTip\n\n\n\nOne way to organise your software environments is to create an environment for each kind of analysis that you might be doing regularly. For example, you could have an environment named bioinformatics with software that you use for analysing sequence data (e.g. fastQC, bwa) and another called processing with software you use for downstream analysis of your results (e.g. Python’s pandas). Alternatively, you can create a separate environment for each tool you’d like to install and use.\n\n\nTo search for the software packages that are available through conda:\n\ngo to anaconda.org.\nin the search box search for a program of your choice. For example: “bowtie2”.\nthe results should be listed as CHANNEL/PROGRAM, where CHANNEL will the the source channel from where the software is available. Usually scientific/bioinformatics software is available through the conda-forge and bioconda channels.\n\nIf you need to install a program from a different channel than the defaults, you can specify it during the install command using the -c option. For example conda install --channel CHANNEL --name ENV PROGRAM.\nLet’s see this with an example, where we create a new environment called “scipy” and install the python scientific packages:\nconda create --name scipy\nconda install --name scipy --channel conda-forge numpy matplotlib\nTo see all the environments you have available, you can use:\nconda info --env\n# conda environments:\n#\nbase                  *  /home/participant36/miniconda3\nscipy                    /home/participant36/miniconda3/envs/scipy\nIn our case it lists the base (default) environment and the newly created scipy environment. The asterisk (“*“) tells us which environment we’re using at the moment.\n\n\nLoading Conda Environments\nOnce your packages are installed in an environment, you can load that environment by using conda activate ENV, where “ENV” is the name of your environment. For example, we can activate our previously created environment with:\nconda activate scipy\nIf you check which python executable is being used now, you will notice it’s the one from this new environment:\nwhich python\n~/miniconda3/envs/scipy/bin/python\nYou can also check that the new environment is in use from:\nconda env list\n# conda environments:\n#\nbase                     /home/participant36/miniconda3\nscipy                 *  /home/participant36/miniconda3/envs/scipy\nAnd notice that the asterisk “*” is now showing we’re using the scipy environment."
  },
  {
    "objectID": "materials/07-software_management/7.1-software_management.html#replacing-conda-with-mamba",
    "href": "materials/07-software_management/7.1-software_management.html#replacing-conda-with-mamba",
    "title": "7.1 Managing Software",
    "section": "7.2 Replacing conda with Mamba",
    "text": "7.2 Replacing conda with Mamba\nconda is an amazing tool which has completely changed the way we install tools, removing the worst of the hassle around ensuring that dependencies are installed alongside the tools. However, conda can be quite slow and sometimes has difficulties resolving the dependencies. Fortunately, Mamba was developed to speed up the process. Mamba is a reimplementation of the conda package manager in C++ and allows you to use exactly the same commands as conda (simply replace conda with mamba). Unlike, the tools we’ve been installing above in their own environments, Mamba needs to be installed in the conda base environment. Let’s try installing it now:\nconda install mamba -n base -c conda-forge\nOnce it’s installed let’s try creating a new environment called bakta and then install the Bakta annotation tool we’re going to use in the Assembly and annotation module.\nmamba create -n bakta \n\nmamba activate bakta\n\nmamba install -c bioconda bakta\nLet’s check that we successfully installed bakta:\nbakta -h\nIf you get the bakta help output, congratulations you’ve successfully installed a tool with Mamba.\nYou can see the commands we use with Mamba are exactly the same as conda but the output looks very different. Don’t worry it’s doing exactly the same thing as conda but better!\n\n\n\n\n\n\nExercise 7.1.1: Create a conda environment\n\n\n\nIn the workshop_files_Bact_Genomics_2023/07_software_management folder, you will find the same files we used in the short read mapping module. Your objective will be to prepare to align the sequences to the MTB H37v reference genome, using a software called bowtie2.\n\nFirst, we need to prepare our genome for this alignment procedure (this is referred to as indexing the genome).\n\nCreate a new conda environment named “bowtie”.\nInstall the bowtie2 program in your new environment.\nCheck that the software installed correctly by running which bowtie2 and bowtie2 --help.\nIndex the reference genome with bowtie2\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n\n\nTo create a new conda environment we run:\n\nmamba create --name bowtie2\n\nIf we search for this software on the Anaconda website, we will find that it is available via the bioconda channel: https://anaconda.org/bioconda/bowtie2\n\nWe can install it on our environment with:\nmamba install --name bowtie2 --channel bioconda bowtie2\n\nFirst we need to activate our environment:\n\nmamba activate bioinformatics\nThen, if we run bowtie2 --help, we should get the software help printed on the console.\n\nNow let’s index our reference genome with bowtie2\n\nbowtie2-build MTB_H37Rv.fasta index\nWe should get several output files in the 07_software_management directory with an extension “.bt2”:\nls 07_software_management\nindex.1.bt2\nindex.2.bt2\nindex.3.bt2\nindex.4.bt2\nindex.rev.1.bt2\nindex.rev.2.bt2\n\n\n\n\n\n\nFurther resources\n\nSearch for Conda packages at anaconda.org\nLearn more about Conda from the Conda User Guide\nConda Cheatsheet (PDF)"
  },
  {
    "objectID": "materials/07-software_management/7.1-software_management.html#credit",
    "href": "materials/07-software_management/7.1-software_management.html#credit",
    "title": "7.1 Managing Software",
    "section": "Credit",
    "text": "Credit\nInformation on this page has been adapted and modified from the following source(s): https://github.com/cambiotraining/hpc-intro-sanger/blob/main/04-software.md"
  },
  {
    "objectID": "materials/08-workflow_managers/8.1-workflow_managers.html",
    "href": "materials/08-workflow_managers/8.1-workflow_managers.html",
    "title": "8.1 Workflow Managers",
    "section": "",
    "text": "Teaching: 40 min || Exercises: 20 min"
  },
  {
    "objectID": "materials/08-workflow_managers/8.1-workflow_managers.html#overview",
    "href": "materials/08-workflow_managers/8.1-workflow_managers.html#overview",
    "title": "8.1 Workflow Managers",
    "section": "Overview",
    "text": "Overview\n\n\n\n\n\n\n\n\n\n\n\n\nQuestions:\n\n\n\n\nWhat is a workflow and what are workflow management systems?\nWhy should I use a workflow management system?\nWhat is Nextflow?\nWhat are the main features of Nextflow?\nWhat are the main components of a Nextflow script?\nHow do I run a Nextflow script?\nWhat is Snakemake?\n\n\n\n\n\n\n\n\n\nLearning Objectives:\n\n\n\n\nUnderstand what a workflow management system is.\nUnderstand the benefits of using a workflow management system.\nExplain the benefits of using Nextflow as part of your bioinformatics workflow.\nExplain the components of a Nextflow script.\nRun a Nextflow script.\n\n\n\n\n\n\n\n\n\nKeypoints:\n\n\n\n\nA workflow is a sequence of tasks that process a set of data.\nA workflow management system (WfMS) is a computational platform that provides an infrastructure for the set-up, execution and monitoring of workflows.\nNextflow is a workflow management system that comprises both a runtime environment and a domain specific language (DSL).\nNextflow scripts comprise of channels for controlling inputs and outputs, and processes for defining workflow tasks.\nYou run a Nextflow script using the nextflow run command."
  },
  {
    "objectID": "materials/08-workflow_managers/8.1-workflow_managers.html#workflows",
    "href": "materials/08-workflow_managers/8.1-workflow_managers.html#workflows",
    "title": "8.1 Workflow Managers",
    "section": "8.1 Workflows",
    "text": "8.1 Workflows\nAnalysing data involves a sequence of tasks, including gathering, cleaning, and processing data. These sequence of tasks are called a workflow or a pipeline. These workflows typically require executing multiple software packages, sometimes running on different computing environments, such as a desktop or a compute cluster. Traditionally these workflows have been joined together in scripts using general purpose programming languages such as Bash or Python.\n\n\n\nExample bioinformatics variant calling workflow/pipeline diagram from nf-core (https://nf-co.re/bactmap).\n\n\nHowever, as workflows become larger and more complex, the management of the programming logic and software becomes difficult."
  },
  {
    "objectID": "materials/08-workflow_managers/8.1-workflow_managers.html#workflow-management-systems",
    "href": "materials/08-workflow_managers/8.1-workflow_managers.html#workflow-management-systems",
    "title": "8.1 Workflow Managers",
    "section": "8.2 Workflow management systems",
    "text": "8.2 Workflow management systems\nWorkflow Management Systems (WfMS), such as Snakemake, Galaxy, and Nextflow have been developed specifically to manage computational data-analysis workflows in fields such as Bioinformatics, Imaging, Physics, and Chemistry.\nWfMS contain multiple features that simplify the development, monitoring, execution and sharing of pipelines.\nKey features include;\n\nRun time management: Management of program execution on the operating system and splitting tasks and data to run at the same time in a process called parallelisation.\nSoftware management: Use of technology like containers, such as Docker or Singularity, that packages up code and all its dependencies so the application runs reliably from one computing environment to another.\nPortability & Interoperability: Workflows written on one system can be run on another computing infrastructure e.g., local computer, compute cluster, or cloud infrastructure.\nReproducibility: The use of software management systems and a pipeline specification means that the workflow will produce the same results when re-run, including on different computing platforms.\nReentrancy: Continuous checkpoints allow workflows to resume from the last successfully executed steps."
  },
  {
    "objectID": "materials/08-workflow_managers/8.1-workflow_managers.html#nextflow-basic-concepts",
    "href": "materials/08-workflow_managers/8.1-workflow_managers.html#nextflow-basic-concepts",
    "title": "8.1 Workflow Managers",
    "section": "8.3 Nextflow basic concepts",
    "text": "8.3 Nextflow basic concepts\nNextflow is a workflow management system that combines a runtime environment, software that is designed to run other software, and a programming domain specific language (DSL) that eases the writing of computational pipelines.\nNextflow is built around the idea that Linux is the lingua franca of data science. Nextflow follows Linux’s “small pieces loosely joined” philosophy: in which many simple but powerful command-line and scripting tools, when chained together, facilitate more complex data manipulations.\nNextflow extends this approach, adding the ability to define complex program interactions and an accessible (high-level) parallel computational environment based on the dataflow programming model, whereby processes are connected via their outputs and inputs to other processes, and run as soon as they receive an input. The diagram below illustrates the differences between a dataflow model and a simple linear program .\n\n\n\nA simple program (a) and its dataflow equivalent (b) https://doi.org/10.1145/1013208.1013209.\n\n\nIn a simple program (a), these statements would be executed sequentially. Thus, the program would execute in three units of time. In the dataflow programming model (b), this program takes only two units of time. This is because the read quantitation and QC steps have no dependencies on each other and therefore can execute simultaneously in parallel.\n\nNextflow core features\n\nFast prototyping: A simple syntax for writing pipelines that enables you to reuse existing scripts and tools for fast prototyping.\nReproducibility: Nextflow supports several container technologies, such as Docker and Singularity, as well as the package manager Conda. This, along with the integration of the GitHub code sharing platform, allows you to write self-contained pipelines, manage versions and to reproduce any former configuration.\nPortability: Nextflow’s syntax separates the functional logic (the steps of the workflow) from the execution settings (how the workflow is executed). This allows the pipeline to be run on multiple platforms, e.g. local compute vs. a university compute cluster or a cloud service like AWS, without changing the steps of the workflow.\nSimple parallelism: Nextflow is based on the dataflow programming model which greatly simplifies the splitting of tasks that can be run at the same time (parallelisation).\nContinuous checkpoints: All the intermediate results produced during the pipeline execution are automatically tracked. This allows you to resume its execution from the last successfully executed step, no matter what the reason was for it stopping.\n\n\n\nScripting language\nNextflow scripts are written using a language intended to simplify the writing of workflows. Languages written for a specific field are called Domain Specific Languages (DSL), e.g., SQL is used to work with databases, and AWK is designed for text processing.\nIn practical terms the Nextflow scripting language is an extension of the Groovy programming language, which in turn is a super-set of the Java programming language. Groovy simplifies the writing of code and is more approachable than Java. Groovy semantics (syntax, control structures, etc) are documented here.\nThe approach of having a simple DSL built on top of a more powerful general purpose programming language makes Nextflow very flexible. The Nextflow syntax can handle most workflow use cases with ease, and then Groovy can be used to handle corner cases which may be difficult to implement using the DSL.\n\n\nDSL2 syntax\nNextflow (version > 20.07.1) provides a revised syntax to the original DSL, known as DSL2. The DSL2 syntax introduces several improvements such as modularity (separating components to provide flexibility and enable reuse), and improved data flow manipulation. This further simplifies the writing of complex data analysis pipelines, and enhances workflow readability, and reusability.\nThis feature is enabled by the following directive at the beginning a workflow script:\nnextflow.enable.dsl=2\n\n\n\n\n\n\nEarlier syntax versions\n\n\n\nScripts that contain the directive nextflow.preview.dsl=2 use an early version of the DSL2 syntax, which may include experimental features that have been changed or removed in the formal DSL2 syntax. Scripts without these directives use the first version of the Nextflow syntax which we refer to as DSL1. DSL1 workflows use many of the same concepts presented in this lesson, but some aspects such as the flow of data are written differently. DSL1 workflows are also written in a single script, unlike DSL2 workflows which can be spread across many files. This lesson will focus on the DSL2 syntax as, after the DSL1 to DSL2 transition period is over, it will become the default way of writing Nextflow workflows.\n\n\n\n\nProcesses, channels, and workflows\nNextflow workflows have three main parts; processes, channels, and workflows. Processes describe a task to be run. A process script can be written in any scripting language that can be executed by the Linux platform (Bash, Perl, Ruby, Python, etc.). Processes spawn a task for each complete input set. Each task is executed independently, and cannot interact with another task. The only way data can be passed between process tasks is via asynchronous queues, called channels.\nProcesses define inputs and outputs for a task. Channels are then used to manipulate the flow of data from one process to the next. The interaction between processes, and ultimately the pipeline execution flow itself, is then explicitly defined in a workflow section.\nIn the following example we have a channel containing three elements, e.g., 3 data files. We have a process that takes the channel as input. Since the channel has three elements, three independent instances (tasks) of that process are run in parallel. Each task generates an output, which is passed to another channel and used as input for the next process.\n\n\n\nProcesses and channels.\n\n\n\n\nWorkflow execution\nWhile a process defines what command or script has to be executed, the executor determines how that script is actually run in the target system.\nIf not otherwise specified, processes are executed on the local computer. The local executor is very useful for pipeline development, testing, and small scale workflows, but for large scale computational pipelines, a High Performance Cluster (HPC) or Cloud platform is often required.\n\n\n\nProcesses and channels.\n\n\nNextflow provides a separation between the pipeline’s functional logic and the underlying execution platform. This makes it possible to write a pipeline once, and then run it on your computer, compute cluster, or the cloud, without modifying the workflow, by defining the target execution platform in a configuration file.\nNextflow provides out-of-the-box support for major batch schedulers and cloud platforms such as Sun Grid Engine, SLURM job scheduler, AWS Batch service and Kubernetes. A full list can be found here.\n\n\nInstall Nextflow\nBefore we continue with this module, we need to install Nextflow. The best way to do this is to create a Conda environment called Nextflow and install it there (to save time we’ve already done this for you):\nmamba create -n nextflow\nNow activate the nextflow environment and install nextflow:\nmamba activate nextflow\n\nmamba install -c bioconda nextflow\n\n\nYour first script\nWe are now going to look at a sample Nextflow script that counts the number of lines in a file.\nOpen the file wc.nf in the workshop_files_Bact_Genomics_2023/08_workflow_management directory with your favourite text editor.\nThis is a Nextflow script. It contains;\n\nAn optional interpreter directive (“Shebang”) line, specifying the location of the Nextflow interpreter.\nnextflow.enable.dsl=2 to enable DSL2 syntax.\nA multi-line Nextflow comment, written using C style block comments, followed by a single line comment.\nA pipeline parameter params.input which is given a default value, of the relative path to the location of a compressed fastq file, as a string.\nAn unnamed workflow execution block, which is the default workflow to run.\nA Nextflow channel used to read in data to the workflow.\nA call to the process NUM_LINES.\nAn operation on the process output, using the channel operator view().\nA Nextflow process block named NUM_LINES, which defines what the process does.\nAn input definition block that assigns the input to the variable read, and declares that it should be interpreted as a file path.\nAn output definition block that uses the Linux/Unix standard output stream stdout from the script block.\nA script block that contains the bash commands printf '${read} to print the name of the read file, and gunzip -c ${read} | wc -l to count the number of lines in the gzipped read file.\n\n#!/usr/bin/env nextflow\n\nnextflow.enable.dsl=2\n\n/*  Comments are uninterpreted text included with the script.\n    They are useful for describing complex parts of the workflow\n    or providing useful information such as workflow usage.\n\n    Usage:\n       nextflow run wc.nf --input <input_file>\n\n    Multi-line comments start with a slash asterisk /* and finish with an asterisk slash. */\n//  Single line comments start with a double slash // and finish on the same line\n\n/*  Workflow parameters are written as params.<parameter>\n    and can be initialised using the `=` operator. */\nparams.input = \"data/yeast/reads/ref1_1.fq.gz\"\n\n//  The default workflow\nworkflow {\n\n    //  Input data is received through channels\n    input_ch = Channel.fromPath(params.input)\n\n    /*  The script to execute is called by its process name,\n        and input is provided between brackets. */\n    NUM_LINES(input_ch)\n\n    /*  Process output is accessed using the `out` channel.\n        The channel operator view() is used to print\n        process output to the terminal. */\n    NUM_LINES.out.view()\n}\n\n/*  A Nextflow process block\n    Process names are written, by convention, in uppercase.\n    This convention is used to enhance workflow readability. */\nprocess NUM_LINES {\n\n    input:\n    path read\n\n    output:\n    stdout\n\n    script:\n    /* Triple quote syntax \"\"\", Triple-single-quoted strings may span multiple lines. The content of the string can cross line boundaries without the need to split the string in several pieces and without concatenation or newline escape characters. */\n    \"\"\"\n    printf '${read} '\n    gunzip -c ${read} | wc -l\n    \"\"\"\n}\n\n\n\n\n\n\nExercise 7.1.1: Run a Nextflow script\n\n\n\nTo run a Nextflow script use the command nextflow run <script_name>. Run the script by entering the following command in your terminal:\nnextflow run wc.nf\n\n\n\n\n\n\nSolution:\n\n\n\n\n\nYou should see output similar to the text shown below:\nN E X T F L O W  ~  version 20.10.0\nLaunching `wc.nf` [fervent_babbage] - revision: c54a707593\nexecutor >  local (1)\n[21/b259be] process > NUM_LINES (1) [100%] 1 of 1 ✔\n\nref1_1.fq.gz 58708\n\nThe first line shows the Nextflow version number.\nThe second line shows the run name fervent_babbage (adjective and scientist name) and revision id c54a707593.\nThe third line tells you the process has been executed locally (executor >  local).\nThe next line shows the process id 21/b259be, process name, number of cpus, percentage task completion, and how many instances of the process have been run.\nThe final line is the output of the view operator.\n\n\nProcess identification\nThe hexadecimal numbers, like 61/1f3ef4, identify the unique process execution These numbers are also the prefix of the directories where each process is executed. You can inspect the files produced by changing to the directory $PWD/work and using these numbers to find the process-specific execution path."
  },
  {
    "objectID": "materials/08-workflow_managers/8.1-workflow_managers.html#process-identification",
    "href": "materials/08-workflow_managers/8.1-workflow_managers.html#process-identification",
    "title": "8.1 Workflow Managers",
    "section": "Process identification",
    "text": "Process identification\nThe hexadecimal numbers, like 61/1f3ef4, identify the unique process execution These numbers are also the prefix of the directories where each process is executed. You can inspect the files produced by changing to the directory $PWD/work and using these numbers to find the process-specific execution path."
  },
  {
    "objectID": "materials/08-workflow_managers/8.1-workflow_managers.html#snakemake",
    "href": "materials/08-workflow_managers/8.1-workflow_managers.html#snakemake",
    "title": "8.1 Workflow Managers",
    "section": "8.4 Snakemake",
    "text": "8.4 Snakemake\nIn this tutorial we’ve focused on Nextflow but many people in the bioinformatics community use Snakemake. Similar to Nextflow, the Snakemake workflow management system is a tool for creating reproducible and scalable data analyses. The main difference is that workflows are described via a human readable, Python based language. They can be seamlessly scaled to server, cluster, grid and cloud environments, without the need to modify the workflow definition. Finally, Snakemake workflows can entail a description of required software, which will be automatically deployed to any execution environment."
  },
  {
    "objectID": "materials/08-workflow_managers/8.1-workflow_managers.html#credit",
    "href": "materials/08-workflow_managers/8.1-workflow_managers.html#credit",
    "title": "8.1 Workflow Managers",
    "section": "Credit",
    "text": "Credit\nInformation on this page has been adapted and modified from the following source(s):\n\nGraeme R. Grimes, Evan Floden, Paolo Di Tommaso, Phil Ewels and Maxime Garcia. Introduction to Workflows with Nextflow and nf-core. https://github.com/carpentries-incubator/workflows-nextflow 2021."
  },
  {
    "objectID": "materials/08-workflow_managers/8.2-nfcore.html",
    "href": "materials/08-workflow_managers/8.2-nfcore.html",
    "title": "8.2 nf-core pipelines",
    "section": "",
    "text": "Teaching: 45 min || Exercises: 15 min"
  },
  {
    "objectID": "materials/08-workflow_managers/8.2-nfcore.html#overview",
    "href": "materials/08-workflow_managers/8.2-nfcore.html#overview",
    "title": "8.2 nf-core pipelines",
    "section": "Overview",
    "text": "Overview\n\n\n\n\n\n\n\n\n\n\n\n\nQuestions:\n\n\n\n\nWhere can I find best-practice Nextflow bioinformatic pipelines?\nHow do I run nf-core pipelines?\nHow do I configure nf-core pipelines to use my data?\nHow do I reference nf-core pipelines?\n\n\n\n\n\n\n\n\n\nLearning Objectives:\n\n\n\n\nUnderstand what nf-core is and how it relates to Nextflow.\nUse the nf-core helper tool to find nf-core pipelines.\nUnderstand how to configuration nf-core pipelines.\nRun a small nf-core pipeline using a test dataset.\n\n\n\n\n\n\n\n\n\nKeypoints:\n\n\n\n\nnf-core is a community-led project to develop a set of best-practice pipelines built using the Nextflow workflow management system.\nThe nf-core tool (nf-core) is a suite of helper tools that aims to help people run and develop nf-core pipelines.\nnf-core pipelines can be found using nf-core list, or by checking the nf-core website.\nnf-core launch nf-core/<pipeline> can be used to write a parameter file for an nf-core pipeline. This can be supplied to the pipeline using the -params-file option.\nAn nf-core workflow is run using nextflow run nf-core/<pipeline> syntax.\nnf-core pipelines can be reconfigured by using custom config files and/or adding command line parameters."
  },
  {
    "objectID": "materials/08-workflow_managers/8.2-nfcore.html#what-is-nf-core",
    "href": "materials/08-workflow_managers/8.2-nfcore.html#what-is-nf-core",
    "title": "8.2 nf-core pipelines",
    "section": "8.1 What is nf-core?",
    "text": "8.1 What is nf-core?\nnf-core is a community-led project to develop a set of best-practice pipelines built using Nextflow workflow management system. Pipelines are governed by a set of guidelines, enforced by community code reviews and automatic code testing.\n\n\n\nnf-core"
  },
  {
    "objectID": "materials/08-workflow_managers/8.2-nfcore.html#what-are-nf-core-pipelines",
    "href": "materials/08-workflow_managers/8.2-nfcore.html#what-are-nf-core-pipelines",
    "title": "8.2 nf-core pipelines",
    "section": "8.2 What are nf-core pipelines?",
    "text": "8.2 What are nf-core pipelines?\nnf-core pipelines are an organised collection of Nextflow scripts, other non-nextflow scripts (written in any language), configuration files, software specifications, and documentation hosted on GitHub. There is generally a single pipeline for a given data and analysis type e.g. There is a single pipeline for bulk RNA-Seq. All nf-core pipelines are distributed under the, permissive free software, MIT licences."
  },
  {
    "objectID": "materials/08-workflow_managers/8.2-nfcore.html#running-nf-core-pipelines",
    "href": "materials/08-workflow_managers/8.2-nfcore.html#running-nf-core-pipelines",
    "title": "8.2 nf-core pipelines",
    "section": "8.3 Running nf-core pipelines",
    "text": "8.3 Running nf-core pipelines\n\nSoftware requirements for nf-core pipelines\nnf-core pipeline software dependencies are specified using either Docker, Singularity or Conda. It is Nextflow that handles the downloading of containers and creation of conda environments. In theory it is possible to run the pipelines with software installed by other methods (e.g. environment modules, or manual installation), but this is not recommended.\n\n\nFetching pipeline code\nUnless you are actively developing pipeline code, you should use Nextflow’s built-in functionality to fetch nf-core pipelines. You can use the following command to pull the latest version of a remote workflow from the nf-core github site.;\nnextflow pull nf-core/<PIPELINE>\nNextflow will also automatically fetch the pipeline code when you run\n`bash nextflow run nf-core/<pipeline>.\n\nFor the best reproducibility, it is good to explicitly reference the pipeline version number that you wish to use with the `-revision`/`-r` flag.\n\nIn the example below we are pulling the rnaseq pipeline version 3.0\n\n```bash\n$ nextflow pull nf-core/rnaseq -revision 3.0"
  },
  {
    "objectID": "materials/08-workflow_managers/8.2-nfcore.html#usage-instructions-and-documentation",
    "href": "materials/08-workflow_managers/8.2-nfcore.html#usage-instructions-and-documentation",
    "title": "8.2 nf-core pipelines",
    "section": "8.4 Usage instructions and documentation",
    "text": "8.4 Usage instructions and documentation\nYou can find general documentation and instructions for Nextflow and nf-core on the nf-core website . Pipeline-specific documentation is bundled with each pipeline in the /docs folder. This can be read either locally, on GitHub, or on the nf-core website.\nEach pipeline has its own webpage e.g. nf-co.re/rnaseq.\nIn addition to this documentation, each pipeline comes with basic command line reference. This can be seen by running the pipeline with the parameter --help , for example:\nnextflow run -r 3.4 nf-core/rnaseq --help\nN E X T F L O W  ~  version 20.10.0\nLaunching `nf-core/rnaseq` [silly_miescher] - revision: 964425e3fd [3.4]\n------------------------------------------------------\n                                        ,--./,-.\n        ___     __   __   __   ___     /,-._.--~'\n  |\\ | |__  __ /  ` /  \\ |__) |__         }  {\n  | \\| |       \\__, \\__/ |  \\ |___     \\`-._,-`-,\n                                        `._,._,'\n  nf-core/rnaseq v3.0\n------------------------------------------------------\n\nTypical pipeline command:\n\n    nextflow run nf-core/rnaseq --input samplesheet.csv --genome GRCh37 -profile docker\n\nInput/output options\n    --input                             [string]  Path to comma-separated file containing information about the samples in the experiment.\n    --outdir                            [string]  Path to the output directory where the results will be saved.\n    --public_data_ids                   [string]  File containing SRA/ENA/GEO identifiers one per line in order to download their associated FastQ files.\n    --email                             [string]  Email address for completion summary.\n    --multiqc_title                     [string]  MultiQC report title. Printed as page header, used for filename if not otherwise specified.\n    --skip_sra_fastq_download           [boolean] Only download metadata for public data database ids and don't download the FastQ files.\n    --save_merged_fastq                 [boolean] Save FastQ files after merging re-sequenced libraries in the results directory.\n..truncated..\n\nThe nf-core launch command\nAs can be seen from the output of the help option nf-core pipelines have a number of flags that need to be passed on the command line: some mandatory, some optional.\nTo make it easier to launch pipelines, these parameters are described in a JSON file, nextflow_schema.json bundled with the pipeline.\nThe nf-core launch command uses this to build an interactive command-line wizard which walks through the different options with descriptions of each, showing the default value and prompting for values.\nOnce all prompts have been answered, non-default values are saved to a params.json file which can be supplied to Nextflow using the -params-file option. Optionally, the Nextflow command can be launched there and then.\nTo use the launch feature, just specify the pipeline name:\nnf-core launch -r 3.0 rnaseq\n\n\n\n\n\n\nExercise 8.2.1: Run a Nextflow script\n\n\n\nUse the nf-core launch command to create a params file named nf-params.json. 1. Use the nf-core launch command to launch the interactive command-line wizard. 1. Add an input file name samples.csv 1. Add a genome GRCh38 Note : Do not run the command now.\n\n\n\n\n\n\nSolution:\n\n\n\n\n\nnf-core launch rnaseq\nThe contents of the nf-params.json file should be\n{\n  \"input\": \"samples.csv\",\n  \"genome\": \"GRCh38\"\n}\n\n\n\n\n\n\n\nConfig files\nnf-core pipelines make use of Nextflow’s configuration files to specify how the pipelines runs, define custom parameters and what software management system to use e.g. docker, singularity or conda.\nNextflow can load pipeline configurations from multiple locations. nf-core pipelines load configuration in the following order:\n\n\n\nconfig\n\n\n\nPipeline: Default ‘base’ config\n\n\nAlways loaded. Contains pipeline-specific parameters and “sensible defaults” for things like computational requirements\nDoes not specify any method for software packaging. If nothing else is specified, Nextflow will expect all software to be available on the command line.\n\n\nCore config profiles\n\n\nAll nf-core pipelines come with some generic config profiles. The most commonly used ones are for software packaging: docker, singularity and conda\nOther core profiles are debug and two test profiles. There two test profile, a small test profile (nf-core/test-datasets) for quick test and a full test profile which provides the path to full sized data from public repositories.\n\n\nServer profiles\n\n\nAt run time, nf-core pipelines fetch configuration profiles from the configs remote repository. The profiles here are specific to clusters at different institutions.\nBecause this is loaded at run time, anyone can add a profile here for their system and it will be immediately available for all nf-core pipelines.\n\n\nLocal config files given to Nextflow with the -c flag\n\nnextflow run nf-core/rnaseq -r 3.0 -c mylocal.config\n\nCommand line configuration: pipeline parameters can be passed on the command line using the --<parameter> syntax.\n\nnextflow run nf-core/rnaseq -r 3.0 --email \"my@email.com\"`\n\n\nConfig Profiles\nnf-core makes use of Nextflow configuration profiles to make it easy to apply a group of options on the command line.\nConfiguration files can contain the definition of one or more profiles. A profile is a set of configuration attributes that can be activated/chosen when launching a pipeline execution by using the -profile command line option. Common profiles are conda, singularity and docker that specify which software manager to use.\nMultiple profiles are comma-separated. When there are differing configuration settings provided by different profiles, the right-most profile takes priority.\nnextflow run nf-core/rnaseq -r 3.0 -profile test,conda\nnextflow run nf-core/rnaseq -r 3.0 -profile <institutional_config_profile>, test, conda\nNote The order in which config profiles are specified matters. Their priority increases from left to right.\n\n\nMultiple Nextflow configuration locations\nBe clever with multiple Nextflow configuration locations. For example, use -profile for your cluster configuration, the file $HOME/.nextflow/config for your personal config such as params.email and a working directory >nextflow.config file for reproducible run-specific configuration.\n\n\n\n\n\n\nExercise 8.2.2: create a custom config\n\n\n\nAdd the params.email to a file called nfcore-custom.config\n\n\n\n\n\n\nSolution\n\n\n\n\n\nA line similar to one below in the file custom.config\nparams.email = \"myemail@address.com\"\n\n\n\n\n\n\n\nRunning pipelines with test data\nThe nf-core config profile test is special profile, which defines a minimal data set and configuration, that runs quickly and tests the workflow from beginning to end. Since the data is minimal, the output is often nonsense. Real world example output are instead linked on the nf-core pipeline web page, where the workflow has been run with a full size data set:\n$ nextflow run nf-core/<pipeline_name-profile test\n\n\n\n\n\n\nSoftware configuration profile\n\n\n\nNote that you will typically still need to combine this with a software configuration profile for your system - e.g. -profile test,conda. Running with the test profile is a great way to confirm that you have Nextflow configured properly for your system before attempting to run with real data"
  },
  {
    "objectID": "materials/08-workflow_managers/8.2-nfcore.html#using-nf-core-pipelines-offline",
    "href": "materials/08-workflow_managers/8.2-nfcore.html#using-nf-core-pipelines-offline",
    "title": "8.2 nf-core pipelines",
    "section": "8.5 Using nf-core pipelines offline",
    "text": "8.5 Using nf-core pipelines offline\nMany of the techniques and resources described above require an active internet connection at run time - pipeline files, configuration profiles and software containers are all dynamically fetched when the pipeline is launched. This can be a problem for people using secure computing resources that do not have connections to the internet.\nTo help with this, the nf-core download command automates the fetching of required files for running nf-core pipelines offline. The command can download a specific release of a pipeline with -r/--release .\nBy default, the pipeline will download the pipeline code and the institutional nf-core/configs files.\nIf you specify the flag --singularity, it will also download any singularity image files that are required (this needs Singularity to be installed). All files are saved to a single directory, ready to be transferred to the cluster where the pipeline will be executed.\n$ nf-core download nf-core/rnaseq -r 3.4\n\n                                          ,--./,-.\n          ___     __   __   __   ___     /,-._.--~\\\n    |\\ | |__  __ /  ` /  \\ |__) |__         }  {\n    | \\| |       \\__, \\__/ |  \\ |___     \\`-._,-`-,\n                                          `._,._,'\n\n    nf-core/tools version 2.1\n\n\n\nINFO     Saving nf-core/rnaseq                                                                                                                                                                                                                                         download.py:148\n          Pipeline release: '3.4'\n          Pull singularity containers: 'No'\n          Output file: 'nf-core-rnaseq-3.4.tar.gz'\nINFO     Downloading workflow files from GitHub                                                                                                                                                                                                                        download.py:151\nINFO     Downloading centralised configs from GitHub                                                                                                                                                                                                                   download.py:155\nINFO     Compressing download..                                                                                                                                                                                                                                        download.py:166\nINFO     Command to extract files: tar -xzf nf-core-rnaseq-3.4.tar.gz                                                                                                                                                                                                  download.py:653\nINFO     MD5 checksum for nf-core-rnaseq-3.4.tar.gz: f0e0c239bdb39c613d6a080f1dee88e9\n\n\n\n\n\n\nExercise 7.2.2: Run a test nf-core pipeline\n\n\n\nRun the nf-core/hlatyping pipeline release 1.2.0 with the provided test data using the profile test and conda. Add the parameter --max_memory 3G on the command line. Include the config file, nfcore-custom.config, from the previous exercise using the option -c, to send an email when your pipeline finishes.\n$ nextflow run nf-core/hlatyping -r 1.2.0 -profile test,conda  --max_memory 3G -c nfcore-custom.config\nThe pipeline does next-generation sequencing-based Human Leukozyte Antigen (HLA) typing and should run quickly.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n$ nextflow run nf-core/hlatyping -r 1.2.0 -profile test,conda  --max_memory 3G\n N E X T F L O W  ~  version 21.04.0\nLaunching `nf-core/hlatyping` [pedantic_engelbart] - revision: 6998794795 [1.2.0]\nBAM file format detected. Initiate remapping to HLA alleles with yara mapper.\n----------------------------------------------------\n                                       ,--./,-.\n       ___     __   __   __   ___     /,-._.--~'\n |\\ | |__  __ /  ` /  \\ |__) |__         }  {\n | \\| |       \\__, \\__/ |  \\ |___     \\`-._,-`-,\n                                       `._,._,'\nnf-core/hlatyping v1.2.0\n----------------------------------------------------\n>\n>Pipeline Release  : 1.2.0\n>Run Name          : pedantic_engelbart\n>File Type         : BAM\n>Seq Type          : dna\n>Index Location    : /home/training/.nextflow/assets/nf-core/hlatyping/data/indices/yara/hla_reference_dna\n>IP Solver         : glpk\n>Enumerations      : 1\n>Beta              : 0.009\n>Max Memory        : 3G\n>Max CPUs          : 2\n>Max Time          : 2d\n>Input             : https://github.com/nf-core/test-datasets/raw/hlatyping/bam/example_pe.bam\n>Data Type         : Paired-End\n>Output Dir        : results\n>Launch Dir        : /home/training\n>Working Dir       : /home/training/work\n>Script Dir        : /home/training/.nextflow/assets/nf-core/hlatyping\n>User              : training\n>Max Resources     : 3G memory, 2 cpus, 2d time per job\n>Config Profile    : conda,test\n>Config Profile Description: Minimal test dataset to check pipeline function\n>Config Files      : /home/training/.nextflow/assets/nf-core/hlatyping/nextflow.config, /home/training/nextflow.config, /home/training/.nextflow/assets/nf->core/hlatyping/nextflow.config\n>----------------------------------------------------\n>BAM file format detected. Initiate remapping to HLA alleles with yara mapper.\n>[-        ] process remap_to_hla          -\n>executor local (6)\n>[05/084b41] process remap_to_hla (1)      [100%] 1 of 1 ✔\n>[5a/9bec8b] process make_ot_config        [100%] 1 of 1 ✔\n>[54/8bc5d7] process run_optitype (1)      [100%] 1 of 1 ✔\n>[a9/2cbea8] process output_documentation  [100%] 1 of 1 ✔\n>[df/d3dac5] process get_software_versions [100%] 1 of 1 ✔\n>[e1/903ed9] process multiqc (1)           [100%] 1 of 1 ✔\n>-[nf-core/hlatyping] Pipeline completed successfully-\n>WARN: To render the execution DAG in the required format it is required to install Graphviz -- See http://www.graphviz.org for more info.\n>Completed at: 26-Oct-2021 10:07:27\n>Duration    : 4m 14s\n>CPU hours   : (a few seconds)\n>Succeeded   : 6"
  },
  {
    "objectID": "materials/08-workflow_managers/8.2-nfcore.html#troubleshooting",
    "href": "materials/08-workflow_managers/8.2-nfcore.html#troubleshooting",
    "title": "8.2 nf-core pipelines",
    "section": "8.6 Troubleshooting",
    "text": "8.6 Troubleshooting\nIf you run into issues running your pipeline you can you the nf-core website to troubleshoot common mistakes and issues https://nf-co.re/usage/troubleshooting .\n\nExtra resources and getting help\nIf you still have an issue with running the pipeline then feel free to contact the nf-core community via the Slack channel . The nf-core Slack organisation has channels dedicated for each pipeline, as well as specific topics (eg. #help, #pipelines, #tools, #configs and much more). The nf-core Slack can be found at https://nfcore.slack.com (NB: no hyphen in nfcore!). To join you will need an invite, which you can get at https://nf-co.re/join/slack.\nYou can also get help by opening an issue in the respective pipeline repository on GitHub asking for help.\nIf you have problems that are directly related to Nextflow and not our pipelines or the nf-core framework tools then check out the Nextflow gitter channel or the google group."
  },
  {
    "objectID": "materials/08-workflow_managers/8.2-nfcore.html#referencing-a-pipeline",
    "href": "materials/08-workflow_managers/8.2-nfcore.html#referencing-a-pipeline",
    "title": "8.2 nf-core pipelines",
    "section": "8.7 Referencing a Pipeline",
    "text": "8.7 Referencing a Pipeline\n\nPublications\nIf you use an nf-core pipeline in your work you should cite the main publication for the main nf-core paper, describing the community and framework, as follows:\nThe nf-core framework for community-curated bioinformatics pipelines. Philip Ewels, Alexander Peltzer, Sven Fillinger, Harshil Patel, Johannes Alneberg, Andreas Wilm, Maxime Ulysse Garcia, Paolo Di Tommaso & Sven Nahnsen. Nat Biotechnol. 2020 Feb 13. doi: 10.1038/s41587-020-0439-x. ReadCube: Full Access Link\nMany of the pipelines have a publication listed on the nf-core website that can be found here.\n\n\nDOIs\nIn addition, each release of an nf-core pipeline has a digital object identifiers (DOIs) for easy referencing in literature The DOIs are generated by Zenodo from the pipeline’s github repository."
  },
  {
    "objectID": "materials/08-workflow_managers/8.2-nfcore.html#credit",
    "href": "materials/08-workflow_managers/8.2-nfcore.html#credit",
    "title": "8.2 nf-core pipelines",
    "section": "Credit",
    "text": "Credit\nInformation on this page has been adapted and modified from the following source(s):\n\nGraeme R. Grimes, Evan Floden, Paolo Di Tommaso, Phil Ewels and Maxime Garcia. Introduction to Workflows with Nextflow and nf-core. https://github.com/carpentries-incubator/workflows-nextflow 2021."
  },
  {
    "objectID": "materials/09-bactmap/9.2-bactmap_results.html",
    "href": "materials/09-bactmap/9.2-bactmap_results.html",
    "title": "9.2 nf-core/bactmap Results",
    "section": "",
    "text": "Teaching: 20 min || Exercises: 10 min"
  },
  {
    "objectID": "materials/09-bactmap/9.2-bactmap_results.html#overview",
    "href": "materials/09-bactmap/9.2-bactmap_results.html#overview",
    "title": "9.2 nf-core/bactmap Results",
    "section": "Overview",
    "text": "Overview\n\n\n\n\n\n\n\n\n\n\n\n\nQuestions:\n\n\n\n\nWhere does nf-core/bactmap put the results files?\nWhat outputs does nf-core/bactmap produce?\nHow do we tell if all the samples passed QC and should be included in the final analysis?\nHow do we clean up after nf-core/bactmap?\n\n\n\n\n\n\n\n\n\nLearning Objectives:\n\n\n\n\nLearn where the output files created by nf-core/bactmap are located.\nUnderstand the output files created by nf-core/bactmap.\nLearn how to interpret a MultiQC summary and make a decision whether to exclude poor quality samples.\nUnderstand that nf-core/bactmap produces a lot of intermediate files which take up space and can be deleted once the pipeline has run successfully.\n\n\n\n\n\n\n\n\n\nKeypoints:\n\n\n\n\nnf-core/bactmap allows you to easily produce a sequence alignment by aligning sequences to a reference genome in a reproducible, documentable fashion"
  },
  {
    "objectID": "materials/09-bactmap/9.2-bactmap_results.html#nf-corebactmap-results",
    "href": "materials/09-bactmap/9.2-bactmap_results.html#nf-corebactmap-results",
    "title": "9.2 nf-core/bactmap Results",
    "section": "9.1 nf-core/bactmap results",
    "text": "9.1 nf-core/bactmap results\nThis morning, we left nf-core/bactmap running. Now, we can look at the output directory (results/bactmap) to see the various directories containing output files created by nf-core/bactmap.\ncd results/bactmap/bactmap_results\n\nls\nYou should see the following:\nbwa  fastp  multiqc  pipeline_info  pseudogenomes  rasusa  samtools  snpsites  variants \nThe directories in results/bactmap/bactmap_results contain the following:\n\n\n\n\n\n\n\nDirectory\nDescription\n\n\n\n\nbwa/index\nContains the index of the reference sequence\n\n\nfastp\nContains the results of the trimming and adapter removal performed by fastp\n\n\nfastqc\nContains QC metrics for the fastq files generated with fastQC\n\n\nmultiqc\nContains a html file containing summaries of the various outputs\n\n\npipeline_info\nContains information about the pipeline run\n\n\npseudogenomes\nContains consensus fasta files for each sample which have the sample variants compared to the reference included. The alignment we’ll use for the next step can also be found in this directory (aligned_pseudogenomes.fas)\n\n\nrasusa\nContains the subsampled post-trimmed fastq files\n\n\nsamtools\nContains the sorted bam files and indices created by bwa and samtools as part of the mapping process\n\n\nsnpsites\nContains a variant alignment file created from aligned_pseudogenomes.fas with snp-sitesthat can be used as input for tree inference tools\n\n\nvariants\nContains filtered vcf files which contain the variants for each sample\n\n\n\n\nThe MultiQC summary report\nThe first thing we’ll check is the html report file created by MultiQC. Copy this to your desktop:\ncp results/bactmap/bactmap_results/multiqc/multiqc_report.html Desktop\nUnfortunately, there is a bug with the conda environment bactmap uses for MultiQC so the pipeline may exit with an error before it finishes. In that case, we will have to generate the multiqc_report.html ourselves. Go to bottom of the white text showing the pipeline execution. To the left of the NF_BACTMAP:BACTMAP:MULTIQC (1) text you should see a combination of letters and numbers e.g. [52/6cd885]. This will enable you to find the sub-directory in the work that contains the results files we can run MultiQC on.\ncd work/52/6cd885 then click TAB\nThis will take you to the directory. Now we can make use of the qc environment which contains MultiQC:\nmamba activate qc\n\nmultiqc -f .\nMultiQC will then run and create the multiqc_report.html. Copy it to the results/bactmap directory:\nmamba activate qc\n\nmultiqc -f .\n\ncp multiqc_report.html ../../../results/bactmap\nGo to File Explorer, navigate to your Desktop and double click on multiqc_report.html. This will open the file in your web browser of choice:\n\n\n\nconfig\n\n\n\nGeneral statistics\nLet’s go through each section starting with the General Statistics:\n\n\n\nnf-core/bactmap MultiQC General Statistics\n\n\nThis is a compilation of statistics collected from the outputs of tools such as fastp, samtools and BCFtools. Sequencing metrics such as the % of duplicated reads and GC content of the reads are shown alongside the results of the mapping (% reads mapped, num). This is a useful way of quickly identifying samples that are of lower quality or perhaps didn’t map very well due to species contamination.\n\n\nfastp\nThere are a number of plots showing the results of the fastp step in the pipeline. The first shows the results of the read filtering step where reads are trimmed, adapters removed and low quality reads are thrown out. The reads that passed this step are highlighted in blue.\n\n\n\nnf-core/bactmap MultiQC fastp filtered reads\n\n\nThe second plot shows the frequency of duplicated reads in each sample. If you hover over the plot around 1 on the x-axis you’ll see that the majority of reads in each sample only appear once.\n\n\n\nnf-core/bactmap MultiQC fastp duplicated reads\n\n\nThe third plot shows the distribution of insert sizes for each set of sequence files. As we’ve included data sequenced on different Illumina platforms with different library construction protocols, there’s a mix of insert sizes.\n\n\n\nnf-core/bactmap MultiQC fastp insert sizes\n\n\nThe next plot shows the average sequence quality across the reads in each sample. You can see we have drop offs in quality at the beginning and end of reads; this is pretty typical and is an artefact of the sequencing process.\n\n\n\nnf-core/bactmap MultiQC fastp sequence quality\n\n\nThe fifth plot shows the average GC content across the reads in each sample. As you might expect, the average GC content is conserved across all the samples as they are all from the same organism (MTB).\n\n\n\nnf-core/bactmap MultiQC fastp GC\n\n\nThe final fastp plot shows the average N content across the reads in each sample. Similar to what we see in the sequence quality plot, the number of Ns tends to increase towards the end of reads.\n\n\n\nnf-core/bactmap MultiQC fastp N’s\n\n\n\n\nSamtools\nThe plots in this section are created from the results of running samtool stats on the sorted bam files produce during the mapping process. The first shows the number or percentage of reads that mapped to the reference.\n\n\n\nnf-core/bactmap MultiQC samtools mapping\n\n\nThe second plot shows the overall alignment metrics for each sample. Hover over each dot to see more detailed information.\n\n\n\nnf-core/bactmap MultiQC samtools alignment\n\n\n\n\nBCFtools\nThe plots in this section provide information about the variants called using bcftools. The first plot shows the numbers or percentage of each type of variant in each sample.\n\n\n\nnf-core/bactmap MultiQC bcftools variants\n\n\nThe second plot shows the quality of each variant called by bcftools. The majority of variants in each sample are high quality.\n\n\n\nnf-core/bactmap MultiQC bcftools variant quality\n\n\nThe third plot shows the distribution of lengths of Indels (insertions are positive values and deletions are negative values). This is useful information to have, but in practice we tend to exclude indels when building alignments for phylogenetic tree building.\n\n\n\nnf-core/bactmap MultiQC bcftools indel distribution\n\n\nThe final bcftools plot shows the distribution of the number of reads mapping to each variant position and is one of the metrics used to filter out low quality variants (the fewer the reads mapping to a variant position, the lower the confidence we have that the variant is in fact real).\n\n\n\nnf-core/bactmap MultiQC bcftools variant depth\n\n\n\n\nSoftware versions\nThis section of the report shows the software run as part of nf-core/bactmap and the versions used. This is particularly important when reproducing the analysis on a different system or when writing the methods section of a paper.\n\n\n\nnf-core/bactmap MultiQC software versions\n\n\n\n\n\n\n\n\nExercise 9.2.1: Extract data from MultiQC report\n\n\n\n\nHow many reads in sample SRX8022206 passed the fastp filtering step?\nHow many reads in sample SRX8038977 mapped to the reference genome?\nHow many A->G variants were called in sample SRX8038976?\nWhich version of samtools does the nf-core/bactmap pipeline use?\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n\n\n2148657 reads passed the fastp filtering step\n3685855 reads mapped to the reference genome\n107 (8.9%) A->G variants were called by bcftools\nThe version of samtools used in the pipeline is 1.10\n\n\n\n\n\n\n\n\n\nThe pseudogenomes directory\nThis directory contains the files that are most useful for our downstream analyses (at least for now). Change to the directory and list the files:\ncd bactmap_results/pseudogenomes\n\nls\nYou will see the pseudogenome fasta files (a version of the reference file where the sample variants have been inserted and low-quality or missing data has been masked) for each sample, an alignment of all the samples and the reference sequence (aligned_pseudogenomes.fas), and a tsv file of genomes removed from the complete alignment due to poor mapping (low_quality_pseudogenomes.tsv):\naligned_pseudogenomes.fas      SRX8021902.fas  SRX8022224.fas  SRX8038978.fas  SRX8038993.fas  SRX8039008.fas\nERX1161713.fas                 SRX8021909.fas  SRX8038964.fas  SRX8038979.fas  SRX8038994.fas  SRX8039009.fas\nERX1161718.fas                 SRX8021985.fas  SRX8038965.fas  SRX8038980.fas  SRX8038995.fas  SRX8039010.fas\nERX1161720.fas                 SRX8021998.fas  SRX8038966.fas  SRX8038981.fas  SRX8038996.fas  SRX8039011.fas\nERX1275297.fas                 SRX8022005.fas  SRX8038967.fas  SRX8038982.fas  SRX8038997.fas  SRX8039012.fas\nERX1287689.fas                 SRX8022010.fas  SRX8038968.fas  SRX8038983.fas  SRX8038998.fas  SRX8039013.fas\nERX646673.fas                  SRX8022021.fas  SRX8038969.fas  SRX8038984.fas  SRX8038999.fas  SRX8039014.fas\nERX694996.fas                  SRX8022023.fas  SRX8038970.fas  SRX8038985.fas  SRX8039000.fas  SRX8039015.fas\nERX695015.fas                  SRX8022052.fas  SRX8038971.fas  SRX8038986.fas  SRX8039001.fas  SRX8039016.fas\nlow_quality_pseudogenomes.tsv  SRX8022137.fas  SRX8038972.fas  SRX8038987.fas  SRX8039002.fas  SRX8039017.fas\nSRX8021803.fas                 SRX8022138.fas  SRX8038973.fas  SRX8038988.fas  SRX8039003.fas\nSRX8021809.fas                 SRX8022191.fas  SRX8038974.fas  SRX8038989.fas  SRX8039004.fas\nSRX8021823.fas                 SRX8022198.fas  SRX8038975.fas  SRX8038990.fas  SRX8039005.fas\nSRX8021849.fas                 SRX8022201.fas  SRX8038976.fas  SRX8038991.fas  SRX8039006.fas\nSRX8021865.fas                 SRX8022206.fas  SRX8038977.fas  SRX8038992.fas  SRX8039007.fas\nLet’s check to see if any of our samples were removed from our alignment:\ncat low_quality_pseudogenomes.tsv\nThis returns an empty file; all of our samples mapped well enough to the reference to be included in our complete alignment. That means we can proceed to the next step of our analysis: phylogenetic tree inference.\n\n\n\n\n\n\nExercise 9.2.2: Count the number of sequences in the complete alignment\n\n\n\nIt’s good practice to sanity check our results before proceeding to the next step of our analysis, phylogenetic tree inference. Count the number of fasta sequences in aligned_pseudogenomes.fas.\n\n\n\n\n\n\nSolution:\n\n\n\n\n\nOne way to do this is to count the number of times the fasta signifier > appears in aligned_pseudogenomes.fas using grep:\ngrep -c \">\" aligned_pseudogenomes.fas\nThis gives us our answer: there are 84 fasta sequences in the alignment (reference + 83 samples):\n84\n\n\n\n\n\n\n\nDisk Usage II — Cleaning up after analysis\n\nNow that we are done investigating our assembling and annotating our genome, let’s pause again and check the space of our current working directory.\nYou can do this with the disk usage du command\ndu -h\nHow much disk space have you used since the start of the analysis?\n\n\nThe work directory\nEach step of the pipeline produces one or more files that are not saved to the results directory but are kept in the work directory. This means that if, for whatever reason, the pipeline doesn’t finish successfully you can resume it. However, once the pipeline has completed successfully, you no longer need this directory (it can take up a lot of space) so you can delete it:\nrm -rf work\nHow much disk space did deleting the work directory free up?"
  },
  {
    "objectID": "materials/09-bactmap/9.2-bactmap_results.html#deactivate-nextflow-environment",
    "href": "materials/09-bactmap/9.2-bactmap_results.html#deactivate-nextflow-environment",
    "title": "9.2 nf-core/bactmap Results",
    "section": "9.2 Deactivate nextflow environment",
    "text": "9.2 Deactivate nextflow environment\nNow that we are done with all our analysis, let’s deactivate the nextflow environment:\nmamba deactivate"
  },
  {
    "objectID": "materials/09-bactmap/9.1-bactmap_run.html",
    "href": "materials/09-bactmap/9.1-bactmap_run.html",
    "title": "9.1 The nf-core/bactmap pipeline",
    "section": "",
    "text": "Teaching: 60 min || Exercises: 30 min"
  },
  {
    "objectID": "materials/09-bactmap/9.1-bactmap_run.html#overview",
    "href": "materials/09-bactmap/9.1-bactmap_run.html#overview",
    "title": "9.1 The nf-core/bactmap pipeline",
    "section": "Overview",
    "text": "Overview\n\n\n\n\n\n\n\n\n\n\n\n\nQuestions:\n\n\n\n\nWhat is nf-core/bactmap?\nHow can I produce a multiple sequence alignment of TB sequences?\n\n\n\n\n\n\n\n\n\nLearning Objectives:\n\n\n\n\nUse nf-core/bactmap to produce a multiple sequence alignment.\n\n\n\n\n\n\n\n\n\nKeypoints:\n\n\n\n\nTo build a phylogenetic tree we need a multiple sequence alignment of the sequences we want to infer a tree from.\nAlignments are usually done against a reference genome.\nWe can use the pipeline nf-core/bactmap to produce a multiple sequence alignment."
  },
  {
    "objectID": "materials/09-bactmap/9.1-bactmap_run.html#nf-corebactmap",
    "href": "materials/09-bactmap/9.1-bactmap_run.html#nf-corebactmap",
    "title": "9.1 The nf-core/bactmap pipeline",
    "section": "9.1 nf-core/bactmap",
    "text": "9.1 nf-core/bactmap\nnf-core/bactmap is a bioinformatics best-practice analysis pipeline for mapping short reads from bacterial WGS to a reference sequence, creating filtered VCF files, making pseudogenomes based on high quality positions in the VCF files and optionally creating a phylogeny from an alignment of the pseudogenomes. It makes use of many of the tools and techniques we’ve covered so far this week.\n\n\n\nnf-core/bactmap variant calling pipeline diagram from nf-core (https://nf-co.re/bactmap)."
  },
  {
    "objectID": "materials/09-bactmap/9.1-bactmap_run.html#tb-dataset",
    "href": "materials/09-bactmap/9.1-bactmap_run.html#tb-dataset",
    "title": "9.1 The nf-core/bactmap pipeline",
    "section": "9.2 TB dataset",
    "text": "9.2 TB dataset\nWe will be analysing a dataset of 83 TB genomes previously analysed by Prince as part of his PhD. As we don’t have access to the Noguchi HPC yet, we’re only going to analyse two of the genomes for now. Don’t worry, we’ll go through the results for all 83 samples"
  },
  {
    "objectID": "materials/09-bactmap/9.1-bactmap_run.html#activate-the-nextflow-environment",
    "href": "materials/09-bactmap/9.1-bactmap_run.html#activate-the-nextflow-environment",
    "title": "9.1 The nf-core/bactmap pipeline",
    "section": "9.3 Activate the nextflow environment",
    "text": "9.3 Activate the nextflow environment\nNow navigate into the 09-bactmap/ directory and activate the nextflow environment:\ncd ~/Desktop/workshop_files_Bact_Genomics_2023/09-bactmap/\nmamba activate nextflow\n\n\nDisk Usage I — Before analysis\n\nBefore we run nf-core/bactmap, let’s pause and check the space of our current working directory as we did for our previous lesson.\nYou can do this with the disk usage du command\ndu -h\n\n\nCurrent Disk Space In 09-bactmap Directory\n\n~247MB\n\nNow, keep this value in mind, and this time, don’t forget it. We will come back to it at the end of this chapter."
  },
  {
    "objectID": "materials/09-bactmap/9.1-bactmap_run.html#the-mtbc-ancestral-sequence",
    "href": "materials/09-bactmap/9.1-bactmap_run.html#the-mtbc-ancestral-sequence",
    "title": "9.1 The nf-core/bactmap pipeline",
    "section": "9.4 The MTBC ancestral sequence",
    "text": "9.4 The MTBC ancestral sequence\nThe reference sequence we’ll use to map our TB samples back to is one commonly used in the MTBC community and is known as the MTBC ancestral sequence. It was generated as part of Comas et al. 2013 (link to paper). This sequence was created by building an alignment and phylogenetic tree of the known MTBC genomes at the time and inferring what the most likely sequence of the ancestor of the MTBC was. It is a useful sequence to use when comparing different members of the MTBC as it should reduce the amount of bias towards any particular reference that is more closely related to one lineage than another."
  },
  {
    "objectID": "materials/09-bactmap/9.1-bactmap_run.html#setting-up-the-nextflow-config-file",
    "href": "materials/09-bactmap/9.1-bactmap_run.html#setting-up-the-nextflow-config-file",
    "title": "9.1 The nf-core/bactmap pipeline",
    "section": "9.5 Setting up the Nextflow config file",
    "text": "9.5 Setting up the Nextflow config file\nAs we learnt in the nf-core pipelines module, we can create a config file that tells Nextflow which parameters to use e.g. the job submission system. We’ve create a basic template that will allow you to run nf-core/bactmap on the teaching computers/HPC. The config file looks like this:\nparams {\n  config_profile_description = 'Noguchi HPC profile.'\n  config_profile_contact = 'Prince Asare (PAsare@noguchi.ug.edu.gh)'\n  config_profile_url = \"\"\n}\n\nprocess {\n  executor = 'local'\n}\n\nconda {\n  enabled = true\n  conda.useMamba = true\n}\n\nparams {\n  max_memory = 7.GB\n  max_cpus = 4\n  max_time = 12.h\n}"
  },
  {
    "objectID": "materials/09-bactmap/9.1-bactmap_run.html#run-nf-corebactmap-to-generate-an-alignment",
    "href": "materials/09-bactmap/9.1-bactmap_run.html#run-nf-corebactmap-to-generate-an-alignment",
    "title": "9.1 The nf-core/bactmap pipeline",
    "section": "9.6 Run nf-core/bactmap to generate an alignment",
    "text": "9.6 Run nf-core/bactmap to generate an alignment\nThe first step in building a phylogenetic tree is to generate a multiple sequence alignment. To do this, we’re going to map the sequence data for our 83 TB genomes to a reference, in this case the MTBC inferred ancestral sequence, using the nf-core/bactmap pipeline.\n\nGenerate a samplesheet\nNow create a samplesheet.csv file containing the sample IDs and the location of the files to be mapped:\nwget -L https://raw.githubusercontent.com/avantonder/bovisanalyzer/main/bin/fastq_dir_to_samplesheet.py\npython fastq_dir_to_samplesheet.py \\\n    data \\\n    samplesheet.csv \\\n    -r1 _1.fastq.gz \\\n    -r2 _2.fastq.gz\nThe meaning of the options used is:\n\n\n\n\n\n\n\n\nInput option\nInput required\nDescription\n\n\n\n\ndata\nDIRECTORY\nDirectory containing the fastq files\n\n\nsamplesheet.csv\nFILENAME\nName of the input file for nf-core/bactmap\n\n\n-r1\nFILE SUFFIX\nSuffix of the Read 1 FASTQ file\n\n\n-r2\nFILE SUFFIX\nSuffix of the Read 2 FASTQ file*\n\n\n\n* The fastq_dir_to_samplesheet.py script will use everything before this to create the sample IDs\nAs we don’t want to run all the samples today, we’ll extract the top two samples from the samplesheet.csv file:\nhead -n 3 samplesheet.csv > samplesheet_test.csv\n\n\n\n\n\n\nRun nf-core/bactmap\n\n\n\nNow run nf-core/bactmap with Nextflow to generate a reference-based alignment:\nnextflow run nf-core/bactmap \\\n    -profile conda \\\n    -c teaching.conf \\\n    --input samplesheet_test.csv \\\n    --reference reference/MTB_ANC_unwrapped.fasta \\\n    --genome_size 4.3mb \\\n    --outdir results/bactmap\nnf-core/bactmap has a number of optional arguments but for now these are the ones we’re going to use:\n\n\n\n\n\n\n\nInput option\nDescription\n\n\n\n\n-profile conda\nCreate conda environments for each tool in the pipeline (we could also use Singularity or Docker)\n\n\n-c teaching.conf\nA confuguration file containing the settings needed to run the pipeline on your machines\n\n\n--input samplesheet_test.csv\nThe samplesheet we created above containing the sample ids and location of the fastq files\n\n\n--reference reference/MTB_ANC_unwrapped.fasta\nThe reference sequence we’re going to map our samples to\n\n\n--genome_size 4.3mb\nThis is used by the pipeline to calculate the approximate genome coverage in the fastq files. By default the pipeline uses a tool called rasusa to subsample the fastq files so the genome coverage is <= 100X\n\n\n--outdir results/bactmap\nThe directory we’re going to save the outputs from nf-core/bactmap to\n\n\n\nVisit the nf-core/bactmap page for further information on running the pipeline with different options.\nIf the pipeline has started successfully, you should see something like this:\nN E X T F L O W  ~  version 22.04.3\nLaunching `https://github.com/nf-core/bactmap` [special_kare] DSL2 - revision: e83f8c5f0e [master]\n\n\n------------------------------------------------------\n                                        ,--./,-.\n        ___     __   __   __   ___     /,-._.--~'\n  |\\ | |__  __ /  ` /  \\ |__) |__         }  {\n  | \\| |       \\__, \\__/ |  \\ |___     \\`-._,-`-,\n                                        `._,._,'\n  nf-core/bactmap v1.0.0\n------------------------------------------------------\nCore Nextflow options\n  revision                  : master\n  runName                   : special_kare\n  containerEngine           : singularity\n  launchDir                 : /rds/project/rds-PzYD5LltalA/Teaching/Ghana\n  workDir                   : /rds/project/rds-PzYD5LltalA/Teaching/Ghana/work\n  projectDir                : /home/ajv37/.nextflow/assets/nf-core/bactmap\n  userName                  : ajv37\n  profile                   : singularity\n  configFiles               : /home/ajv37/.nextflow/assets/nf-core/bactmap/nextflow.config, /rds/project/rds-PzYD5LltalA/Teaching/Ghana/cambridge.config\n\nInput/output options\n  input                     : samplesheet.csv\n  outdir                    : bactmap_results\n\nCompulsory parameters\n  reference                 : MTB_ANC_unwrapped.fasta\n\nOptional pipeline steps\n  adapter_file              : /home/ajv37/.nextflow/assets/nf-core/bactmap/assets/adapters.fas\n  genome_size               : 4.3mb\n\nMax job request options\n  max_cpus                  : 56\n  max_memory                : 192 GB\n  max_time                  : 12h\n\nInstitutional config options\n  config_profile_description: Cambridge HPC cluster profile.\n  config_profile_contact    : Andries van Tonder (ajv37@cam.ac.uk)\n  config_profile_url        : https://docs.hpc.cam.ac.uk/hpc\n\n!! Only displaying parameters that differ from the pipeline defaults !!\n------------------------------------------------------\nIf you use nf-core/bactmap for your analysis please cite:\n\n* The nf-core framework\n  https://doi.org/10.1038/s41587-020-0439-x\n\n* Software dependencies\n  https://github.com/nf-core/bactmap/blob/master/CITATIONS.md\n------------------------------------------------------\nexecutor >  slurm (2)\nexecutor >  slurm (2)\n[3f/931dc1] process > NFCORE_BACTMAP:BACTMAP:INPUT_CHECK:SAMPLESHEET_CHECK (samplesheet.csv)   [  0%] 0 of 1\n[59/74ef4b] process > NFCORE_BACTMAP:BACTMAP:BWA_INDEX (MTB_ANC_unwrapped.fasta)               [  0%] 0 of 1\n[-        ] process > NFCORE_BACTMAP:BACTMAP:FASTP                                             -\n[-        ] process > NFCORE_BACTMAP:BACTMAP:SUB_SAMPLING:RASUSA                               -\n[-        ] process > NFCORE_BACTMAP:BACTMAP:BWA_MEM                                           -\n[-        ] process > NFCORE_BACTMAP:BACTMAP:BAM_SORT_SAMTOOLS:SAMTOOLS_SORT                   -\n[-        ] process > NFCORE_BACTMAP:BACTMAP:BAM_SORT_SAMTOOLS:SAMTOOLS_INDEX                  -\n[-        ] process > NFCORE_BACTMAP:BACTMAP:BAM_SORT_SAMTOOLS:BAM_STATS_SAMTOOLS:SAMTOOLS_... -\n[-        ] process > NFCORE_BACTMAP:BACTMAP:BAM_SORT_SAMTOOLS:BAM_STATS_SAMTOOLS:SAMTOOLS_... -\n[-        ] process > NFCORE_BACTMAP:BACTMAP:BAM_SORT_SAMTOOLS:BAM_STATS_SAMTOOLS:SAMTOOLS_... -\n[-        ] process > NFCORE_BACTMAP:BACTMAP:VARIANTS_BCFTOOLS:BCFTOOLS_MPILEUP                -\n[-        ] process > NFCORE_BACTMAP:BACTMAP:VARIANTS_BCFTOOLS:BCFTOOLS_FILTER                 -\n[-        ] process > NFCORE_BACTMAP:BACTMAP:VCF2PSEUDOGENOME                                  -\n[-        ] process > NFCORE_BACTMAP:BACTMAP:ALIGNPSEUDOGENOMES                                -\n[-        ] process > NFCORE_BACTMAP:BACTMAP:SNPSITES                                          -\n[-        ] process > NFCORE_BACTMAP:BACTMAP:GET_SOFTWARE_VERSIONS                             -\n[-        ] process > NFCORE_BACTMAP:BACTMAP:MULTIQC                                           -\n\n\nThe pipeline will take a while to run so we’ll have a look at the results after lunch."
  },
  {
    "objectID": "materials/10-intro_phylogenetics/10.1-intro_phylogenetics.html",
    "href": "materials/10-intro_phylogenetics/10.1-intro_phylogenetics.html",
    "title": "10.1 Introduction to Phylogenetics",
    "section": "",
    "text": "Teaching: 90 min || Exercises: 40 min"
  },
  {
    "objectID": "materials/10-intro_phylogenetics/10.1-intro_phylogenetics.html#overview",
    "href": "materials/10-intro_phylogenetics/10.1-intro_phylogenetics.html#overview",
    "title": "10.1 Introduction to Phylogenetics",
    "section": "Overview",
    "text": "Overview\n\n\n\n\n\n\n\n\n\n\n\n\nQuestions:\n\n\n\n\nWhat is a multiple sequence alignment (MSA)\nWhat is a phylogenetic tree?\nHow can I build a phylogenetic tree from my multiple sequence alignment?\nHow can I visualize and annotate my phylogenetic tree?\n\n\n\n\n\n\n\n\n\nLearning Objectives:\n\n\n\n\nUnderstand how to generate a multiple sequence alignment using a standard pipeline such as nf-core/bactmap\nUnderstand the basics of how phylogeny trees are constructed using maximum likelihood methods.\nUse IQ-TREE for phylogenetic tree inference.\nVisualize and annotate your tree with R and ggtree\n\n\n\n\n\n\n\n\n\nKeypoints:\n\n\n\n\nGenerating a multiple sequence alignment is the first step in building a phylogenetic tree.\nA phylogenetic tree is a graph representing evolutionary history and shared ancestry.\nThere are multiple methods and tools available for constructing phylogenetic trees.\nVisualization of a phylogenetic tree alongside available metadata is commonly how the relatedness of samples is portrayed in a scientific paper."
  },
  {
    "objectID": "materials/10-intro_phylogenetics/10.1-intro_phylogenetics.html#multiple-sequence-alignments",
    "href": "materials/10-intro_phylogenetics/10.1-intro_phylogenetics.html#multiple-sequence-alignments",
    "title": "10.1 Introduction to Phylogenetics",
    "section": "10.1 Multiple sequence alignments",
    "text": "10.1 Multiple sequence alignments\nPhylogenetic methods require sequence alignments. These can range from alignments of a single gene from different species to whole genome alignments where a sample’s sequence reads are mapped to a reference genome. Alignments attempt to place nucleotides from the same ancestral nucleotide in the same column. One of the most commonly used alignment formats in phylogenetics is FASTA:\n>Sample_1\nAA-GT-T\n>Sample_2\nAACGTGT\nN and - characters represent missing data and are interpreted by phylogenetic methods as such.\nThe two most commonly used muliple sequence alignments in bacterial genomics are reference-based whole genome alignments and core genome alignments generated by comparing genes between different isolates and identifying the genes found in all or nearly all isolates (the core genome). As a broad rule of thumb, if your species is not genetically diverse and doesn’t recombine (TB, Brucella) then picking a suitable good-quality reference and generating a whole genome alignment is appropriate. However, when you have a lot of diversity or multiple divergent lineages (E. coli) the a single reference may not represent all the diversity in your dataset. Here it would be more typical to create de novo assemblies, annotate them and then use a tool like roary or panaroo to infer the pan-genome and create a core genome alignment. The same phylogenetic methods are then applied to either type of multiple sequence alignment."
  },
  {
    "objectID": "materials/10-intro_phylogenetics/10.1-intro_phylogenetics.html#phylogenetic-tree-inference",
    "href": "materials/10-intro_phylogenetics/10.1-intro_phylogenetics.html#phylogenetic-tree-inference",
    "title": "10.1 Introduction to Phylogenetics",
    "section": "10.2 Phylogenetic tree inference",
    "text": "10.2 Phylogenetic tree inference\nA phylogenetic tree is a graph (structure) representing evolutionary history and shared ancestry. It depicts the lines of evolutionary descent of different species, lineages or genes from a common ancestor. A phylogenetic tree is made of nodes and edges, with one edge connecting two nodes.\nA node can represent an extant species, and extinct one, or a sampled pathogen: these are all cases of “terminal” nodes, nodes in the tree connected to only one edge, and usually associated with data, such as a genome sequence.\nA tree also contains “internal” nodes: these usually represent most recent common ancestors (MRCAs) of groups of terminal nodes, and are typically not associated with observed data, although genome sequences and other features of these ancestors can be statistically inferred. An internal node is most often connected to 3 branches (two descendants and one ancestral), but a multifurcation node can have any number >2 of descendant branches.\n\n\n\nNewick Format Example tree\n\n\n\nTree topology\nA clade is the set of all terminal nodes descending from the same ancestor. Each branch and internal node in a tree is associated with a clade. If two trees have the same clades, we say that they have the same topology. If they have the same clades and the same branch lengths, the two tree are equivalent, that is, they represent the same evolutionary history.\n\n\nUses of phlogenetic trees\nIn may cases, the phylogenetic tree represents the end results of an analysis, for example if we are interested in the evolutionary history of a set of species.\nHowever, in many cases a phylogenetic tree represents an intermediate step, and there are many ways in which phylogenetic trees can be used to help understand evolution and the spread of infectious disease.\nIn many cases, we may want to know more about genome evolution, for example about mutational pressures, but more frequently about selective pressures. Selection can affect genome evolution in many ways such as slowing down evolution of portion of the genome in which changes are deleterious (“purifying selection”). Instead, “positive selection” can favor changes at certain positions of the genome, effectively accelerating their evolution. Using genome data and phylogenetic trees, molecular evolution methods can infer different types of selection acting in different parts of the genome and different branches of a tree.\n\n\nNewick format\nWe often need to represent trees in text format, for example to communicate them as input or output of phylogenetic inference software. The Newick format is the most common text format for phylogenetic trees.\nThe Newick format encloses each subtree (the part of a tree relating the terminal nodes part of the same clade) with parenthesis, and separates the two child nodes of the same internal node with a “,”. At the end of a Newick tree there is always a “;”.\nFor example, the Newick format of a rooted tree relating two samples “S1” and “S2”, with distances from the root respectively of 0.1 and 0.2, is\n(S1:0.1,S2:0.2);\nIf we add a third sample “S3” as an outgroup, the tree might become\n((S1:0.1,S2:0.2):0.3,S3:0.4);\n\n\nMethods for inferring phylogenetic trees\nA few different methods exist for inferring phylogenetic trees:\n\nDistance-based methods such as Neighbour-Joining and UPGMA\nParsimony-based phylogenetics\nMaximum likelihood methods making use of nuclotide substitution models\n\n\nDistance-based methods\nThese are the simplest and fastest phylogenetic methods we can use and are often a useful way to have a quick look at our data before running more robust phylogenetic methods. Here, we infer evolutionary distances from the multiple sequence alignment. In the example below there is 1 subsitution out of 16 informative columns (we exclude columns with gaps or N’s) so the distance is approximately 1/16:\n\n\n\nEvolutionary distance between two sequences\n\n\nTypically, we have multiple sequences in an alignment so here we would generate a matrix of pairwise distances between all samples (distance matrix) and then use Neighbour-Joining or UPGMA to infer our phylogeny:\n\n\n\nDistance matrix to Neighbour-Joining tree\n\n\n\n\nParsimony methods\nMaximum parsimony methods assume that the best phylogenetic tree requires the fewest number of mutations to explain the data (i.e. the simplest explanation is the most likely one). By reconstructing the ancestral sequences (at each node), maximum parsimony methods evaluate the number of mutations required by a tree then modify the tree a little bit at a time to improve it.\n\n\n\nMaximum parsimony\n\n\nMaximum parsimony is an intuitive and simple method and is reasonably fast to run. However, because the most parsimonius tree is always the shortest tree, compared to the hypothetical “true” tree it will often underestimate the actual evolutionary change that may have occurred.\n\n\nMaximum likelihood methods\nThe most commonly encountered phylogenetic method when working with bacterial genome datasets is maximum likelihood. These methods use probabilistic models of genome evolution to evaluate trees and whilst similar to maximum parsimony, they allow statistical flexibility by permitting varying rates of evolution across different lineages and sites. This additional complexity means that maximum likelihood models are much slower than the previous two models discussed. Maximum likelihood methods make use of substitution models (models of DNA sequence evolution) that describe changes over evolutionary time. Two commonly used substitution models, Jukes-Cantor (JC69; assumes only one mutation rate) and Hasegawa, Kishino and Yano (HKY85; assumes different mutation rates - transitions have different rates) are depicted below:\n\n\n\nTwo DNA substitution models\n\n\nIt is also possible to incorporate additional assumptions about your data e.g. assuming that a proportion of the the alignment columns (the invariant or constant sites) cannot mutate or that there is rate variation between the different alignment columns (columns may evolve at different rates). The choice of which is the best model to use is often a tricky one; generally starting with one of the simpler models e.g. General time reversible (GTR) or HKY is the best way to proceed. Accounting for rate variation and invariant sites is an important aspect to consider so using models like HKY+G4+I (G4 = four types of rate variation allowed; I = invariant sites don’t mutate) should also be considered.\nThere are a number of different tools for phylogenetic inference via maximum-likelihood and some of the most popular tools used for phylogenetic inference are FastTree, IQ-TREE and RAxML-NG. For this lesson, we’re going to use IQ-TREE as it is fast and has a large number of substitution models to consider. It also has a model finder option which tells IQ-TREE to pick the best fitting model for your dataset, thus removing the decision of which model to pick entirely.\n\n\n\nTree uncertainty - bootstrap\nAll the methods for phylogenetic inference that we discussed so far aim at estimating a single realistic tree, but they don’t automatically tell us how confident we should be in the tree, or in individual branches of the tree.\nOne common way to address this limitation is using the phylogenetic bootstrap approach (Felsenstein, 1985). This consist first in sampling a large number (say, 1000) of bootstrap alignments. Each of these alignments has the same size as the original alignment, and is obtained by sampling with replacement the columns of the original alignment; in each bootstrap alignment some of the columns of the original alignment will usually be absent, and some other columns would be represented multiple times. We then infer a bootstrap tree from each bootstrap alignment. Because the bootstrap alignments differ from each other and from the original alignment, the bootstrap trees might different between each other and from the original tree. The bootstrap support of a branch in the original tree is then defined as the proportion of times in which this branch is present in the bootstrap trees."
  },
  {
    "objectID": "materials/10-intro_phylogenetics/10.1-intro_phylogenetics.html#activate-the-phylogenetics-environment",
    "href": "materials/10-intro_phylogenetics/10.1-intro_phylogenetics.html#activate-the-phylogenetics-environment",
    "title": "10.1 Introduction to Phylogenetics",
    "section": "10.3 Activate the phylogenetics environment",
    "text": "10.3 Activate the phylogenetics environment\nmamba activate phylogenetics\n\n\nDisk Usage I — Before analysis\n\nBefore we start performing any assemblies, let’s pause and check the space of our current working directory as we did for our previous lesson.\nYou can do this with the disk usage du command\ndu -h\n\n\nCurrent Disk Space In results/iqtree Directory\n\n~247MB\n\nNow, keep this value in mind, and this time, don’t forget it. We will come back to it at the end of this chapter."
  },
  {
    "objectID": "materials/10-intro_phylogenetics/10.1-intro_phylogenetics.html#phylogenetic-inference-of-tb-dataset",
    "href": "materials/10-intro_phylogenetics/10.1-intro_phylogenetics.html#phylogenetic-inference-of-tb-dataset",
    "title": "10.1 Introduction to Phylogenetics",
    "section": "10.4 Phylogenetic inference of TB dataset",
    "text": "10.4 Phylogenetic inference of TB dataset\nFor the rest of this lesson we’re going to build a phylogenetic tree using the multiple sequence alignment we generated with nf-core/bactmap.\n\nVariant site extraction with snp-sites\nPhylogenetic inference software such as IQ-tree typically takes as input an alignment of just the variant sites in a multiple sequence alignment. So, before running IQ-tree, we need to extract the variant sites from the masked alignment. To ensure that the branch lengths are correctly scaled, we also need to tell IQ-tree how many invariant or constant sites there are in our multiple sequence alignment. To do both those things, we can use a tool called snp-sites.\n\n\n\n\n\n\nHelp\n\n\n\nDo this to get the help information for snp-sites:\nsnp-sites -h\nsnp-sites: invalid option -- 'h'\nUsage: snp-sites [-rmvpcbhV] [-o output_filename] <file>\nThis program finds snp sites from a multi FASTA alignment file.\n -r     output internal pseudo reference sequence\n -m     output a multi fasta alignment file (default)\n -v     output a VCF file\n -p     output a phylip file\n -o STR specify an output filename [STDOUT]\n -c     only output columns containing exclusively ACGT\n -C     only output count of constant sites (suitable for IQ-TREE -fconst) and nothing else\n -b     output monomorphic sites, used for BEAST\n -h     this help message\n -V     print version and exit\n <file> input alignment file which can optionally be gzipped\n\nExample: creating files for BEAST\n snp-sites -cb -o outputfile.aln inputfile.aln\n\nIf you use this program, please cite:\n\"SNP-sites: rapid efficient extraction of SNPs from multi-FASTA alignments\",\nAndrew J. Page, Ben Taylor, Aidan J. Delaney, Jorge Soares, Torsten Seemann, Jacqueline A. Keane, Simon R. Harris,\nMicrobial Genomics 2(4), (2016). http://dx.doi.org/10.1099/mgen.0.000056\n\n\n\n\n\n\n\n\nUsage\n\n\n\nThe basic usage for snp-sites is:\nsnp-sites <ALIGNMENT> > -o <VARIANT SITE ALIGNMENT>\nThe meaning of the option used is:\n\n\n\nInput option\nInput required\nDescription\n\n\n\n\n-o\nFILENAME\nFilename of output variant alignment\n\n\n\n\n\n\n\n\n\n\n\nRun snp-sites\n\n\n\nTo extract the variant site alignment and count the number of constant sites in our alignment, run the following commands in the results/iqtreedirectory:\nsnp-sites \\\n    aligned_pseudogenomes.fas \\\n    -o aligned_pseudogenomes_snps.fas\n\nsnp-sites \\\n    -C aligned_pseudogenomes.fas \\\n    > constant_sites.txt\n\n\n\n\nPhylogenetic tree inference with IQ-TREE\n\n\n\n\n\n\nHelp\n\n\n\nDo this to get the help information for IQ-TREE (we’ve excluded much of what’s printed as there’s many, many options available for use with IQ-TREE):\niqtree -h\nIQ-TREE multicore version 2.2.0.3 COVID-edition for Linux 64-bit built May 11 2022\nDeveloped by Bui Quang Minh, James Barbetti, Nguyen Lam Tung,\nOlga Chernomor, Heiko Schmidt, Dominik Schrempf, Michael Woodhams, Ly Trong Nhan.\n\nUsage: iqtree [-s ALIGNMENT] [-p PARTITION] [-m MODEL] [-t TREE] ...\n\nGENERAL OPTIONS:\n  -h, --help           Print (more) help usages\n  -s FILE[,...,FILE]   PHYLIP/FASTA/NEXUS/CLUSTAL/MSF alignment file(s)\n  -s DIR               Directory of alignment files\n  --seqtype STRING     BIN, DNA, AA, NT2AA, CODON, MORPH (default: auto-detect)\n  -t FILE|PARS|RAND    Starting tree (default: 99 parsimony and BIONJ)\n  -o TAX[,...,TAX]     Outgroup taxon (list) for writing .treefile\n  --prefix STRING      Prefix for all output files (default: aln/partition)\n  --seed NUM           Random seed number, normally used for debugging purpose\n  --safe               Safe likelihood kernel to avoid numerical underflow\n  --mem NUM[G|M|%]     Maximal RAM usage in GB | MB | %\n  --runs NUM           Number of indepedent runs (default: 1)\n  -v, --verbose        Verbose mode, printing more messages to screen\n  -V, --version        Display version number\n  --quiet              Quiet mode, suppress printing to screen (stdout)\n  -fconst f1,...,fN    Add constant patterns into alignment (N=no. states)\n  --epsilon NUM        Likelihood epsilon for parameter estimate (default 0.01)\n  -T NUM|AUTO          No. cores/threads or AUTO-detect (default: 1)\n  --threads-max NUM    Max number of threads for -T AUTO (default: all cores)\n  --export-alisim-cmd  Export a command-line from the inferred tree and model params\n\n  --> MORE OPTIONS\n\n\n\n\n\n\n\n\nUsage\n\n\n\nAs indicated above, IQ-TREE has many different parameters available depending on what kind of tree you’d like to build including a comprehensive list of different nucleotide substitution models. In practice, there only a few different options that need to be required to get a well-supported phylogentic tree using one of the the established nucleotide substitition models such as GTR or HKY.\niqtree \\\n    -fconst <NUMBER OF INVARIANT SITES FOR EACH NUCLEOTIDE>\n    -s <VARIANT SITE ALIGNMENT>\n    -m <NUCLEOTIDE SUBSTITUTION MODEL>\n    -bb <ULTRAFAST BOOTSTRAPS>\nThe meaning of the options used is:\n\n\n\n\n\n\n\n\nInput option\nInput required\nDescription\n\n\n\n\n-fconst\nINT\nThe number of invariant/constant sites in the alignment (e.g. 15,10,10,15)\n\n\n-s\nFILENAME\nThe input variant (SNP) alignment\n\n\n-m\nMODEL\nThe nucleotide substitution model to use\n\n\n-bb\nINT\nThe number of ultrafast bootstraps to perform\n\n\n\n\n\n\n\n\n\n\n\nRun IQ-TREE\n\n\n\nNow we can run IQ-TREE using the two files we just created with snp-sites as input:\niqtree \\\n    -fconst $(cat constant_sites.txt) \\\n    -s aligned_pseudogenomes_snps.fas \\\n    -nt auto \\\n    -ntmax 8 \\\n    -mem 8G \\\n    -m GTR \\\n    -bb 1000\nThe meaning of the options used is:\n\n\n\n\n\n\n\nInput option\nDescription\n\n\n\n\n--fconst $(cat constant_sites.txt)\nExtract the number of invariant sites from the file we created using SNP-sites\n\n\n-s aligned_pseudogenomes_snps.fas\nThe variant site alignment\n\n\n-nt auto\nAutodetects the number of available cores\n\n\n-ntmax 8\nThe maximum number of cores to use\n\n\n-mem 8G\nThe maximum RAM usage in GB\n\n\n-m GTR\nUse the GTR (General time reversible model of) nucleotide substitution\n\n\n-bb 1000\nThe number of ultrafast bootstraps to perform\n\n\n\nIQ-TREE multicore version 2.1.2 COVID-edition for Linux 64-bit built Mar 30 2021\nDeveloped by Bui Quang Minh, James Barbetti, Nguyen Lam Tung,\nOlga Chernomor, Heiko Schmidt, Dominik Schrempf, Michael Woodhams.\n\nHost:    login-e-15 (AVX512, FMA3, 187 GB RAM)\nCommand: iqtree -fconst 757236,1447033,1441675,757145 -s aligned_pseudogenomes_snps.fas -nt auto -ntmax 8 -mem 8G -m GTR -bb 1000\nSeed:    753164 (Using SPRNG - Scalable Parallel Random Number Generator)\nTime:    Tue Dec 13 14:43:53 2022\nKernel:  AVX+FMA - auto-detect threads (64 CPU cores detected)\n\nReading alignment file aligned_pseudogenomes_snps.fas ... Fasta format detected\nAlignment most likely contains DNA/RNA sequences\nAlignment has 84 sequences with 8443 columns, 2717 distinct patterns\n5840 parsimony-informative, 2603 singleton sites, 0 constant sites\n            Gap/Ambiguity  Composition  p-value\n   1  SRX8038984    5.35%    passed     97.21%\n   2  SRX8038991    5.63%    passed     99.66%\n...\nOnce IQ-TREE has run, we can look at the output directory (results/iqtree) to see the various output files created by IQ-tree:\n\n\n\nOutput file\nDescription\n\n\n\n\naligned_pseudogenomes_snps.fas.iqtree\nText file containing a report of the IQ-Tree run, including a representation of the tree in text format\n\n\naligned_pseudogenomes_snps.fas.treefile\nThe estimated tree in NEWICK format. We can use this file with other programs, such as FigTree, to visualise our tree\n\n\naligned_pseudogenomes_snps.fas.log\nThe log file containing the messages that were also printed on the screen\n\n\naligned_pseudogenomes_snps.fas.bionj\nThe initial tree estimated by neighbour joining (NEWICK format)\n\n\naligned_pseudogenomes_snps.fas.mldist\nThe maximum likelihood distances between every pair of sequences\n\n\naligned_pseudogenomes_snps.fas.ckp.gz\nThis is a “checkpoint” file, which IQ-Tree uses to resume a run in case it was interrupted (e.g. if you are estimating very large trees and your job fails half-way through)."
  },
  {
    "objectID": "materials/10-intro_phylogenetics/10.1-intro_phylogenetics.html#visualization-of-phylogenetic-trees",
    "href": "materials/10-intro_phylogenetics/10.1-intro_phylogenetics.html#visualization-of-phylogenetic-trees",
    "title": "10.1 Introduction to Phylogenetics",
    "section": "10.5 Visualization of phylogenetic trees",
    "text": "10.5 Visualization of phylogenetic trees\nDepending on what you want to do with your phylogenetic tree, there are many programs that can be used to visualise phylogenetic trees. Some of the most commonly used standalone tools are iTOL, Microreact and Figtree. Increasingly, there is a move to visualizing and annotating your trees using R and Python packages. For the purposes of today’s lesson we’re going to visualize your trees in Figtree showing how trees can quickly and easily be manipulated just by opening the file containing the tree data. We’ve also provided an exercise showing how we could also do this using the ggtree package in R.\n\nVisualizing your phylogenetic trees with Figtree\nFigTree is a very established piece of software as it provides a simple user graphical user interface and can be used to produce publication-ready figures. We don’t cover Bayesian phylogenetic methods for creating time-scaled phylogenies but displaying these kind of trees is what FigTree was designed to do.\nThe first thing we’ll need to do is copy the phylogenetic tree we created to your Desktop:\ncp results/iqtree/aligned_pseudogenomes_snps.fas.treefile Desktop\nOpen FigTree, then go to File > Open… and browse to the Desktop folder.\nSelect the file with .treefile extension and click Open. Then click OK on the Input dialogue that pops up. You will be presented with a visual representation of the tree:\n\n\n\nTB phylogenetic tree in FigTree\n\n\nWe’re currently looking at an unrooted tree so the first thing we’re going to do is to root the tree with the MTB ancestral sequence (labelled MTB_anc in the tree). Rooting a tree with a more distantly related lineage or species is important as it enables a better representation of the evolutionary history of the samples you’re working with. To root our tree, click on the branch leading to MTB_anc and click the Reroot button at the top of the window. Now you’ll see a different representation of the tree compared to the one we first loaded:\n\n\n\nTB phylogenetic tree rooted with the ancestral sequence\n\n\nWe can also import a “tab-separated values” (TSV) file with annotations to add to the tree. We’ve provided a TSV file (data/tb_metadata.tsv) which contains the metadata for the isolates in our tree. You can prepare this file in Excel or another spreadsheet program. To load the annotation, go to File > Import annotations… and open the annotation file. The first thing we’re going to do is replace the ENA run accessions with the real names of our samples. On the menu on the left, click Tip Labels and under “Display” choose the TBNm_ID field of our metadata table. You’ll see that the tip labels have now been replaced with the correct sample names. FigTree is pretty flexible in what it allows you to do. For now we’ll just change the colours of the tip labels to reflect the sub-lineage of the samples. Again, under Tip Labels click “Colour by:” and change “User selection” to Sub-lineage. It’s also good practice to add a legend to our figure. At the top of the window click Annotate and select Sub-lineage then go to Legend, click the tick box and under “Attribute” select Sub_lineage. A legend should now be displayed. Finally click Align Tip Labels to line up our sample names on the left hand side of the figure. You should have a figure that looks something like this:\n\n\n\nTB phylogenetic tree with annotation\n\n\n\n\n\n\n\n\nExercise 10.1.1: Tweak the annotation\n\n\n\nWe loaded a few different types of information in our metadata TSV file. Have a go at changing/adding the metadata in the tree:\n\nColour the labels according to the MDR status of the samples and update the legend.\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n\n\nTo change the label colour to show the MDR status for each sample, click Tip Labels and under “Display” choose the MDR field of our metadata table. To update the legend, click Annotate and select MDR then go to Legend, click the tick box and under “Attribute” select MDR.\n\n\n\n\nTB phylogenetic tree with revised annotatiokn\n\n\n\n\n\n\n\n\n\n\n\n\n\nBonus Exercise: Use ggtree to produce a publication-quality image\n\n\n\nIf you’ve made it this far, congratulations! For this bonus exercise we’re going to provide some code that you can run in RStudio. This code will show you how to read in a phylogenetic tree and the metadata before plotting the tree with some of the metadata as strips.\n######################\n## Import libraries ##\n######################\n\nlibrary(tidyverse)\nlibrary(ggtree)\nlibrary(ggnewscale)\nlibrary(phytools)\n\n######################################\n## Read in metadata for all samples ##\n######################################\n\nmetadata <- read_tsv(\"tb_metadata.tsv\")\n\n#########################\n## Read in iqtree tree ##\n#########################\n\ntree <- read.tree(\"aligned_pseudogenomes_snps.fas.treefile\")\n\n#################################\n## Root tree with MTB ancestor ##\n#################################\n\ntreeRooted <- root(tree, outgroup = \"MTB_anc\", resolve.root = TRUE)\n\n#################################\n# Relabel tips with sample name #\n#################################\n\ntreeRooted$tip.label <- metadata$TBNm_ID[match(treeRooted$tip.label, metadata$sample)]\n\n##############################################\n## Create a metadata dataframe for plotting ##\n##############################################\n\nmetadataDF <- as.data.frame(metadata[,c(8:10,13)])\n\nrownames(metadataDF) = metadata$TBNm_ID\n\n################################\n## Plot tree without metadata ##\n################################\n\ntreePlot <- ggtree(treeRooted) +\n  geom_tiplab(align = T, \n              size = 2) +\n  geom_treescale(x = 0, y = 45) +\n  xlim_tree(0.0001) +\n  theme_tree()\n\n##########################\n## Add metadata to tree ##\n##########################\n\n# Add sub lineage\n\ntreePlotMeta <- gheatmap(treePlot, \n                         metadataDF[3],\n                         offset = 0.00001,\n                         color = NA,\n                         width = 0.05, \n                         colnames = T,\n                         colnames_angle=90,\n                         colnames_position = \"top\",\n                         hjust = 0,\n                         font.size = 4) +\n  scale_fill_viridis_d(option = \"D\", name = \"Sub-lineage\") +\n  coord_cartesian(clip = \"off\")\n\ntreePlotMeta2 <- treePlotMeta + new_scale_fill()\n\n# Add Region\n\ntreePlotMeta3 <- gheatmap(treePlotMeta2, \n                         metadataDF[1],\n                         offset = 0.000025,\n                         color = NA,\n                         width = 0.05, \n                         colnames = T,\n                         colnames_angle=90,\n                         colnames_position = \"top\",\n                         hjust = 0,\n                         font.size = 4) +\n  scale_fill_viridis_d(option = \"D\", name = \"Region\") +\n  coord_cartesian(clip = \"off\")\n\ntreePlotMeta4 <- treePlotMeta3 + new_scale_fill()\n\n# Add MDR status\n\nsuscColourStrips <- c(\"Positive\" = \"#ff0000\",\"Negative\" = \"#ffffff\")\n\ntreePlotMeta5 <- gheatmap(treePlotMeta4, \n                          metadataDF[4],\n                          offset = 0.00004,\n                          color = NA,\n                          width = 0.05, \n                          colnames = T,\n                          colnames_angle=90,\n                          colnames_position = \"top\",\n                          hjust = 0,\n                          font.size = 4) +\n  scale_fill_manual(values = suscColourStrips, name = \"MDR\") +\n  coord_cartesian(clip = \"off\")\nHopefully, you get something like this:\n\n\n\nTB phylogenetic tree visualized in ggtree\n\n\nHave a play with what metadata you display and try changing the colours. A major advantage of ggtree is the flexibility it allows.\n\n\n\n\n\n\n\n\nBonus Exercise: Build and annotate a phylogenetic tree using reference-based assemblies from the Short Read Mapping exercise\n\n\n\n\n\n\n\n\nDisk Usage II — Cleaning up after analysis\n\nNow that we are done investigating our assembling and annotating our genome, let’s pause again and check the space of our current working directory.\nYou can do this with the disk usage du command\ndu -h\nHow much disk space have you used since the start of the analysis?"
  },
  {
    "objectID": "materials/10-intro_phylogenetics/10.1-intro_phylogenetics.html#deactivate-phylogenetics-environment",
    "href": "materials/10-intro_phylogenetics/10.1-intro_phylogenetics.html#deactivate-phylogenetics-environment",
    "title": "10.1 Introduction to Phylogenetics",
    "section": "10.6 Deactivate phylogenetics environment",
    "text": "10.6 Deactivate phylogenetics environment\nNow that we are done with all our analysis, let’s deactivate the phylogenetics environment:\nmamba deactivate"
  },
  {
    "objectID": "materials/10-intro_phylogenetics/10.1-intro_phylogenetics.html#credit",
    "href": "materials/10-intro_phylogenetics/10.1-intro_phylogenetics.html#credit",
    "title": "10.1 Introduction to Phylogenetics",
    "section": "Credit",
    "text": "Credit\nInformation on this page has been adapted and modified from the following source(s):\nhttps://github.com/cambiotraining/sars-cov-2-genomics"
  },
  {
    "objectID": "materials/11-genotyping/11.1-genotyping.html",
    "href": "materials/11-genotyping/11.1-genotyping.html",
    "title": "11.1 Bacterial Genotyping and Drug Resistance Prediction",
    "section": "",
    "text": "Teaching: 120 min || Exercises: 30 min"
  },
  {
    "objectID": "materials/11-genotyping/11.1-genotyping.html#overview",
    "href": "materials/11-genotyping/11.1-genotyping.html#overview",
    "title": "11.1 Bacterial Genotyping and Drug Resistance Prediction",
    "section": "Overview",
    "text": "Overview\n\n\n\n\n\n\n\n\n\n\n\n\nQuestions:\n\n\n\n\nHow do I perform basic genotyping and serotyping of bacterial strains?\nWhat are the available genotyping and serotyping tools?\nHow do I identify resistant bacteria?\n\n\n\n\n\n\n\n\n\nLearning Objectives:\n\n\n\n\nTo perform basic genotyping and serotyping of bacterial strains from WGS data.\nTo perform antimicrobial resistance (AMR) prediction of bacterial strains using WGS data.\n\n\n\n\n\n\n\n\n\nKey points:\n\n\n\n\nBasic bioinformatics tools exists to genotype as well as serotype bacterial strains from identifying species right down to determining lineages, sub-lineages and strains.\nUsing WGS data, and by just calling out drug resistant SNPs, you can readily determine the AMR pattern of bacterial strains.\nSome widely used bacteria genotyping, serotyping and AST tools include:\n\nMulti Locus Sequence Typing (MLST) — genotyping\nSeroba — serotyping\nTBprofiler — genotyping and AST\nAriba — genotyping and AST"
  },
  {
    "objectID": "materials/11-genotyping/11.1-genotyping.html#background",
    "href": "materials/11-genotyping/11.1-genotyping.html#background",
    "title": "11.1 Bacterial Genotyping and Drug Resistance Prediction",
    "section": "11.1.1 Background",
    "text": "11.1.1 Background\nWhole genome sequencing data enables us to acquire several information relating to the:\n\ngenotype,\nserotype and\nprediction of antimicrobial resistance\n\n… of our bacterial sample.\nIn this module of the workshop we will be introduced to the various tools that are widely used for obtaining the above information from our genome.\nLet’s jump into business by looking at our first tool."
  },
  {
    "objectID": "materials/11-genotyping/11.1-genotyping.html#mlst",
    "href": "materials/11-genotyping/11.1-genotyping.html#mlst",
    "title": "11.1 Bacterial Genotyping and Drug Resistance Prediction",
    "section": "11.1.2 MLST",
    "text": "11.1.2 MLST\nMulti-Locus Sequence Typing (MLST) typically refers to the systematic sequencing and typing of five to ten well-conserved (usually seven), house-keeping genes or loci within the bacterial genome. Allelic variation at each locus is catalogued, and a sequence type or lineage is assigned by comparing the set of alleles to other isolate profiles in one of several available databases. The majority of MLST databases are hosted at a web server currently located in Oxford University pubmlst.org.\nYou can read more about the practical steps involved in MLST in this publication.\n\n\n\n\n\n\nHouse keeping genes\n\n\n\nHousekeeping genes are examples of regions in a genome that tend to be highly conserved and evolve slower than other genes mainly due to their roles in the maintenance of basic cellular functions and are essential for the existence of a cell. They are expressed in all cells of an organism under normal and patho-physiological conditions.\n\n\nIt is worth noting that both the number and type of housekeeping genes interrogated by MLST may differ from species to species. MLST typing for Staphylococcus aureus for example uses seven housekeeping genes. As specified by the MLST website, these genes include:\n\ncarbamate kinase (arcC),\nshikimate dehydrogenase (aroE),\nglycerol kinase (glpF),\nguanylate kinase (gmk),\nphosphate acetyltransferase (pta),\ntriosephosphate isomerase (tpi) and\nacetyl coenzyme A acetyltransferase (yqiL)\n\nWhereas MLST typing for Vibrio vulnificus uses the following housekeeping genes:\n\nglucose-6-phosphate isomerase (glp),\nDNA gyrase, subunit B (gyrB),\nmalate-lactate dehydrogenase (mdh),\nmethionyl-tRNA synthetase (metG),\nphosphoribosylaminoimidazole synthetase (purM),\nthreonine dehydrogenase (dtdS),\ndiaminopimelate decarboxylase (lysA),\ntranshydrogenase alpha subunit (pntA),\ndihydroorotase (pyrC) and\ntryptophanase (tnaA).\n\nMLST compares allelic diversity based on approximately 450-500 bp internal gene fragments. For each house-keeping gene, the different sequences present within a bacterial species are assigned as distinct alleles and, for each isolate, the alleles at each of the seven (or specific number of loci) loci define the allelic profile or Sequence Type (ST). You should remember encountering ST from two of our exercises (Exercise 5.1.8) in our short read mapping and (Exercise 11. ) in our phylogenetics sessions, where we attempted to reproduce data results from the paper titled “Genomic analysis of ST88 community-acquired methicillin resistant Staphylococcus aureus in Ghana”. You can have a look at those exercises again if you haven’t done that already. Thousands of sequences have been submitted, generating numerous STs. Organisms that share all seven alleles are defined as clones, those that share five of seven identical alleles are defined as Clonal Complexes (CC), and those that share less than five alleles are defined as unrelated.\nIn MLST the number of nucleotide differences between alleles is ignored and sequences are given different allele numbers whether they differ at a single nucleotide site or at many sites. The rationale is that a single genetic event resulting in a new allele can occur by a point mutation (altering only a single nucleotide site), or by a recombinational replacement (that will often change multiple sites) - weighting according to the number of nucleotide differences between alleles would erroneously consider the allele to be more different than by treating the nucleotide changes as a single genetic event. The allelic profiles can be considered as a character set of n categorical characters (n being the number of house-keeping genes investigated). MLST has been used successfully to study population genetics and reconstruct micro-evolution of epidemic bacteria and other micro-organisms.\n\n\n\n\n\n\nConventional MLST\n\n\n\nConventionally, MLST has been performed on purified isolates by running sequencing analysis on PCR amplified house-keeping genes. In the past, this really cut the cost of whole genome sequencing (WGS) by just performing a targeted sequencing on the main house-keeping genes used for MLST as per the organism. The figure below adapted from Ruppitsch 2016 summarised this very well.\n\n\n\nConventional MLST - Scheme for multilocus sequence typing adapted from mlst.net. MLST uses sequence variations in up to seven housekeeping genes. Allele numbers are assigned to unique sequences and the allele number combination result in a sequence type.\n\n\n\n\nWith the growing decreasing cost of WGS, it is now even cheaper to perform WGS and then do in silico MLST analysis. This is what we will do in the workshop.\n\n\n\n\n\n\nA typical MLST phylogeny\n\n\n\nAll the MLST allelic information generated from a population of microorganisms can be nicely plotted to study the variation using phylogenetic analysis. You should be an expert of plotting phylogenetic trees by now.\n\n\n\nMLST sequence type phylogeny - Image adapted from Jolley and Maiden 2014\n\n\n\n\nThis is too much of introduction. Now let’s get to what we are here for. What bioinformatics tool can we apply to define STs from our genome?\nFortunately for us, there is a tool called mlst. What the *mlst tool does is to scan contig files against traditional PubMLST typing schemes.\n\n\n\n\n\n\nHelp\n\n\n\nDo this to get the help information for mlst\nmlst -h\nSYNOPSIS\n  Automatic MLST calling from assembled contigs\nUSAGE\n  % mlst --list                                            # list known schemes\n  % mlst [options] <contigs.{fasta,gbk,embl}[.gz]          # auto-detect scheme\n  % mlst --scheme <scheme> <contigs.{fasta,gbk,embl}[.gz]> # force a scheme\nGENERAL\n  --help            This help\n  --version         Print version and exit(default ON)\n  --check           Just check dependencies and exit (default OFF)\n  --quiet           Quiet - no stderr output (default OFF)\n  --threads [N]     Number of BLAST threads (suggest GNU Parallel instead) (default '1')\n  --debug           Verbose debug output to stderr (default OFF)\nSCHEME\n  --scheme [X]      Don't autodetect, force this scheme on all inputs (default '')\n  --list            List available MLST scheme names (default OFF)\n  --longlist        List allelles for all MLST schemes (default OFF)\n  --exclude [X]     Ignore these schemes (comma sep. list) (default 'ecoli,abaumannii,vcholerae_2')\nOUTPUT\n  --csv             Output CSV instead of TSV (default OFF)\n  --json [X]        Also write results to this file in JSON format (default '')\n  --label [X]       Replace FILE with this name instead (default '')\n  --nopath          Strip filename paths from FILE column (default OFF)\n  --novel [X]       Save novel alleles to this FASTA file (default '')\n  --legacy          Use old legacy output with allele header row (requires --scheme) (default OFF)\n...\n\n\n\n\n\n\n\n\nUsage\n\n\n\nThe general format of the command is:\nmlst [options] <contigs.{fasta,gbk,embl}[.gz]>\nGenerally, the command above auto-detects an appropriate scheme to use. However, you can force a scheme by adding the option --scheme and specifying an appropriate scheme:\nmlst --scheme <scheme> <contigs.{fasta,gbk,embl}[.gz]>\nYou can list known schemes with the below command:\nmlst --list \nThe above command gives a shortened list. You can get more details using mlst --longlist.\n\n\nAs you can see from the general command above, you just simply give it a genome file in FASTA/GenBank/EMBL format, optionally compressed with gzip, zip or bzip2. You can also give it multiple files at once.\n\n\n\n\n\n\nOutput\n\n\n\nThe command returns a tab-separated line containing\n\nthe filename\nthe matching PubMLST scheme name\nthe ST (sequence type)\nthe genes and their allele IDs\n\n\n\n\nmlst output\n\n\n\n\nIf you have tried out any of the above commands, you may have realised it gives you error messages. This is because you probably don’t have mlst installed on your system. We have created a conda environment called mlst. We will activate this environment to perform the tasks in this chapter of the workshop.\n\n\n\n\n\n\nNavigate to the genotyping_and_dr directory and activate the mlst environment\n\n\n\ncd ~/Desktop/workshop_files_Bact_Genomics_2023/11_genotyping_and_dr/\nmamba activate mlst\nHave a quick look at the directory\nls -al\n\n\nWe are now ready to perform some genotyping starting with mlst.\nFirst, let’s try our hands on knowing what the ST of our genome is. Our first genome to analyse is G26832 and we will use the contigs.fa as our input.\nLet’s go ahead and run the below command with no options to genotype our sample:\n\n\n\n\n\n\nmlst — raw\n\n\n\nmlst G26832.contigs.fa \n[11:37:27] This is mlst 2.22.1 running on linux with Perl 5.032001\n[11:37:27] Checking mlst dependencies:\n[11:37:27] Found 'blastn' => /home/pa486/miniconda3/envs/mlst/bin/blastn\n[11:37:27] Found 'any2fasta' => /home/pa486/miniconda3/envs/mlst/bin/any2fasta\n[11:37:27] Found blastn: 2.13.0+ (002013)\n[11:37:27] Excluding 3 schemes: vcholerae_2 ecoli abaumannii\n[11:37:32] Found exact allele match mycobacteria_2.S7-19\n[11:37:32] Found exact allele match mycobacteria_2.S12-20\n[11:37:32] Found exact allele match mycobacteria_2.L19-19\n[11:37:32] Found exact allele match mycobacteria_2.L35-15\n[11:37:32] Found exact allele match mycobacteria_2.L16-22\n[11:37:32] Found exact allele match mycobacteria_2.S8-18\n[11:37:32] Found exact allele match mycobacteria_2.S19-20\n[11:37:32] Found exact allele match mycobacteria_2.S14Z-17\nG26832.contigs.fa       mycobacteria_2  215     S14Z(17)        L35(15) S19(20) L19(19) S12(20) S8(18)      L16(22) S7(19)\n[11:37:32] If you have problems, please file at https://github.com/tseemann/mlst/issues\n[11:37:32] Done.\n\n\nAre you happy with the output?\nLet’s say you are, because at least you can scan through and identify the line that contains the information you need. Can you think of a way to just get the line of output you need.\nYes, you can pipe the output to a grep command. Let’s try our hands on this exercise and see if it works.\n\n\n\n\n\n\nExercise 11.1.2.1: Extracting informative line from mlst output\n\n\n\nUsing the mlst command above combined with any text finding command of your choice how will you obtain the expected output below.\nG26832.contigs.fa       mycobacteria_2  215     S14Z(17)        L35(15) S19(20) L19(19) S12(20) S8(18)      L16(22) S7(19)\n\n\n\n\n\n\nSolution:\n\n\n\n\n\nYou probably tried solving this by passing the output of mlst to grep using a | (pipe):\nmlst G26832.contigs.fa | grep \"G26832\"\n[12:40:24] This is mlst 2.22.1 running on linux with Perl 5.032001\n[12:40:24] Checking mlst dependencies:\n[12:40:24] Found 'blastn' => /home/pa486/miniconda3/envs/mlst/bin/blastn\n[12:40:24] Found 'any2fasta' => /home/pa486/miniconda3/envs/mlst/bin/any2fasta\n[12:40:24] Found blastn: 2.13.0+ (002013)\n[12:40:24] Excluding 3 schemes: ecoli vcholerae_2 abaumannii\n[12:40:27] Found exact allele match mycobacteria_2.S7-19\n[12:40:27] Found exact allele match mycobacteria_2.S12-20\n[12:40:27] Found exact allele match mycobacteria_2.L19-19\n[12:40:27] Found exact allele match mycobacteria_2.L35-15\n[12:40:27] Found exact allele match mycobacteria_2.L16-22\n[12:40:27] Found exact allele match mycobacteria_2.S8-18\n[12:40:27] Found exact allele match mycobacteria_2.S19-20\n[12:40:27] Found exact allele match mycobacteria_2.S14Z-17\n[12:40:27] Please also cite 'Jolley & Maiden 2010, BMC Bioinf, 11:595' if you use mlst.\n[12:40:27] Done.\n**G26832**.contigs.fa       mycobacteria_2  215     S14Z(17)        L35(15) S19(20) L19(19) S12(20) S8(18)      L16(22) S7(19)\nAww, but you end up with the same output but with G26832 highlighted.\nI guess we all failed the test. Yes, it does happen sometimes even when you are 100% sure that your command should give you an expected output.\n\n\n\n\n\nAlthough we probably failed the test, let’s not worry, mlst has a special flag that we can use to get a quiet output by using the --quiet option.\nLet’s go ahead and try this out.\n\n\n\n\n\n\nUsing the --quiet option\n\n\n\nmlst --quiet G26832.contigs.fa\nG26832.contigs.fa       mycobacteria_2  215     S14Z(17)        L35(15) S19(20) L19(19) S12(20) S8(18)      L16(22) S7(19)\n\n\nThe --quiet option is also used in many other programs.\nYou actually do not need to specify a --quiet option if the command is used in a pipeline as the output it actually passes on to the next command is just the line of interest.\nYou can check this by outputting our previous raw command to a text file:\nmlst G26832.contigs.fa > G26832.mlst.txt\nNow, view the text file\nhead G26832.mlst.txt\nLet’s have a look at another option that helps you change the name of your sample in the output. This is very useful if your original sample name contains a name that you probably do not want to see in an excel file.\n\n\n\n\n\n\nUsing the --label option\n\n\n\nWe can assign a new name to each sample by applying the --label option and specifying the new preferred name right after the option:\nmlst --quiet --label firstTBsample G26832.contigs.fa\nfirstTBsample   mycobacteria_2  215     S14Z(17)        L35(15) S19(20) L19(19) S12(20) S8(18)  L16(22)     S7(19)\n\n\nNow we see how easy it is to genotype our sample using mlst. Let’s go ahead and type all the samples in our genotyping directory beginning with the letter G.\nThe easy way to do this is to use a wildcard to call all the contigs.fa samples beginning with G and run the mlst on each file.\n\n\n\n\n\n\nGenotype all samples beginning with the letter G\n\n\n\nmlst --quiet G*.contigs.fa\nG26831.contigs.fa       mycobacteria_2  299     S14Z(17)        L35(15) S19(20) L19(19) S12(20) S8(18)      L16(185)        S7(19)\nG26832.contigs.fa       mycobacteria_2  215     S14Z(17)        L35(15) S19(20) L19(19) S12(20) S8(18)      L16(22) S7(19)\nG26854.contigs.fa       mycobacteria_2  215     S14Z(17)        L35(15) S19(20) L19(19) S12(20) S8(18)      L16(22) S7(19)\n\n\nYou can also direct the output to a specified file name say (G_mlst.tsv) and have a look at it with an appropriate spreadsheet application.\nmlst --quiet G*.contigs.fa > G_mlst.tsv\nExamine the ouptut of the analysis and see what you make out of it. Do you observe any possible clone or clonal complex?\nThe samples are probably related if they form a clone. What do you think?\nNB. We can also specify a .csv output using the option --csv, in which case our output should be appropriately labelled as (G_mlst.csv)\n\n\n\n\n\n\nExercise 11.1.2.2: renaming mlst output using a for loop\n\n\n\nConsider all the downloaded and processed samples from NCBI within the current genotyping directory. The processed samples have the prefix ERR or SRR.\nYour task is to apply mlst on all these samples and rename them by removing the suffix contigs.fa.\nYou can try first with multiple steps, however, your final task is to come up with a for loop that performs the task in a single line of code.\nGo ahead and interpret your output after running the command.\n\nHow many species do you see?\nWhat are their ST?\nDo you find anything strange?\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n\nFirst let’s have a look at all the ERR and SRR contigs.fa files in our directory.\nWe can do this by a simple ls with the wildcard *:\nls *RR*.contigs.fa \nERR10437992.contigs.fa  ERR1638070.contigs.fa  ERR369522.contigs.fa\nERR10438004.contigs.fa  ERR1638071.contigs.fa  SRR16634399.contigs.fa\nWe see that we have exactly 6 contigs.fa files with our search.\nNow, if we wanted to perform mlst on one sample (say ERR10437992.contigs.fa) and rename it concurrently, we will do this\nmlst --quiet --label ERR10437992 ERR10437992.contigs.fa\nBut, how do we extract the ID individually to include as a label.\nTo extract just the ID, we can use a for loop as below:\n\n\n\n\n\n\nfor contig in *RR*.contigs.fa \ndo \necho ${contig%.contigs.fa}\ndone\nor nicely in one line as:\nfor contig in *RR*.contigs.fa; do  echo ${contig%.contigs.fa}; done\nERR10437992\nERR10438004\nERR1638070\nERR1638071\nERR369522\nSRR16634399\n\n\n\nNow let’s fit the mlst command somewhere in the for loop:\nfor contig in *RR*.contigs.fa; do mlst --quiet --label ${contig%.contigs.fa} $contig >> downloaded_genome_mlst.tsv; done\n\n\n\n\n\nTill now, we have used the contigs.fa file. let’s try one pseudogenome .fasta file from our short_read_mapping directory and see if we get the same output as with what we did with the contigs.fa file.\nLet’s first copy the file to our current working directory:\ncp ../05_mapping/short_read_mapping_MTB/G26832.fas .\nAnd now perform mlst (focus on only the last two lines of the output):\nmlst --quiet G26832.fas G26832.contigs.fa\n...\nCFastaReader: Hyphens are invalid and will be ignored around line 70535\nCFastaReader: Hyphens are invalid and will be ignored around line 70536\nG26832.fas      mycobacteria_2  215     S14Z(17)        L35(15) S19(20) L19(19) S12(20) S8(18)  L16(22)     S7(19)\nG26832.contigs.fa       mycobacteria_2  215     S14Z(17)        L35(15) S19(20) L19(19) S12(20) S8(18)      L16(22) S7(19)\nViola, we had the same ST. mlst did a great job.\n\nUsing mlst without auto-detection\nYou can force a particular scheme (useful for reporting systems).\nLet’s try this on our famous genome:\nmlst --quiet --scheme mycobacteria_2 G26832.contigs.fa\nG26832.contigs.fa       mycobacteria_2  215     S14Z(17)        L35(15) S19(20) L19(19) S12(20) S8(18)      L16(22) S7(19)\n\n\n\n\n\n\nSpecifying a wrong scheme\n\n\n\nDon’t expect to get anything if you specify a wrong scheme for your organism.\nmlst --quiet --scheme neisseria G26832.contigs.fa\nG26832.contigs.fa       neisseria       -       abcZ(-) adk(-)  aroE(-) fumC(-) gdh(-)  pdhC(-) pgm(-)\n\n\n\n\nGenerating a more friendly format with --legacy and --scheme options\nYou can make mlst behave like older version before auto-detection existed by providing the --legacy parameter with the --scheme parameter. In that case it will print a fixed tabular output with a heading containing allele names specific to that scheme. This is very useful if you are investigating the same species :\nmlst --quiet --legacy --scheme mycobacteria_2 G*.contigs.fa\nG26831.contigs.fa       mycobacteria_2  299     17      15      20      19      20      18      18519\nG26832.contigs.fa       mycobacteria_2  215     17      15      20      19      20      18      22 19\nG26854.contigs.fa       mycobacteria_2  215     17      15      20      19      20      18      22 19\n\n\nOther useful information\n\n\n\n\n\n\nMissing data\n\n\n\nVersion 2.x does not just look for exact matches to full length alleles. It attempts to tell you as much as possible about what it found using the notation below:\n\n\n\n\n\n\n\n\n\nSymbol\nMeaning\nLength\nIdentity\n\n\n\n\nn\nexact intact allele\n100%\n100%\n\n\n~n\nnovel full length allele similar to n\n100%\n≥ --minid\n\n\nn?\npartial match to known allele\n≥ --mincov\n≥ --minid\n\n\n-\nallele missing\n< --mincov\n< --minid\n\n\nn,m\nmultiple alleles\n \n \n\n\n\n\n\n\n\n\n\n\n\nScoring system\n\n\n\nEach MLST prediction gets a score out of 100. The score for a scheme with N alleles is as follows:\n\n+90/N points for an exact allele match e.g. 42\n+63/N points for a novel allele match (50% of an exact allele) e.g. ~42\n+18/N points for a partial allele match (20% of an exact alelle) e.g. 42?\n0 points for a missing allele e.g. -\n+10 points if there is a matching ST type for the allele combination\n\nIt is possible to filter results using the --minscore option which takes a value between 1 and 100. If you only want to report known ST types, then use --minscore 100. To also include novel combinations of existing alleles with no ST type, use --minscore 90. The default is --minscore 50 which is an ad hoc value I have found allows for genuine partial ST matches but eliminates false positives.\n\n\nTo know more about the below topics, visit the github page tseemann/mlst\n\nMapping to genus/species\nUpdating the database\nAdding a new scheme\n\n\n\n\n\n\n\nChange working environment\n\n\n\nStill in the genotyping_and_dr directory run the below command to deactivate the mlst environment and activate the seroba environment:\nmamba deactivate\nmamba activate seroba"
  },
  {
    "objectID": "materials/11-genotyping/11.1-genotyping.html#serotyping",
    "href": "materials/11-genotyping/11.1-genotyping.html#serotyping",
    "title": "11.1 Bacterial Genotyping and Drug Resistance Prediction",
    "section": "11.1.3 Serotyping",
    "text": "11.1.3 Serotyping\nA serotype or serovar is a distinct variation within a species of bacteria or virus or among immune cells of different individuals. These microorganisms, viruses, or cells are classified together based on their surface antigens, allowing the epidemiologic classification of organisms to the subspecies level. A group of serovars with common antigens is called a serogroup or sometimes serocomplex.\n Figure adapted from https://en.wikipedia.org/wiki/Serotype.\nSerotyping often plays an essential role in determining species and subspecies. The Salmonella genus of bacteria, for example, has been determined to have over 2600 serotypes. Vibrio cholerae, the species of bacteria that causes cholera, has over 200 serotypes, based on cell antigens — however, only two of them have been observed to produce the potent enterotoxin that results in cholera: O1 and O139.\nSo you can imaging that being able to accurately identify serotypes of strains can already give you some very important information about the pathogenicity of the bacterial strain.\n\n\n\n\n\n\nWhat has proteins got to do in a genomics class?\n\n\n\nIf you are wondering why we are discussing antigens (proteins) in a genomics (DNA) workshop, don’t worry, you are not the only one and perhaps this is the right place to discuss this. However, we will not go further with our discussion. If you can recall from your basic molecular biology class, all proteins are products of DNA. So there we go, all the information in the antigens (proteins) will actually be coded in the genome (DNA), so let’s go ahead and do some investigations of our genome to predict serotypes of bacterial strains.\n\n\n\nThere are a number of bioinformatics tools available for serotyping bacterial strains. Most of these, however, are species specific tools:\nFor Salmonella spp.\n\nSeqSero\nsistr_cmd\nSeroTools\n\nFor Neisseria meningitidis\n\nmeningotype\n\nFor Escherichia coli\n\nECTyper\necoli_serotyper\n\nFor Shigatoxin producing E. coli (STEC)\n\nSTECFinder\n\nFor Vibrio parahaemolyticus\n\nVPsero\n\nFor Pseudomonas aeruginosa\n\nPAst\n\nFor Listeria monocytogenes\n\nLisSero\n\nFor Shigella\n\nShigEiFinder\nShigaPass\n\nFor Streptococcus pneumoniae\n\nseroBA\nPneumoCaT\nSeroCall\nSerotyping\nseqSerotyper\n\nFor Streptococcus suis\n\nSsuisSerotyping_pipeline\n\nFor Streptococcus agalactiae\n\nmlst_Sagalactiae\n\nFor this workshop, we will focus on one main tool — seroBA.\n\nseroBA\nSeroBA is a k-mer based Pipeline to identify the Serotype from Illumina NGS reads for given references. You can use SeroBA to download references from (https://github.com/phe-bioinformatics/PneumoCaT) to identify the capsular type of Streptococcus pneumoniae.\nSeroBA can predict serotypes, by identifying the cps locus, directly from raw whole genome sequencing read data with 98% concordance using a k-mer based method, can process 10,000 samples in just over one day using a standard server and can call serotypes at a coverage as low as 10x.\nYou can visit the publication to read more about seroBA\n\n\n\n\n\n\nHelp\n\n\n\nDo this to get the help information for seroBA\nseroba --help\nusage: seroba <command> <options>\n\noptional arguments:\n  -h, --help     show this help message and exit\n\nAvailable commands:\n  \n    getPneumocat\n                 downloads genetic information from PneumoCat\n    createDBs    creates Databases for kmc and ariba\n    runSerotyping\n                 identify serotype of your input data\n    summary      output folder has to contain all folders with prediction results\n    version      Get versions and exit\n\nseroba runSerotyping -h\nusage: seroba runSerotyping [options]  <read1> <read2> <prefix>\n\nidentify serotype of your input data\n\npositional arguments:\n  read1                 forward read file\n  read2                 backward read file\n  prefix                unique prefix\n\noptional arguments:\n  -h, --help            show this help message and exit\n\nOther options:\n  --databases DATABASES\n                        path to database directory, default\n                        /home/pa486/miniconda3/envs/seroba/share/seroba-1.0.2/database\n  --noclean             Do not clean up intermediate files (assemblies, ariba report)\n  --coverage COVERAGE   threshold for k-mer coverage of the reference sequence , default = 20\n\n\n:::{.callout} ## Usage The general format of the command is:\nseroba runSerotyping [options]  <databases directory> <read1> <read2> <prefix>\nOptional arguments:\n- noclean NOCLEAN  Do not clean up intermediate files (assemblies, ariba report)\n- coverage COVERAGE  threshold for k-mer coverage of the reference sequence (default = 20)  \nYou can also use the below command to summarize the output in one .tsv file\nseroba summary  <output folder>\nNB.\n\nis directory where the output directories from seroba runSerotyping are stored\n:::\n\n\n\n\n\n\nInput\n\n\n\n\nThe command requires a pair of fastq files as input files.\nseroba database. seroBA debends on a database to run — without which the process will fail. This is how seroBA functions. In your working directory, you will find a file named seroba_db — this is the compiled databse for seroBA. Without specifying a seroba_db database in the working directory, none of the commands used here will work.\n\n\n\n\n\n\n\n\n\nOutput\n\n\n\nIn the folder ‘prefix’ you will find a pred.tsv including your predicted serotype as well as a file called detailed_serogroup_info.txt including information about SNP, genes, and alleles that are found in your reads. After the use of “seroba summary” a tsv file called summary.tsv is created that consists of three columns (sample Id , serotype, comments). Serotypes that do not match any reference are marked as “untypable”(v0.1.3) \n\n\n\n\n\nFlowchart outlining the main steps of the SeroBA algorithm\n\n\nWe are now ready to perform our serotyping.\nFirst, let’s try our hands on knowing what the serotype of our first pneumococcal genome is. Our first pneumococcal genome to analyse is ERR1638213 and we will use the ERR1638213.R?.trim.fastq.gz files as our input.\nLet’s go ahead and run the below command to genotype our first sample:\n\n\n\n\n\n\nseroba runSerotyping — error\n\n\n\nseroba runSerotyping --databases seroba_db/ ERR1638213.R1.trim.fastq.gz ERR1638213.R2.trim.fastq.gz ERR1638213\noops! we get an error message:\nNames for forwards and reverse reads does not match. Cannot continue\n\n\nLet’s fix this error. It turns out that the developers of seroBA require the .fastq input files to be in a particular format. Instead of ERR1638213.R1.trim.fastq.gz they prefer ERR1638213_R1.trim.fastq.gz. Thus, the . in .R1 and .R2, should be replaced with a _ to give _R1and _R2.\n\n\n\n\n\n\nNote\n\n\n\n.fastq files are normally produces with any of ’_R1’, ’_R2’, ’_1’ or ’_2’ to differentiate the forward and reverse reads during paired-end Illumina sequencing. In the example above, we have used .R to name some of our files just to let you know how some tools may be strict on what name they require as input files. It is not really the best practice to write a script with such a limitation.\n\n\nLet’s try and fix the error by attempting the below exercise to replace all .Rs with _R.\n\n\n\n\n\n\nExercise 11.1.3.1: replace . with _ in a file name\n\n\n\nUsing any command of your choice, write out a simple line of code to replace all .Rs with _R in all the trimmed fastq files that have this format.\n\n\nHint\n\nYou can try using mv. If you’ve forgotten the general format of the command you can refresh your memory by going back to renaming files. Remember to use echo to also test it on several files using a for loop.\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n\nTo rename just one file you run this command:\nmv ERR1638213.R1.trim.fastq.gz ERR1638213_R1.trim.fastq.gz\nYou can use a for loop to rename all the trimmed fastq files with the .R format.\nRun this to rename all the .R1.trim.fastq.gz files\nfor i in *.R1.trim.fastq.gz ; do mv $i ${i%.R1*}_R1.trim.fastq.gz; done\nRun this to rename all the .R2.trim.fastq.gz files\nfor i in *.R2.trim.fastq.gz ; do mv $i ${i%.R2*}_R2.trim.fastq.gz; done\nAlternatively, you can use the below command to rename all the files at a go:\nfor i in *.R?.trim.fastq.gz ; do mv $i ${i%.R*}_${i#*.}; done\n\n\n\n\n\nLet’s go ahead and rename all the .R1.trim.fastq.gz files to _R1.trim.fastq.gz using our solution from the exercise above.\nNow let’s try running our first sample again.\n\n\n\n\n\n\nseroba runSerotyping\n\n\n\nseroba runSerotyping --databases seroba_db/ ERR1638213_R1.trim.fastq.gz ERR1638213_R2.trim.fastq.gz ERR1638213\n -ci4  -m2 -t1\n/home/pa486/miniconda3/envs/seroba/bin/kmc -k71  -ci4  -m2 -t1 ERR1638213_R1.trim.fastq.gz /tmp/temp.kmc7vn6w_17/ERR1638213 /tmp/temp.kmc7vn6w_17\n*****************************\nStage 1: 100%\nStage 2: 100%\n1st stage: 0.988177s\n2nd stage: 2.33987s\nTotal    : 3.32805s\nTmp size : 24MB\n\nStats:\n   No. of k-mers below min. threshold :       589835\n   No. of k-mers above max. threshold :            0\n   No. of unique k-mers               :      2642594\n   No. of unique counted k-mers       :      2052759\n   Total no. of k-mers                :     26123286\n   Total no. of reads                 :       167385\n   Total no. of super-k-mers          :       923602\n...\n\n\nViola, now we get an output.\nLet’s see how we interpret the output.\n\n\n\n\n\n\ninterpreting pred.tsv output\n\n\n\nLook at the file called pred.tsv in your results directory — ERR1638213/.\ncat ERR1638213/pred.tsv\nERR1638213      01      contamination\nYou can see three columns. The first one contains the prefix you chose for the run. The second one contains the predicted serotype and the third column may contain a comment regarding contamination. So, in this case we can see that ERR1638213 was predicted to be of serotype 01 and at least 10% of the reads are called as a different snp than the other reads i.e. there is contamination.\n\n\nWe can now proceed to run seroba runSerotyping on all the remaining Streptococcus pneumoniae samples below using a for loop:\nNB SeroBA tool is for serotyping only Streptococcus pneumoniae strains and so we don’t need to type all the strains on our current directory. Nevertheless, let’s go ahead and serotype all our samples using seroba runSerotyping and see what we get in our output.\n\n\n\n\n\n\nseroba runSerotyping — for loop\n\n\n\nBecause we have already typed our first sample, we may get an error message when we try to rerun the serotyping on the same sample.\nLet’s remove the output directory of our first sample — ERR1638213/\nrm -r ERR1638213/\nNow let’s run seroba runSerotyping on all our samples\nfor i in *_R1.trim.fastq.gz; do echo seroba runSerotyping --databases seroba_db/ $i ${i%_R*}_R2.trim.fastq.gz ${i%_R*}; done\nThis will take about 15 minutes …\n -ci4  -m2 -t1\n/home/pa486/miniconda3/envs/seroba/bin/kmc -k71  -ci4  -m2 -t1 ERR097430_R1.trim.fastq.gz /tmp/temp.kmctiwe1a3b/ERR097430 /tmp/temp.kmctiwe1a3b\n**************************************************************************************\nStage 1: 100%\nStage 2: 100%\n1st stage: 3.72407s\n2nd stage: 1.42577s\nTotal    : 5.14984s\nTmp size : 66MB\n\nStats:\n   No. of k-mers below min. threshold :      1073070\n   No. of k-mers above max. threshold :            0\n   No. of unique k-mers               :      2499001\n   No. of unique counted k-mers       :      1425931\n   Total no. of k-mers                :     11763976\n   Total no. of reads                 :      3908241\n   Total no. of super-k-mers          :      3403220\n...\n\n\nNow that we have performed multiple runs, we might want to create a summary of the results. To do this, let’s run the seroba summary option.\n\n\n\n\n\n\nSummarizing the outputs with seroba summary\n\n\n\nWe will run the command in the current directory .. We use . to specify the directory where our output files are located.\nseroba summary .\nHave a look at the resulting .tsv file.\ncat summary.tsv\nERR097430       Swiss_NT        contamination\nERR10310429     19F\nERR10437992     coverage too low\nERR10438004     coverage too low\nERR1638070      coverage too low\nERR1638071      coverage too low\nERR1638213      01      contamination\nERR369522       coverage too low\nG26831  coverage too low\nG26832  coverage too low\nG26854  coverage too low\nSRR16634399     coverage too low\nWhat do make you of the results?\nWe see that those samples that are not Streptococcus pneumoniae have coverage too low. This is because, the SNPs or sequences being investigated by seroBA can’t be found in these other species.\n\n\n\n\n\n\n\n\nTroubleshooting\n\n\n\nCase 1:\n\nSeroBA predicts ‘untypable’. An ‘untypable’ prediction can either be a real ‘untypable’ strain or can be caused by different problems. Possible problems are: bad quality of your input data, submission of a wrong species or to low coverage of your sequenced reads. Please check your data again and run a quality control.\n\nCase 2:\n\nLow alignment identity in the ‘detailed_serogroup_info’ file. This can be a hint for a mosaic serotype.\nPossible solution: perform a blast search on the whole genome assembly\n\nCase 3:\n\nThe third column in the summary.tsv indicates “contamination”. This means that at least one heterozygous SNP was detected in the read data with at least 10% of the mapped reads at the specific position supporting the SNP.\nPossible solution: please check the quality of your data and have a look for contamination within your reads"
  },
  {
    "objectID": "materials/11-genotyping/11.1-genotyping.html#mycobacterium-tuberculosis-specific-genotyping-and-amr-prediction-tools",
    "href": "materials/11-genotyping/11.1-genotyping.html#mycobacterium-tuberculosis-specific-genotyping-and-amr-prediction-tools",
    "title": "11.1 Bacterial Genotyping and Drug Resistance Prediction",
    "section": "11.1.4 Mycobacterium tuberculosis specific genotyping and AMR prediction tools",
    "text": "11.1.4 Mycobacterium tuberculosis specific genotyping and AMR prediction tools\nThe genotyping and serotyping tools we have explored above are all very useful for bacteria typing.\nHowever, same can not be said for Mycobacterium tuberculosis typing. You should have realized by now that most of the tools used above are meaningless for typing Mycobacterium tuberculosis.\nMycobacterium tuberculosis, like other Mycobacterium species, are a very unique group of bacteria which also require specialized tools.\nFor this workshop, we will investigate two main tools — SpoTyping and tb-profiler. Where as the former is used for only genotyping, the latter can be used for both genotyping and antimicrobial resistance (AMR) prediction.\nLet’s now explore both tools.\n\n\n\n\n\n\nChange working environment\n\n\n\nStill in the genotyping_and_dr directory run the below command to deactivate the seroba environment and activate the spoligotyping environment:\nmamba deactivate\nmamba activate spoligotyping\n\n\n\nSpoTyping\nSpoTyping is a fast and accurate program for in silico spoligotyping of Mycobacterium tuberculosis isolates from next-generation sequencing reads. It predicts spoligotypes from sequencing reads, complete genomic sequences and assembled contigs. You can access the publication by clicking here or visit the github page.\nBefore we explore the SpoTyping tool, let’s have a look at what spoligotyping is.\nSpoligotyping is a simple method which allows simultaneous detection and typing of M. tuberculosis in clinical specimens and reduces the time between suspicion of the disease and typing from 1 or several months to 1 or 3 days. The method is based on polymorphism of the chromosomal direct repeat (DR) locus, which contains a variable number of short direct repeats interspersed with nonrepetitive spacers.\nThe method is referred to as spacer oligonucleotide typing or “spoligotyping” because it is based on strain-dependent hybridization patterns of in vitro-amplified DNA with multiple spacer oligonucleotides.\n\n\n\n(A) Structure of the DR locus in the mycobacterial genome. The chromosomes of M. tuberculosis H37Rv and M. bovis BCG contain 48 and 41 DRs, respectively (depicted as rectangles), which are interspersed with unique spacers varying in length from 35 to 41 bp. The (numbered) spacers used correspond to 37 spacers from M. tuberculosis H37Rv and 6 from M. bovis BCG. The site of integration of insertion element IS6110 is depicted. (B) Principle of in vitro amplification of the DR region by PCR. Any DR in the DR region may serve as a target for these primers; therefore, the amplified DNA is composed of a mixture of a large number of different-size fragments. Shown is the combination of fragments that would be produced by in vitro amplification of a DR target containing only five contiguous DRs. Kamerbeek et. al., 1997\n\n\n\n\n\nHybridization patterns (spoligotypes) of amplified mycobacterial DNAs of 35 M. tuberculosis and 5 M. bovis strains. The order of the spacers on the filter corresponds to their order in the genome. Note that the spoligotype of strains 6, 12, and 37 corresponds to that of strains from the Beijing genotypic group. Kamerbeek et. al., 1997\n\n\nMost TB clinical isolates show unique hybridization patterns, whereas outbreak strains share the same spoligotype. Spoligotyping is able to differentiate M. bovis from M. tuberculosis, a distinction which is often difficult to make by traditional methods.\n\n\n\n\n\n\nSequences of 43 oligonucleotides mostly used in spoligotyping\n\n\n\n\n\n\n\n\n\n\n\n\n\nSpacer no.\nOligonucleotide sequence\n\nSpacer no.\nOligonucleotide sequence\n\n\n\n\n1\nATAGAGGGTCGCCGGTTCTGGATCA\n\n23\nAGCATCGCTGATGCGGTCCAGCTCG\n\n\n2\nCCTCATAATTGGGCGACAGCTTTTG\n\n24\nCCGCCTGCTGGGTGAGACGTGCTCG\n\n\n3\nCCGTGCTTCCAGTGATCGCCTTCTA\n\n25\nGATCAGCGACCACCGCACCCTGTCA\n\n\n4\nACGTCATACGCCGACCAATCATCAG\n\n26\nCTTCAGCACCACCATCATCCGGCGC\n\n\n5\nTTTTCTGACCACTTGTGCGGGATTA\n\n27\nGGATTCGTGATCTCTTCCCGCGGAT\n\n\n6\nCGTCGTCATTTCCGGCTTCAATTTC\n\n28\nTGCCCCGGCGTTTAGCGATCACAAC\n\n\n7\nGAGGAGAGCGAGTACTCGGGGCTGC\n\n29\nAAATACAGGCTCCACGACACGACCA\n\n\n8\nCGTGAAACCGCCCCCAGCCTCGCCG\n\n30\nGGTTGCCCCGCGCCCTTTTCCAGCC\n\n\n9\nACTCGGAATCCCATGTGCTGACAGC\n\n31\nTCAGACAGGTTCGCGTCGATCAAGT\n\n\n10\nTCGACACCCGCTCTAGTTGACTTCC\n\n32\nGACCAAATAGGTATCGGCGTGTTCA\n\n\n11\nGTGAGCAACGGCGGCGGCAACCTGG\n\n33\nGACATGACGGCGGTGCCGCACTTGA\n\n\n12\nATATCTGCTGCCCGCCCGGGGAGAT\n\n34\nAAGTCACCTCGCCCACACCGTCGAA\n\n\n13\nGACCATCATTGCCATTCCCTCTCCC\n\n35\nTCCGTACGCTCGAAACGCTTCCAAC\n\n\n14\nGGTGTGATGCGGATGGTCGGCTCGG\n\n36\nCGAAATCCAGCACCACATCCGCAGC\n\n\n15\nCTTGAATAACGCGCAGTGAATTTCG\n\n37\nCGCGAACTCGTCCACAGTCCCCCTT\n\n\n16\nCGAGTTCCCGTCAGCGTCGTAAATC\n\n38\nCGTGGATGGCGGATGCGTTGTGCGC\n\n\n17\nGCGCCGGCCCGCGCGGATGACTCCG\n\n39\nGACGATGGCCAGTAAATCGGCGTGG\n\n\n18\nCATGGACCCGGGCGAGCTGCAGATG\n\n40\nCGCCATCTGTGCCTCATACAGGTCC\n\n\n19\nTAACTGGCTTGGCGCTGATCCTGGT\n\n41\nGGAGCTTTCCGGCTTCTATCAGGTA\n\n\n20\nTTGACCTCGCCAGGAGAGAAGATCA\n\n42\nATGGTGGGACATGGACGAGCGCGAC\n\n\n21\nTCGATGTCGATGTCCCAATCGTCGA\n\n43\nCGCAGAATCGCACCGGGTGCGGGAG\n\n\n22\nACCGCAGACGGCACGATTGAGACAA\n\n\n\n\n\n\n\n\nYou can read more on Spoligotyping by accessing this publication.\nNow, let’s move on to the typing tool.\nSpoTyping basically extracts hexadecimal and binary spoligotype codes from fastq files.\nIt achieves high accuracy for reads of both uniform and varying lengths, and is about 20 to 40 times faster than SpolPred — another spoligotyping tool. SpoTyping also integrates the function of producing a report summarizing associated epidemiological data from a global database of all isolates having the same spoligotype.\n\n\n\nA schematic representation of the SpoTyping workflow. If the specified input contains sequencing reads, SpoTyping first concatenates the sequencing reads to form an artificial sequence. The artificial sequence, or genetic sequences when the input contains complete genomic sequence or assembled contigs, would be built into the BLAST database. After querying the 43 spacer sequences in the database, the results are parsed to count the number of hits for each spacer sequence. A hit threshold is set to define a spacer as ‘present’ in the genome, resulting in a 43-digit binary code with 1 as present and 0 as absent, which is further translated into the octal code of the spoligotype. The SITVIT database is then queried to identify matching isolates having the same spoligotype, where the associated data of the matched isolates are downloaded and summarized as pie charts. Xia, E., Teo, YY. & Ong, R.TH 2015\n\n\n\n\n\n\n\n\nHelp\n\n\n\nDo this to get the help information for SpoTyping\nSpoTyping.py --help\nUsage: python SpoTyping.py [options] FASTQ_1/FASTA FASTQ_2(optional)\n\nOptions:\n  --version             show program's version number and exit\n  -h, --help            show this help message and exit\n  --seq                 Set this if input is a fasta file that contains only a\n                        complete genomic sequence or assembled contigs from an\n                        isolate [Default is off]\n  -s SWIFT, --swift=SWIFT\n                        swift mode, either \"on\" or \"off\" [Defulat: on]\n  -m MIN_STRICT, --min=MIN_STRICT\n                        minimum number of error-free hits to support presence\n                        of a spacer [Default: 0.1*average read depth]\n  -r MIN_RELAX, --rmin=MIN_RELAX\n                        minimum number of 1-error-tolerant hits to support\n                        presence of a spacer [Default: 0.12 * average read\n                        depth]\n  -O OUTDIR, --outdir=OUTDIR\n                        output directory [Default: running directory]\n  -o OUTPUT, --output=OUTPUT\n                        basename of output files generated [Default:\n                        SpoTyping]                        \n...\nYou will notice that, what we just did was to call out a python script SpoTyping.py in the current directory. This is how SPoTyping functions. Without the SpoTyping.py script in the working directory, none of the commands used here will work.\n\n\n\n\n\n\n\n\nUsage\n\n\n\nThe general format of the command is:\npython SpoTyping.py [options] FASTQ_1 FASTQ_2(optional)\n\n\n\n\n\n\n\n\nInput\n\n\n\n\nFastq file or pair-end fastq files\nFasta file of a complete genomic sequence or assembled contigs of an isolate\n\n\n\n\n\n\n\n\n\nOutput\n\n\n\n\nIn the output file specified: predicted spoligotype in the format of binary code and octal code.\nIn the output log file: count of hits from BLAST result for each spacer sequence.\nIn the .xls excel file: spoligotype query result downloaded from SITVIT WEB.\n\nNote: if the same spoligotype is queried before and have an xls file in the output directory, it will not be queried again.\n\n\nWe are now ready to perform our SPoTyping.\nFirst, let’s try our hands on knowing what the spoligotype of our first genome is. As this is a TB specific tool, our first genome to analyse is G26832 and we will first use the G26832_R?.trim.fastq.gz files as our input and also try with the G26832.contigs.\nLet’s go ahead and run the below command with no options to genotype our sample:\n\n\n\n\n\n\nSpoTyping\n\n\n\nBecause SpoTyping generated a lot of output, we will store all its output into a new directory.\nLet’s create our new directory to keep our output files:\nmkdir spoligotype_results\nNB. We will also need to specify our output name -o.\n\n\n\n\n\n\n.fastq as input\n\n\n\nSpoTyping.py G26832_R1.trim.fastq.gz G26832_R2.trim.fastq.gz -o spoligotypes\nThis may take about a minute to run.\n\nBuilding a new DB, current time: 12/05/2022 16:26:16\nNew DB name:   /rds/user/pa486/hpc-work/workshop_files_Bact_Genomics_2023/12_genotyping_delete/spoligotypes.SpoTyping.tmp.0\nNew DB title:  ./spoligotypes.SpoTyping.tmp.0\nSequence type: Nucleotide\nKeep MBits: T\nMaximum file size: 3000000000B\nAdding sequences from FASTA; added 1 sequences in 3.40358 seconds.\n\n\n\nLet’s have a look at the spoligotype pattern called.\ncat spoligotypes\nG26832_R1.trim.fastq.gz&G26832_R2.trim.fastq.gz 1111111111111111111111000111111100001111111     777777743760771\nNow let’s repeat the SpoTyping but this time with our contig file.\n\n\n\n\n\n\n.contig as input\n\n\n\nNB.\n\nSpecifying the same output file will add the results to the existing one.\nWhen using contigs as inputs, you will need to include --seq option to tell SpoTyping that you are using an input fasta file that contains only complete genomic sequence or assembled contigs from an isolate.\n\nSpoTyping.py --seq G26832.contigs.fa -o spoligotypes\n\nBuilding a new DB, current time: 12/05/2022 16:34:48\nNew DB name:   /rds/user/pa486/hpc-work/workshop_files_Bact_Genomics_2023/12_genotyping_delete/G26832.contigs.fa\nNew DB title:  G26832.contigs.fa\nSequence type: Nucleotide\nKeep MBits: T\nMaximum file size: 3000000000B\nAdding sequences from FASTA; added 179 sequences in 0.144553 seconds.\n\n\n\nYou can now look at both spoligotypes. Are they same?\ncat spoligotypes\nG26832_R1.trim.fastq.gz&G26832_R2.trim.fastq.gz 1111111111111111111111000111111100001111111 777777743760771\nG26832.contigs.fa   1111111111111111111111000111111100001111111 777777743760771\n\n\n\n\n\n\n\n\nExercise 11.1.4.1: Perform SpoTyping on remaining TB genomes\n\n\n\nIn your working directory, you will find two other TB genomes.\n\nUsing the contig files run SpoTyping on these two genomes.\nReport your results.\n\n\n\n\n\n\n\nSolution:\n\n\n\n\n\nThe two TB genomes left are G26831 and G26854.\nWe perform SpoTyping by running the following commands:\nSpoTyping.py --seq G26831.contigs.fa -o spoligotypes\nSpoTyping.py --seq G26854.contigs.fa -o spoligotypes\n\n\n\n\n\n\n\n\n\n\n\nSuggestions:\n\n\n\nIt’s highly suggested to use the swift mode (set as the default) if the sequencing throughput is no less than 135Mbp.\nFor sequencing experiments with throughputs below 135Mbp, please adjust the thresholds to be 0.0180 to 0.1486 times the estimated read depth for error-free hits and 0.0180 to 0.1488 times the estimated read depth for 1-error-tolerant hits. (The read depth is estimated by dividing the sequencing throughput by 4,500,000, which is the estimated Mtb genome length.)\nIf you do wish to take in all reads for sequencing experiments with throughputs above 1260Mbp, please adjust the thresholds to be 0.0180 to 0.1486 times the estimated read depth for error-free hits and 0.0180 to 0.1488 times the estimated read depth for 1-error-tolerant hits.\n\n\nYou can produce summary pie chart plot from the .xls files. We may not have time to do this but you can try it out in R.\n\n\n\n\n\n\nExercise 11.1.4.2: Advance Exercise — Plotting in R\n\n\n\nFollowing the instructions below, make a plot of your results.\nFirst download the .xls file to your desktop or downloads\nPrerequisites:\n\nR\nR package: gdata\n\nInput:\n\nThe xls file downloaded from SITVIT WEB.\n\nOutput:\nA pdf file with the information in the .xls file summarized with pie charts.\nUsage:\nRscript SpoTyping_plot.r query_from_SITVIT.xls output.pdf\nAn example call:\nRscript SpoTyping_plot.r SITVIT_ONLINE.777777477760771.xls SITVIT_ONLINE.777777477760771.pdf\n\n\n\n\n\n\nSolution:\n\n\n\n\n\n\n\n\n\n\n\nThe hexadecimal and octal codes generated from SpoTyping can predict the lineage of the TB sample you are dealing with.\nHowever, a more accurate TB lineage typing tool is TBprofiler. Let’s explore that in our next session.\n\n\n\n\n\n\nChange working environment\n\n\n\nStill in the genotyping_and_dr directory run the below command to deactivate the spoligotyping environment and activate the tbprofiler environment:\nmamba deactivate\nmamba activate tbprofiler\n\n\n\n\nlineage assignment and antimicrobial resistance prediction using tb-profiler\ntb-profiler is a tool used to detect resistance and lineages of M. tuberculosis genomes.\nIt is made up of a pipeline which by default uses trimmomatic to trim reads, aligns the reads to the H37Rv reference using bowtie2, BWA or minimap2 and then calls variants using bcftools. These variants are then compared to a drug-resistance database. The tool also predicts the number of reads supporting drug resistance variants as an insight into hetero-resistance (not applicable for minION data).\n\n\n\nTB-profiler pipeline\n\n\n\n\n\n\n\n\nKeeping up to date\n\n\n\nNote that, like many other database-based tools TBProfiler is under constant rapid development. If you plan to use the program in your work please make sure you are using the most up to date version! Similarly, the database is not static and is continuously being improved so make sure you are using the most latest version. If you use TBProfiler in your work please state the version of both the tool and the database as they are developed independently from each other.\n\n\nThere is an online version of the tool which is very useful for analysing few genomes. You can try it out later at your free time by following this link.\n\n\n\ntb-profiler online tool\n\n\nFor this workshop, we will focus on analysing our genomes the command-line way so let’s get started.\n\n\n\n\n\n\nHelp\n\n\n\n\n\n\n\n\n\ntb-profiler\n\n\n\nDo this to view the various usage options of tb-profiler\ntb-profiler\nusage: tb-profiler\n                   {profile,vcf_profile,fasta_profile,lineage,spoligotype,collate,reprofile,reformat,create_db,update_tbdb,batch,list_db,version}\n                   ...\n\nTBProfiler pipeline\n\npositional arguments:\n  {profile,vcf_profile,fasta_profile,lineage,spoligotype,collate,reprofile,reformat,create_db,update_tbdb,batch,list_db,version}\n                        Task to perform\n    profile             Run whole profiling pipeline\n    vcf_profile         Run profiling pipeline on VCF file. Warning: this assumes that you have good\n                        coverage across the genome\n    fasta_profile       Run profiling pipeline on Fasta file. Warning: this assumes that this is a good\n                        quality assembly which covers all drug resistance loci\n    lineage             Profile only lineage\n    spoligotype         Profile spoligotype (experimental feature)\n    collate             Collate results form multiple samples together\n...\n\n\nThe main positional argument we will explore in this workshop is the profile.\nDo this to get the help information for tb-profiler profile\n\n\n\n\n\n\ntb-profiler profile\n\n\n\ntb-profiler profile --help\nusage: tb-profiler profile [-h] [--read1 READ1] [--read2 READ2] [--bam BAM] [--fasta FASTA] [--vcf VCF]\n                           [--platform {illumina,nanopore}] [--db DB] [--external_db EXTERNAL_DB]\n                           [--prefix PREFIX] [--csv] [--txt] [--pdf] [--output_template OUTPUT_TEMPLATE]\n                           [--add_columns ADD_COLUMNS] [--call_whole_genome] [--dir DIR]\n                           [--mapper {bwa,minimap2,bowtie2,bwa-mem2}]\n...\noptional arguments:\n  -h, --help            show this help message and exit\n\nInput options:\n  --read1 READ1, -1 READ1\n                        First read file (default: None)\n  --read2 READ2, -2 READ2\n                        Second read file (default: None)\n  --bam BAM, -a BAM     BAM file. Make sure it has been generated using the H37Rv genome\n                        (GCA_000195955.2) (default: None)\n  --fasta FASTA, -f FASTA\n...\nOutput options:\n  --prefix PREFIX, -p PREFIX\n                        Sample prefix for all results generated (default: tbprofiler)\n  --csv                 Add CSV output (default: False)\n...\n\n\n\n\nAs you can see there are a lot of elaborate uses of tb-profile profile and it accepts various options as inputs and outputs information into various file formats.\nWe will look at its very simple use.\n\n\n\n\n\n\nUsage\n\n\n\nThe general format of the tb-profiler command is:\ntb-profiler {profile,vcf_profile,fasta_profile,lineage,spoligotype,collate,reprofile,reformat,create_db,update_tbdb,batch,list_db,version}\nAll the commands you see in {} above are the positional arguments. You can try your hands on each one at your free time and see what it does. The simple way to see what each positional argument does is by calling out its help function. eg. tb-profiler lineage --help to call out the help function for the positional argument lineage.\nFor now let’s focus on the profile argument.\nThe general format of the tb-profiler profile command is:\ntb-profiler profile --read1 /path/to/reads_1.fastq.gz --read2 /path/to/reads_2.fastq.gz -p prefix\nThe first argument indicates the analysis type to perform. At the moment the tool currently only support the calling of small variants.\nThe prefix (-p) is useful when you need to run more that one sample. This will store BAM, VCF and result files in respective directories.\n\n\n\n\n\n\n\n\nInput\n\n\n\n\nDepending on the positional argument called, the command will require various types of inputs. For the profile argument, a pair of fastq files as input files are required.\ntbdb database. Just like seroBA, tb-profiler also heavily depends on a database to run — without which the process will fail. This is how tb-profiler functions. The tb-profiler database tbdb was installed during the setting up of the environment and will be located in one of the sub-directories in our working environment probably named tbprofiler.  It is possible to compile your own tbdb database and point your command to the location of the database with the option --external_db. You will find this useful if you need to update the native tbdb to include novel SNP data that are not already there.\n\n\n\n\n\n\n\n\n\nUpdating the tbdb database\n\n\n\nNew mutations/genes are periodically added to the database. Run the following to make sure you are up to date before you run any analysis.\ntb-profiler update_tbdb\n\n\n\n\n\n\n\n\nOutput\n\n\n\n\nBy default, for the profile argument, results are output in .json and text formats.\nThree directories are created where outputs are stored.\n\nvcf directory\nbam directory\nresults directory — where the .json file are stored\n\n\n\n\nYou can explore more on the detailed use of tb-profiler by looking at its documentation here or have a look at their publication here.\nWe are now ready to perform our tb-profiling.\nFirst, let’s try our hands on knowing what the genotypic profile of our first genome is. Just like for spoligotyping, this is a TB-specific tool so we will focus on only the TB genomes. Our first genome to analyse is G26832 and we will use the G26832_R?.trim.fastq.gz files as our input.\nLet’s go ahead and run the below command with no options to concurrently genotype our sample and identify the drugs that it may be resistant to:\n\n\n\n\n\n\ntb-profiler profile — minimal options\n\n\n\ntb-profiler profile --read1 G26832_R1.trim.fastq.gz --read2 G26832_R2.trim.fastq.gz --prefix G26832\nThis will take about 7 minutes to run\nUsing ref file: /home/pa486/miniconda3/envs/tbprofiler/share/tbprofiler//tbdb.fasta\nUsing gff file: /home/pa486/miniconda3/envs/tbprofiler/share/tbprofiler//tbdb.gff\nUsing bed file: /home/pa486/miniconda3/envs/tbprofiler/share/tbprofiler//tbdb.bed\nUsing version file: /home/pa486/miniconda3/envs/tbprofiler/share/tbprofiler//tbdb.version.json\nUsing json_db file: /home/pa486/miniconda3/envs/tbprofiler/share/tbprofiler//tbdb.dr.json\nUsing variables file: /home/pa486/miniconda3/envs/tbprofiler/share/tbprofiler//tbdb.variables.json\nUsing spoligotype_spacers file: /home/pa486/miniconda3/envs/tbprofiler/share/tbprofiler//tbdb.spoligotype_spacers.txt\nUsing spoligotype_annotations file: /home/pa486/miniconda3/envs/tbprofiler/share/tbprofiler//tbdb.spoligotype_list.csv\nUsing barcode file: /home/pa486/miniconda3/envs/tbprofiler/share/tbprofiler//tbdb.barcode.bed\n\nRunning command:\nset -u pipefail; trimmomatic PE -threads 1 -phred33 G26832_R1.trim.fastq.gz G26832_R2.trim.fastq.gz -baseout ./fdc5e931-5501-4f08-a0f7-89a8b758422a LEADING:3 TRAILING:3 SLIDINGWINDOW:4:20 MINLEN:36\n\nRunning command:\nset -u pipefail; cat ./fdc5e931-5501-4f08-a0f7-89a8b758422a_1U ./fdc5e931-5501-4f08-a0f7-89a8b758422a_2U > ./fdc5e931-5501-4f08-a0f7-89a8b758422a_TU\n...\n...\n\nWriting outputs\n---------------\nWriting json file: ./results/G26832.results.json\n\n\nYou can have a look at the results from the results directory\ncat results/G26832.results.json\nThis may not be readily helpful to you.\nLet’s try and rerun the above command but now specifying other option that will be useful to us:\n\n--csv — to save an output in a .csv format\n--txt — to save an output in a text format\n--no_trim — to ignore trimming to speed up the process as we have already trimmed our reads.\n--threads — to specify the amount of computing resources to allocate to the run.\n--external_db — to specify an external database. Very useful if you have build your own database and have included extra drug resistant SNPs for instance.\n--dir — we will use this to specify a storage directory and not use the deafult results.\n--spoligotype — to perform experimental spoligotyping. This is enabled for bam and fastq input.\n\n\n\n\n\n\n\ntb-profiler profile — more options\n\n\n\nWe will now include more options and rerun our analysis, this time specifying our own directory.\ntb-profiler profile --read1 G26832_R1.trim.fastq.gz --read2 G26832_R2.trim.fastq.gz --csv --txt --pdf --no_trim --threads 4 --spoligotype --prefix G26832 --dir tbprofiler_results\nUsing ref file: /home/pa486/miniconda3/envs/tbprofiler/share/tbprofiler//tbdb.fasta\nUsing gff file: /home/pa486/miniconda3/envs/tbprofiler/share/tbprofiler//tbdb.gff\nUsing bed file: /home/pa486/miniconda3/envs/tbprofiler/share/tbprofiler//tbdb.bed\nUsing version file: /home/pa486/miniconda3/envs/tbprofiler/share/tbprofiler//tbdb.version.json\nUsing json_db file: /home/pa486/miniconda3/envs/tbprofiler/share/tbprofiler//tbdb.dr.json\nUsing variables file: /home/pa486/miniconda3/envs/tbprofiler/share/tbprofiler//tbdb.variables.json\nUsing spoligotype_spacers file: /home/pa486/miniconda3/envs/tbprofiler/share/tbprofiler//tbdb.spoligotype_spacers.txt\nUsing spoligotype_annotations file: /home/pa486/miniconda3/envs/tbprofiler/share/tbprofiler//tbdb.spoligotype_list.csv\nUsing barcode file: /home/pa486/miniconda3/envs/tbprofiler/share/tbprofiler//tbdb.barcode.bed\n\nRunning command:\nset -u pipefail; bwa mem -t 4 -K 10000000 -c 100 -R '@RG\\tID:G26831\\tSM:G26831\\tPL:illumina' -M -T 50 /home/pa486/miniconda3/envs/tbprofiler/share/tbprofiler//tbdb.fasta G26831_R1.trim.fastq.gz G26831_R2.trim.fastq.gz | samtools sort -@ 4 -o tbprofiler_results/ad5e1742-3bd6-415d-86a2-72e32d68a49f.pair.bam -\n...\n...\nWriting outputs\n---------------\nWriting json file: tbprofiler_results/results/G26831.results.json\nWriting text file: tbprofiler_results/results/G26831.results.txt\nWriting csv file: tbprofiler_results/results/G26831.results.csv\nNotice that, with the addition of more threads and the –no_trim flags, we have significantly reduced our running time to less than 3 minutes.\n\n\nLet’s have a quick look at the .csv output\n\n\n\n\n\n\nhead -n 22 tbprofiler_results/results/G26832.results.csv\nTBProfiler report\n=================\n\nThe following report has been generated by TBProfiler.\n\nSummary\n-------\nID,G26832\nDate,Tue Dec  6 13:44:47 2022\nStrain,lineage4.6.2.2\nDrug-resistance,Sensitive\nMedian Depth,133\n\nLineage report\n--------------\nLineage,Estimated Fraction,Spoligotype,Rd\nlineage4,1.000,LAM;T;S;X;H,None\nlineage4.6,0.999,T;LAM,None\nlineage4.6.2,0.996,T;LAM,RD726\nlineage4.6.2.2,0.998,LAM10-CAM,RD726\n\n\n\n\nThe output looks great!\nThere are lots of interesting summaries about our genome. Can you interpret the output?\n\nSummarising runs\nThe results from numerous runs can be collated into one table.\nFirst, let’s run the full analysis on our remaining TB genomes so we have more results to collate.\nWe will do this with a for loop:\nfor i in G*_R1.trim.fastq.gz; do tb-profiler profile --csv --txt --pdf --no_trim --spoligotype --prefix ${i%_R1.trim.fastq.gz} --threads 4 --read1 $i --read2 ${i%_R1.trim.fastq.gz}_R2.trim.fastq.gz --dir tbprofiler_results; done\nNow, let’s cd into the tbprofiler_results and using the command below, let’s collate our results from the three genomes:\ncd tbprofiler_results\ntb-profiler collate\nUsing ref file: /home/pa486/miniconda3/envs/tbprofiler/share/tbprofiler//tbdb.fasta\nUsing gff file: /home/pa486/miniconda3/envs/tbprofiler/share/tbprofiler//tbdb.gff\nUsing bed file: /home/pa486/miniconda3/envs/tbprofiler/share/tbprofiler//tbdb.bed\nUsing version file: /home/pa486/miniconda3/envs/tbprofiler/share/tbprofiler//tbdb.version.json\nUsing json_db file: /home/pa486/miniconda3/envs/tbprofiler/share/tbprofiler//tbdb.dr.json\nUsing variables file: /home/pa486/miniconda3/envs/tbprofiler/share/tbprofiler//tbdb.variables.json\nUsing spoligotype_spacers file: /home/pa486/miniconda3/envs/tbprofiler/share/tbprofiler//tbdb.spoligotype_spacers.txt\nUsing spoligotype_annotations file: /home/pa486/miniconda3/envs/tbprofiler/share/tbprofiler//tbdb.spoligotype_list.csv\nUsing barcode file: /home/pa486/miniconda3/envs/tbprofiler/share/tbprofiler//tbdb.barcode.bed\n100%|████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00, 44.33it/s]\nThe above command, which runs in seconds, will automatically create a number of collated result files from all the individual result files in the tbprofiler_results directory. If you would like to generate this file for a subset of the runs you can provide a list with the run names using the –samples flag. The prefix for the output files is tbprofiler by default but this can be changed with the –prefix flag.\n\n\n\n\n\n\nExercise 11.1.4.3: Interpreting your findings\n\n\n\nYou have results of three TB genomes.\nUsing any results file of your choice, go ahead and interpret your findings.\n\n\n\n\n\n\nSolution:\n\n\n\n\n\nThis will be discussed in class\n\n\n\n\n\nLet’s remember to cd back into our working directory.\ncd ../\nIf the bam and vcf directories created are not needed you can go ahead and remove them as they occupy a lot of space.\nrm -r tbprofiler_results/vcf tbprofiler_results/bam"
  },
  {
    "objectID": "materials/11-genotyping/11.1-genotyping.html#antimicrobial-susceptibility-testing",
    "href": "materials/11-genotyping/11.1-genotyping.html#antimicrobial-susceptibility-testing",
    "title": "11.1 Bacterial Genotyping and Drug Resistance Prediction",
    "section": "11.1.5 Antimicrobial Susceptibility Testing",
    "text": "11.1.5 Antimicrobial Susceptibility Testing\nWe have already performed some antimicrobial susceptibility analysis on TB pathogens using tb-profiler.\nNow, let’s look at other tools to perform antimicrobial resistance (AMR) prediction.\nLike the serotyping and the genotyping analysis we have already been previewed to, there exists many AMR prediction tools. In this course, we will introduce you to one which is very widely used — ARIBA\nAvailable tools used to perform AMR predictions include:\n\nARIBA\nShort Read Sequence Typing for Bacterial Pathogens\nMykrobe\nResFinder\n ARG-ANNOT\nSSTAR\nAmromics\nNastyBugs\nPIMA\nAMR for R\nVAMPr\nC-BIRD\n\nNote that most of these tools also have functionalities of performing genotyping to identify the ST of the bacteria.\nNow let’s focus of ARIBA\n\n\n\n\n\n\nChange working environment\n\n\n\nStill in the genotyping_and_dr directory run the below command to deactivate the tbprofiler environment and activate the ariba environment:\nmamba deactivate\nmamba activate ariba\n\n\n\nAntimicrobial Resistance Identification By Assembly (ARIBA)\nARIBA is a tool that identifies antibiotic resistance genes by running local assemblies. It can also be used for MLST calling. The input is a FASTA file of reference sequences (can be a mix of genes and noncoding sequences) and paired sequencing reads. ARIBA reports which of the reference sequences were found, plus detailed information on the quality of the assemblies and any variants between the sequencing reads and the reference sequences.\nIn addition to the github page you can have a look at their publication for more details to the use of the tool.\nARIBA was developed to identify AMR determinants from public or custom databases using paired read data as input.\nBriefly, reference sequences in the AMR database are clustered by similarity using CD-HIT. Reads are mapped to the reference sequences using minimap to produce a set of reads for each cluster. These reads map to at least one of the sequences in that cluster. The reads for each cluster and their sequence pairs are assembled independently using fermi-lite under a variety of parameter combinations, and the closest reference sequence to the resulting contigs is identified with the program nucmer from the MUMmer package. The assembly is compared to the reference sequence to identify completeness and any variants between the sequences using the nucmer and show-snps programs from MUMmer. The reads for the cluster are mapped to the assembly with Bowtie2 and variants are called with SAMtools. Finally, a detailed report is made of all the sequences identified in the sample, including, but not limited to, the presence or absence of variants pre-defined to be of importance to AMR.\nOverview of the ARIBA mapping and targeted assembly pipeline retrieved from https://www.microbiologyresearch.org/content/journal/mgen/10.1099/mgen.0.000131\n\n\n\n\n\n\nHelp — ariba\n\n\n\nDo this to get the help information for ariba which shows all the available commands\nariba --help\nusage: ariba <command> <options>\n\nARIBA: Antibiotic Resistance Identification By Assembly\n\noptional arguments:\n  -h, --help      show this help message and exit\n\nAvailable commands:\n  \n    aln2meta      Converts multi-aln fasta and SNPs to metadata\n    expandflag    Expands flag column of report file\n    flag          Translate the meaning of a flag\n    getref        Download reference data\n    micplot       Make violin/dot plots using MIC data\n    prepareref    Prepare reference data for input to \"run\"\n    prepareref_tb\n                  Prepare reference TB data for input to \"run\"\n    pubmlstget    Download species from PubMLST and make db\n    pubmlstspecies\n                  Get list of available species from PubMLST\n    refquery      Get cluster or sequence info from prepareref output\n    run           Run the local assembly pipeline\n    summary       Summarise multiple reports made by \"run\"\n    test          Run small built-in test dataset\n    version       Get versions and exit\n\n\nThe main commands we will explore in this workshop are the run — to run the local assembly pipeline and the summary — to summarise multiple reports made by “run”.\nDo this to get the help information for ariba run\n\n\n\n\n\n\nHelp — ariba run\n\n\n\nariba run --help\nusage: ariba run [options] <prepareref_dir> <reads1.fq> <reads2.fq> <outdir>\n\nRuns the local assembly pipeline. Input is dir made by prepareref, and paired reads\n\npositional arguments:\n  prepareref_dir        Name of output directory when \"ariba prepareref\" was run\n  reads_1               Name of fwd reads fastq file\n  reads_2               Name of rev reads fastq file\n  outdir                Output directory (must not already exist)\n\noptional arguments:\n  -h, --help            show this help message and exit\n...\n\n\n\n\n\n\n\n\nHelp — ariba summary\n\n\n\nariba summary --help\nusage: ariba summary [options] <outprefix> [report1.tsv report2.tsv ...]\n\nMake a summary of multiple ARIBA report files, and also make Phandango files\n\npositional arguments:\n  outprefix             Prefix of output files\n  infiles               Files to be summarised\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -f FILENAME, --fofn FILENAME\n                        File of filenames of ariba reports to be summarised. Must be used if no input\n                        files listed after the outfile. The first column should be the filename. An\n                        optional second column can be used to specify a sample name \n...\n\n\n\n\n\n\n\n\nUsage\n\n\n\nThe general format for running ariba is:\nariba <command> <options>\nWhen specifying the run command, the general format is:\nariba run [options] <prepareref_dir> <reads1.fq> <reads2.fq> <outdir>\nWhen specifying the summary command, the general format is:\nariba summary [options] <outprefix> [report1.tsv report2.tsv ...]\n\n\n\n\n\n\n\n\nInput\n\n\n\nThe run command requires two main inputs:\n\na FASTA file of reference sequences (This should be prepared before hand following instructions from ariba github page. The reference database used are mostly from NCBI or CARD. For this workshop, we have prepared this file for you called amr_ref_db.card.prepareref.\na pair of fastq files as your input sample files.\n\nThe summary command requires only one input:\n\nThe report.tsv files\n\n\n\n\n\n\n\n\n\nOutput\n\n\n\nThe run command returns seven files stored in the specified output directory. The files are:\n\nassembled_genes.fa.gz\nassembled_seqs.fa.gz\nassemblies.fa.gz\ndebug.report.tsv\nlog.clusters.gz\nreport.tsv\nversion_info.txt\n\nThe most important for us is the report.tsv. However, we may not be able to readily interpret the content now. This is summarised to a more informative file with the summary command.\nThe summary command returns three files stored with the specified output file prefix. The files are:\n\n.csv\n.phandango.csv\n.phandango.tre\n\n\n\nWe are now ready to perform our AMR prediction.\nFirst, let’s try our hands on knowing what the predicted AMR pattern of our first genome is. Our first genome to analyse is G26832 and we will use the G26832_R?.trim.fastq.gz files as our input.\nLet’s go ahead and run the below command with no options to perform local assemblies and call variants from our sample:\n\n\n\n\n\n\nariba\n\n\n\nariba run amr_ref_db.card.prepareref G26832_R1.trim.fastq.gz G26832_R2.trim.fastq.gz G26832_ariba_output\nThis should take about 4 minutes to run\nAAC_2___Ic detected 1 threads available to it\nAAC_2___Ic reported completion\nBrachyspira_hyodysenteriae_23S detected 1 threads available to it\nBrachyspira_hyodysenteriae_23S reported completion\nCampylobacter_jejuni_23S detected 1 threads available to it\nCampylobacter_jejuni_23S reported completion\nChlamydomonas_reinhardtii_23S detected 1 threads available to it\nChlamydomonas_reinhardtii_23S reported completion\nClostridioides_difficile_23S detected 1 threads available to it\nClostridioides_difficile_23S reported completion\nCutibacterium_acnes_16S detected 1 threads available to it\nCutibacterium_acnes_16S reported completion\nErm_37_ detected 1 threads available to it\nErm_37_ reported completion\nEscherichia_coli_23S detected 1 threads available to it\nEscherichia_coli_23S reported completion\nHelicobacter_pylori_23S detected 1 threads available to it\nHelicobacter_pylori_23S reported completion\nMoraxella_catarrhalis_23S detected 1 threads available to it\nMoraxella_catarrhalis_23S reported completion\nMyco- detected 1 threads available to it\nMyco- reported completion\nMycobacterium_tuberculosis_- detected 1 threads available to it\nMycobacterium_tuberculosis_- reported completion\nMycobacterium_tuberculosis_16S+ detected 1 threads available to it\n...\n\n\nDownload the G26832_ariba_output/report.tsv file onto your desktop and have a look at it.\nDo you find it informative?\nLet’s perform a second run on our genome G26831.\nariba run amr_ref_db.card.prepareref G26831_R1.trim.fastq.gz G26831_R2.trim.fastq.gz G26831_ariba_output\n… and now summarise data from both runs with the summary command:\nariba summary ariba_summary *_ariba_output/report.tsv\nAs already previewed to, the summary command produces three files:\nls ariba*\n--- ariba_summary.csv\n--- ariba_summary.phandango.csv\n--- ariba_summary.phandango.tre\nLet’s have a look at the ariba_summary.csv file by downloading onto our desktop and viewing with an appropriate spreadsheet application.\n\n\n\n\n\n\nExercise 11.1.5.1: analysing all samples with ariba using a for loop\n\n\n\n\nUsing a for loop, run ariba on all the samples in the current working directory and then summarise their output.\nReport and interpret what you see.\nYou will notice that the file names have the format G26832_ariba_output/report.tsv. The workflow modifies our original sample names in the summary.csv file, by adding information about the location of the report.tsv file. We only want to keep the sample names, in this example just G26832. Write a simple script to clean the file names in the summary.csv file.\nWhat other way could you have explored to avoid having the long file names in the summary.csv file?\nHave a look at the other two files generated in the summary output by downloading them and dragging and dropping them into phandango.\n\nPhandango allows an interactive visualization of genome phylogenies.\nYou can perform this exercise in groups. Each group will present on their results.\n\n\n\n\n\n\nSolution:\n\n\n\n\n\nDon’t expect to see the solution yet.\n\n\n\n\n\n\n\n\n\ndeactivate ariba environment\nNow that we are done with all our analysis, let’s deactivate the ariba environment:\nmamba deactivate"
  },
  {
    "objectID": "materials/11-genotyping/11.1-genotyping.html#credit",
    "href": "materials/11-genotyping/11.1-genotyping.html#credit",
    "title": "11.1 Bacterial Genotyping and Drug Resistance Prediction",
    "section": "11.1.6 Credit",
    "text": "11.1.6 Credit\nInformation on this page has been adapted and modified from the following source(s):\n\nhttps://github.com/tseemann/mlst\nhttps://en.wikipedia.org/wiki/Serotype\nhttps://github.com/sanger-pathogens/seroba\nhttps://www.microbiologyresearch.org/content/journal/mgen/10.1099/mgen.0.000186\nhttps://github.com/xiaeryu/SpoTyping-v2.0\nhttps://genomemedicine.biomedcentral.com/articles/10.1186/s13073-016-0270-7\nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC229700/\nhttps://github.com/jodyphelan/TBProfiler\nhttps://github.com/sanger-pathogens/ariba\nhttps://www.microbiologyresearch.org/content/journal/mgen/10.1099/mgen.0.000131"
  },
  {
    "objectID": "materials/12-intro_hpc/12.1-intro_hpc.html",
    "href": "materials/12-intro_hpc/12.1-intro_hpc.html",
    "title": "12.1 Introduction to High Performance Computing",
    "section": "",
    "text": "Teaching: – min || Exercises: – min"
  },
  {
    "objectID": "materials/12-intro_hpc/12.1-intro_hpc.html#overview",
    "href": "materials/12-intro_hpc/12.1-intro_hpc.html#overview",
    "title": "12.1 Introduction to High Performance Computing",
    "section": "Overview",
    "text": "Overview\n\n\n\n\n\n\n\n\n\n\n\n\nQuestions:\n\n\n\n\nWhat is a HPC and how does it differ from a regular computer?\nWhat can a HPC be used for?\n\n\n\n\n\n\n\n\n\nLearning Objectives:\n\n\n\n\nDescribe how a typical HPC is organised: nodes, job scheduler and filesystem.\nDistinguish the roles of a login node and a compute node.\nDescribe the role of a job scheduler.\nUnderstand the difference between “scratch” and “home” storage.\n\n\n\n\n\n\n\n\n\nKey Points:\n\n\n\n\nA HPC consists of several computers connected in a network. These are called nodes:\n\nThe login nodes are the machines that we connect to and from where we interact with the HPC. These should not be used to run resource-intensive tasks.\nThe compute nodes are the high-performance machines on which the actual heavy computations run. Jobs are submitted to the compute nodes through a job scheduler.\n\nThe job scheduler is used to submit scripts to be run on the compute nodes.\n\nThe role of this software is to manage large numbers of jobs being submitted and prioritise them according to their resource needs.\nWe can configure how our jobs are run by requesting the adequate resources (CPUs and RAM memory).\nChoosing resources appropriately helps to get our jobs the right level of priority in the queue.\n\nThe filesystem on a HPC is often split between a small (backed) home directory, and a large and high-performance (non-backed) scratch space.\n\nThe user’s home is used for things like configuration files and local software instalation.\nThe scratch space is used for the data and analysis scripts.\nNot all HPC servers have this filesystem organisation - always check with your local HPC admin."
  },
  {
    "objectID": "materials/12-intro_hpc/12.1-intro_hpc.html#what-is-an-hpc-and-what-are-its-uses",
    "href": "materials/12-intro_hpc/12.1-intro_hpc.html#what-is-an-hpc-and-what-are-its-uses",
    "title": "12.1 Introduction to High Performance Computing",
    "section": "12.1.1 What is an HPC and what are its uses?",
    "text": "12.1.1 What is an HPC and what are its uses?\nHPC stands for high-performance computing and usually refers to several computers connected together in a network (forming a HPC cluster). Each of these computers is referred to as a node in the network.\nThe main usage of HPC clusters is to run resource-intensive and/or parallel tasks. For example: running thousands of simulations, each one taking several hours; assembling a genome from sequencing data, which requires computations on large volumes of data in memory. These tasks would be extremely challenging to complete on a regular computer. However, they are just the kind of task that a HPC would excel at.\n\n\n\n\n\n\nTerminology Alert!\n\n\n\nThe terms HPC and cluster are often used interchangeably to mean the same thing.\n\n\nWhen working on a HPC it is important to understand what kinds of resources are available to us. These are the main resources we need to consider:\n\nCPU (central processing units) is the “brain” of the computer, performing a wide range of operations and calculations. CPUs can have several “cores”, which means they can run tasks in parallel, increasing the throughput of calculations per second. A typical personal computer may have a CPU with 4-8 cores. A single compute node on the HPC may have 32-48 cores (and often these are faster than the CPU on our computers).\nRAM (random access memory) is a quick access storage where data is temporarily held while being processed by the CPU. A typical personal computer may have 8-32Gb of RAM. A single compute nodes on a HPC may often have >100Gb RAM.\nGPUs (graphical processing units) are similar to CPUs, but are more specialised in the type of operations they can do. While less flexible than CPUs, each GPU can do thousands of calculations in parallel. This makes them extremely well suited for graphical tasks, but also more generally for matrix computations and so are often used in machine learning applications.\n\nUsually, HPC servers are available to members of large institutions (such as a Universities or research institutes) or sometimes from cloud providers. This means that:\n\nThere are many users, who may simultaneously be using the HPC.\nEach user may want to run several jobs concurrently.\nOften large volumes of data are being processed and there is a need for high-performance storage (allowing fast read-writting of files).\n\nSo, at any one time, across all the users, there might be many thousands of processes running on the HPC! There has to be a way to manage all this workload, and this is why HPC clusters are typically organised somewhat differently from what we might be used to when we work on our own computers. Figure 1 shows a schematic of a HPC, and we go into its details in the following sections.\n\n\n\nOrganisation of a typical HPC\n\n\n\nNodes\nThere are two types of nodes on a cluster:\n\nlogin nodes (also known as head or submit nodes).\ncompute nodes (also known as worker nodes).\n\nThe login nodes are the computers that the user connects to and from where they interact with the cluster. Depending on the size of the cluster, there is often only one login node, but larger clusters may have several of them. Login nodes are used to interact with the filesystem (move around the directories), download and move files, edit and/or view text files and doing other small routine tasks.\nThe compute nodes are the machines that will actually do the hard work of running jobs. These are often high-spec computers with many CPUs and high RAM (or powerful GPU cards), suitable for computationally demanding tasks. Often, there are several “flavours” of compute nodes on the same cluster. For example some compute nodes may have fewer CPUs but higher memory (suitable for memory-intensive tasks), while others may have the opposite (suitable for highly-parallelisable tasks).\nUsers do not have direct access to the compute nodes and instead submitting jobs via a job scheduler.\n\n\nJob Scheduler\nA job scheduler is a software used to submit commands to be run on the compute nodes (orange box in Figure 1). This is needed because there may often be thousands of processes that all the users of the HPC want to run at any one time. The job scheduler’s role is to manage all these jobs, so you don’t have to worry about it.\nWe will cover the details of how to use a job scheduler in “Using a Job Scheduler”. \nFor now, it is enough to know that, using the job scheduler, the user can request specific resources to run their job (e.g. number of cores, RAM, how much time we want to reserve the compute node to run our job, etc.). The job scheduler software then takes care of considering all the jobs being submitted by all the users and putting them in a queue until there are compute nodes available to run the job with the requested resources.\n\n\n\nAn analogy of the role of a job scheduler. You can think of a job scheduler as a porter in a restaurant, who checks the groups of people in the queue and assigns them a seat depending on the size of the group and how long they might stay for dinner.\n\n\nIn terms of parallelising calculations, there are two ways to think about it, and which one we use depends on the specific application. Some software packages have been developed to internally parallelise their calculations (or you may write your own script that uses a parallel library). These are very commonly used in bioinformatic applications, for example. In this case we may want to submit a single job, requesting several CPU cores for it.\nIn other cases, we may have a program that does not parallelise its calculations, but we want to run many iterations of it. A typical example is when we want to run simulations: each simulation only uses a single core, but we want to run thousands of them. In this case we would want to submit each simulation as a separate job, but only request a single CPU core for each job.\nFinally, we may have a case where both of these are true.\nFor example, we want to process several data files, where each data file can be processed using tools that parallelise their calculations. In this case we would want to submit several jobs, requesting several CPU cores for each.\n\n\n\n\n\n\nJob Schedulers\n\n\n\nThere are many job scheduler programs available, in this course we will cover one called SLURM, but other common ones include LSF, PBS, HT Condor, among others.\n\n\n\n\nFilesystem\nThe filesystem on a HPC cluster often consists of storage partitions that are shared across all the nodes, including both the login and compute nodes (green box in Figure 1). This means that data can be accessed from all the computers that compose the HPC cluster.\nAlthough the filesystem organisation may differ depending on the institution, typical HPC servers often have two types of storage:\n\nThe user’s home directory (e.g. /home/user) is the default directory that one lands on when logging in to the HPC. This is often quite small and possibly backed up. The home directory can be used for storing things like configuration files or locally installed software.\nA scratch space (e.g. /scratch/user), which is high-performance, large-scale storage. This type of storage may be private to the user or shared with a group. It is usually not backed up, so the user needs to ensure that important data are stored elsewhere. This is the main partition were data is processed from.\n\n\n\n\n\n\n\nHPC Filesystem\n\n\n\nThe separation into “home” and “scratch” storage space may not always apply to the HPC available at your institution. Also, the location of the “scratch space” will most likely differ from the example used in this course. Ask your local HPC admin to learn more about your specific setup."
  },
  {
    "objectID": "materials/12-intro_hpc/12.1-intro_hpc.html#getting-help",
    "href": "materials/12-intro_hpc/12.1-intro_hpc.html#getting-help",
    "title": "12.1 Introduction to High Performance Computing",
    "section": "12.1.2 Getting Help",
    "text": "12.1.2 Getting Help\nIn most cases there will be a HPC administrator (or team), who you can reach out for help if you need to obtain more information about how your HPC is organised.\nSome of the questions you may want to ask when you start using a HPC are:\n\nwhat kind of compute nodes are available?\nwhat storage do I have access to, and how much?\nwhat job scheduler software is used, and can you give me an example submission script to get started?\nwill I be charged for the use of the HPC?\n\nAlso, it is often the case that the HPC needs some maintenance service, and you should be informed that this is happening (e.g. by a mailing list). Sometimes things stop working or break, and there may be some time when your HPC is not available while work is being done on it.\n\n\n\n\n\n\nExercise 12.1.2.1:\n\n\n\nA PhD student wants to process some microscopy data using a python script developed by a postdoc colleague. They have instructions for how to install the necessary python packages, and also the actual python script to process the images.\nQ1. Which of the following describes the best practice for the student to organise their files/software?\nOption A:\n/scratch/user/project_name/software/ # python packages\n/scratch/user/project_name/data/     # image files\n/scratch/user/project_name/scripts/  # analysis script\nOption B:\n/home/user/software/                # python packages\n/scratch/user/project_name/data/    # image files \n/scratch/user/project_name/scripts/ # analysis script\nOption C:\n/home/user/project_name/software/ # python packages\n/home/user/project_name/data/        # image files\n/home/user/project_name/scripts/     # analysis script\nQ2. It turns out that the microscopy data were very large and compressed as a zip file. The postdoc told the student they can run unzip image_files.zip to decompress the file. Should they run this command from the login node or submit it as a job to one of the compute nodes?\nQ3. The analysis script used by the student generates new versions of the images. In total, after processing the data, the student ends up with ~1TB of data (raw + processed images). Their group still has 5TB of free space on the HPC, so the student decides to keep the data there until they finish the project. Do you agree with this choice, and why? What factors would you take into consideration in deciding what data to keep and where?\n\n\n\n\n\n\nSolution:\n\n\n\n\n\nA1.\nOption C is definitely discouraged: as /home is typically not high-performance and has limited storage, it should not be used for storing/processing data. Option A and B only differ in terms of where the software packages are installed. Typically software can be installed in the user’s /home, avoiding the need to reinstall it multiple times, in case the same software is used in different projects. Therefore, option B is the best practice in this example.\nA2.\nSince compressing/uncompressing files is a fairly routine task and unlikely to require too many resources, it would be OK to run it on the login node. If in doubt, the student could have gained “interactive” access to one of the compute nodes (we will cover this in another section).\nA3.\nLeaving the data on the HPC is probably a bad choice. Since typically “scratch” storage is not backed-up it should not be relied on to store important data. If the student doesn’t have access to enough backed-up space for all the data, they should at least back up the raw data and the scripts used to process it. This way, if there is a problem with “scratch” and some processed files are lost, they can recreate them by re-running the scripts on the raw data.\nOther criteria that could be used to decide which data to leave on the HPC, backup or even delete is how long each step of the analysis takes to run, as there may be a significant computational cost associated with re-running heavy data processing steps."
  },
  {
    "objectID": "materials/12-intro_hpc/12.1-intro_hpc.html#credit",
    "href": "materials/12-intro_hpc/12.1-intro_hpc.html#credit",
    "title": "12.1 Introduction to High Performance Computing",
    "section": "12.1.3 Credit",
    "text": "12.1.3 Credit\nInformation on this page has been adapted and modified from the following source(s):\n\nhttps://github.com/cambiotraining/hpc-intro"
  },
  {
    "objectID": "materials/template_file.html",
    "href": "materials/template_file.html",
    "title": "Title of Page",
    "section": "",
    "text": "materials in markdown"
  },
  {
    "objectID": "unix_cheat_sheet.html",
    "href": "unix_cheat_sheet.html",
    "title": "Unix Cheat Sheet",
    "section": "",
    "text": "This document gives a brief summary of useful Unix commands."
  },
  {
    "objectID": "unix_cheat_sheet.html#documentation-and-help",
    "href": "unix_cheat_sheet.html#documentation-and-help",
    "title": "Unix Cheat Sheet",
    "section": "Documentation and Help",
    "text": "Documentation and Help\n\n\n\n\n\n\n\nman {command}\nmanual page for the program\n\n\nwhatis {command}\nshort description of the program\n\n\n{command} --help\nmany programs use the --help flag to print documentation"
  },
  {
    "objectID": "unix_cheat_sheet.html#listing-files",
    "href": "unix_cheat_sheet.html#listing-files",
    "title": "Unix Cheat Sheet",
    "section": "Listing files",
    "text": "Listing files\n\n\n\n\n\n\n\nls\nlist files in the current directory\n\n\nls {path}\nlist files in the specified path\n\n\nls -l {path}\nlist files in long format (more information)\n\n\nls -a {path}\nlist all files (including hidden files)"
  },
  {
    "objectID": "unix_cheat_sheet.html#change-directories",
    "href": "unix_cheat_sheet.html#change-directories",
    "title": "Unix Cheat Sheet",
    "section": "Change Directories",
    "text": "Change Directories\n\n\n\n\n\n\n\ncd {path}\nchange to the specified directory\n\n\ncd or cd ~\nchange to the home directory\n\n\ncd ..\nmove back one directory\n\n\npwd\nprint working directory. Shows the full path of where you are at the moment (useful if you are lost)"
  },
  {
    "objectID": "unix_cheat_sheet.html#make-or-remove-directories",
    "href": "unix_cheat_sheet.html#make-or-remove-directories",
    "title": "Unix Cheat Sheet",
    "section": "Make or Remove Directories",
    "text": "Make or Remove Directories\n\n\n\n\n\n\n\nmkdir {dirname}\ncreate a directory with specified name\n\n\nrmdir {dirname}\nremove a directory (only works if the directory is empty)\n\n\nrm -r {dirname}\nremove the directory and all it’s contents (use with care)"
  },
  {
    "objectID": "unix_cheat_sheet.html#copy-move-and-remove-files",
    "href": "unix_cheat_sheet.html#copy-move-and-remove-files",
    "title": "Unix Cheat Sheet",
    "section": "Copy, Move and Remove Files",
    "text": "Copy, Move and Remove Files\n\n\n\n\n\n\n\ncp {source/path/file1} {target/path/}\ncopy “file1” to another directory keeping its name\n\n\ncp {source/path/file1} {target/path/file2}\ncopy “file1” to another directory naming it “file2”\n\n\ncp {file1} {file2}\nmake a copy of “file1” in the same directory with a new name “file2”\n\n\nmv {source/path/file1} {target/path/}\nmove “file1” to another directory keeping its name\n\n\nmv {source/path/file1} {target/path/file2}\nmove “file1” to another directory renaming it as “file2”\n\n\nmv {file1} {file2}\nis equivalent to renaming a file\n\n\nrm {filename}\nremove a file"
  },
  {
    "objectID": "unix_cheat_sheet.html#view-text-files",
    "href": "unix_cheat_sheet.html#view-text-files",
    "title": "Unix Cheat Sheet",
    "section": "View Text Files",
    "text": "View Text Files\n\n\n\n\n\n\n\nless {file}\nview and scroll through a text file\n\n\nhead {file}\nprint the first 10 lines of a file\n\n\nhead -n {N} {file}\nprint the first N lines of a file\n\n\ntail {file}\nprint the last 10 lines of a file\n\n\ntail -n {N} {file}\nprint the last N lines of a file\n\n\nhead -n {N} {file} | tail -n 1\nprint the Nth line of a file\n\n\ncat {file}\nprint the whole content of the file\n\n\ncat {file1} {file2} {...} {fileN}\nconcatenate files and print the result\n\n\nzcat {file} and zless {file}\nlike cat and less but for compressed files (.zip or .gz)"
  },
  {
    "objectID": "unix_cheat_sheet.html#find-patterns",
    "href": "unix_cheat_sheet.html#find-patterns",
    "title": "Unix Cheat Sheet",
    "section": "Find Patterns",
    "text": "Find Patterns\nFinding (and replacing) patterns in text is a very powerful feature of several command line programs. The patterns are specified using regular expressions (shortened as regex), which are not covered in this document. See this Regular Expressions Cheat Sheet for a comprehensive overview.\n\n\n\n\n\n\n\ngrep {regex} {file}\nprint the lines of the file that have a match with the regular expression pattern"
  },
  {
    "objectID": "unix_cheat_sheet.html#wildcards",
    "href": "unix_cheat_sheet.html#wildcards",
    "title": "Unix Cheat Sheet",
    "section": "Wildcards",
    "text": "Wildcards\n\n\n\n\n\n\n\n*\nmatch any number of characters\n\n\n?\nmatch any character only once\n\n\nExamples\n\n\n\nls sample*\nlist all files that start with the word “sample”\n\n\nls *.txt\nlist all the files with .txt extension\n\n\ncp * {another/directory}\ncopy all the files in the current directory to a different directory"
  },
  {
    "objectID": "unix_cheat_sheet.html#redirect-output",
    "href": "unix_cheat_sheet.html#redirect-output",
    "title": "Unix Cheat Sheet",
    "section": "Redirect Output",
    "text": "Redirect Output\n\n\n\n\n\n\n\n{command} > {file}\nredirect output to a file (overwrites if the file exists)\n\n\n{command} >> {file}\nappend output to a file (creates a new file if it does not already exist)"
  },
  {
    "objectID": "unix_cheat_sheet.html#combining-commands-with-pipes",
    "href": "unix_cheat_sheet.html#combining-commands-with-pipes",
    "title": "Unix Cheat Sheet",
    "section": "Combining Commands with | Pipes",
    "text": "Combining Commands with | Pipes\n\n\n\n\n\n\n\n<command1> | <command2>\nthe output of “command1” is passed as input to “command2”\n\n\nExamples\n\n\n\nls | wc -l\ncount the number of files in a directory\n\n\ncat {file1} {file2} | less\nconcatenate files and view them with less\n\n\ncat {file} | grep \"{pattern}\" | wc -l\ncount how many lines in the file have a match with “pattern”"
  },
  {
    "objectID": "unix_cheat_sheet.html#credit",
    "href": "unix_cheat_sheet.html#credit",
    "title": "Unix Cheat Sheet",
    "section": "Credit",
    "text": "Credit\nInformation on this page has been adapted and modified from the following source(s):\n\nhttps://github.com/cambiotraining/hpc-intro"
  }
]